<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Python | atodorov.org - you can logoff, but you can never leave]]></title>
  <link href="http://atodorov.org/blog/categories/python/atom.xml" rel="self"/>
  <link href="http://atodorov.org/"/>
  <updated>2015-11-24T22:05:37+02:00</updated>
  <id>http://atodorov.org/</id>
  <author>
    <name><![CDATA[Alexander Todorov]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[python-libs in RHEL 7.2 broke SSL verification in s3cmd]]></title>
    <link href="http://atodorov.org/blog/2015/11/24/python-libs-in-rhel-7.2-broke-ssl-verification-in-s3cmd/"/>
    <updated>2015-11-24T21:44:00+02:00</updated>
    <id>http://atodorov.org/blog/2015/11/24/python-libs-in-rhel-7.2-broke-ssl-verification-in-s3cmd</id>
    <content type="html"><![CDATA[<p>Today started with <a href="http://planet.sofiavalley.com">Planet Sofia Valley</a> being
broken again. Indeed it's been broken since last Friday when I've upgraded to
the latest RHEL 7.2. I quickly identified that I was hitting
<a href="https://github.com/s3tools/s3cmd/issues/647">Issue #647</a>. Then I tried the
git checkout without any luck. This is when I started to suspect that python-libs
has been updated in an incompatible way.</p>

<p>After series of reported bugs,
<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1284916">rhbz#1284916</a>,
<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1284930">rhbz#1284930</a>,
<a href="http://bugs.python.org/issue25722">Python#25722</a>, it was clear that
<code>ssl.py</code> was working according to RFC6125, that Amazon S3 was not playing
nicely with this same RFC and that my patch proposal was wrong.
This immediately had me looking upper in the stack at <code>httplib.py</code> and <code>s3cmd</code>.</p>

<p>Indeed there was a change in <code>httplib.py</code> which introduced two parameters,
<em>context</em> and <em>check_hostname</em>, to <code>HTTPSConnection.__init__</code>. The change
also supplied the logic which performs SSL hostname validation.</p>

<pre><code>if not self._context.check_hostname and self._check_hostname:
    try:
        ssl.match_hostname(self.sock.getpeercert(), server_hostname)
    except Exception:
        self.sock.shutdown(socket.SHUT_RDWR)
        self.sock.close()
        raise
</code></pre>

<p>This looks a bit doggy as I don't quite understand the intention behind
<em>not PREDICATE and PREDICATE</em>. Anyway to disable the validation you need
both parameters set to False, which is
<a href="https://github.com/s3tools/s3cmd/pull/668">PR #668</a>.</p>

<p>Notice the two try-except blocks. This is in case we're running with a
version that has a context but not the check_hostname parameter. I've found
the <em>inspect.getmembers</em> function which can be used to figure out what
parameters are there for the init method but a solution based on it
doesn't appear to be more elegant. I will describe this in more details in
my next post.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Speeding Up Celery Backends, Part 3]]></title>
    <link href="http://atodorov.org/blog/2014/11/11/speeding-up-celery-backends-part-3/"/>
    <updated>2014-11-11T15:59:00+02:00</updated>
    <id>http://atodorov.org/blog/2014/11/11/speeding-up-celery-backends-part-3</id>
    <content type="html"><![CDATA[<p>In the second part of this article we've seen
<a href="/blog/2014/11/07/speeding-up-celery-backends-part-2/">how slow Celery actually is</a>.
Now let's explore what happens inside and see if we can't speed things up.</p>

<p>I've used <a href="http://pycallgraph.slowchop.com/en/latest/">pycallgraph</a> to create
call graph visualizations of my application. It has the nice feature to also show
execution time and use different colors for fast and slow operations.</p>

<p>Full command line is:</p>

<pre><code>pycallgraph -v --stdlib --include ... graphviz -o calls.png -- ./manage.py celery_load_test
</code></pre>

<p>where the <code>--include</code> is used to limit the graph to a particular Python module(s).</p>

<h2>General findings</h2>

<p><img src="/images/celery/general.png" title="call graph" alt="call graph" /></p>

<ul>
<li>The first four calls is where most of the time is spent as seen on the picture.</li>
<li>As it seems most of the slow down comes from Celery itself, not the underlying messaging
transport Kombu (not shown on picture)</li>
<li><code>celery.app.amqp.TaskProducer.publish_task</code> takes half of the execution time of
<code>celery.app.base.Celery.send_task</code></li>
<li><code>celery.app.task.Task.delay</code> directly executes <code>.apply_async</code> and can be skipped if one
rewrites the code.</li>
</ul>


<h2>More findings</h2>

<p>In <code>celery.app.base.Celery.send_task</code> there is this block of code:</p>

<pre><code>349         with self.producer_or_acquire(producer) as P:
350             self.backend.on_task_call(P, task_id)
351             task_id = P.publish_task(
352                 name, args, kwargs, countdown=countdown, eta=eta,
353                 task_id=task_id, expires=expires,
354                 callbacks=maybe_list(link), errbacks=maybe_list(link_error),
355                 reply_to=reply_to or self.oid, **options
356             )
</code></pre>

<p><code>producer</code> is always None because delay() doesn't pass it as argument.
I've tried passing it explicitly to apply_async() as so:</p>

<pre><code>from djapp.celery import *

# app = debug_task._get_app() # if not defined in djapp.celery
producer = app.amqp.producer_pool.acquire(block=True)
debug_task.apply_async(producer=producer)
</code></pre>

<p>However this doesn't speedup anything. If we replace the above code block like this:</p>

<pre><code>349         with producer as P:
</code></pre>

<p>it blows up on the second iteration because producer and its channel is already None !?!</p>

<p>If you are unfamiliar with the with statement in Python please read
<a href="http://effbot.org/zone/python-with-statement.htm">this article</a>. In short the with statement is
a compact way of writing try/finally. The underlying <code>kombu.messaging.Producer</code> class does a
<code>self.release()</code> on exit of the with statement.</p>

<p>I also tried killing the with statement and using producer directly but with limited success. While
it was not released(was non None) on subsequent iterations the memory usage grew much more and there
wasn't any performance boost.</p>

<h2>Conclusion</h2>

<p>The with statement is used throughout both Celery and Kombu and I'm not at all sure if
there's a mechanism for keep-alive connections. My time constraints are limited and I'll probably
not spend anymore time on this problem soon.</p>

<p>Since my use case involves task producer and consumers on localhost I'll try to workaround the
current limitations by using Kombu directly
(see <a href="https://gist.github.com/atodorov/2bc1fcd34531ad260ed7">this gist</a>) with a transport that
uses either a UNIX domain socket or a name pipe (FIFO) file.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Speeding up Celery Backends, Part 2]]></title>
    <link href="http://atodorov.org/blog/2014/11/07/speeding-up-celery-backends-part-2/"/>
    <updated>2014-11-07T15:48:00+02:00</updated>
    <id>http://atodorov.org/blog/2014/11/07/speeding-up-celery-backends-part-2</id>
    <content type="html"><![CDATA[<p>In the <a href="/blog/2014/11/05/speeding-up-celery-backends/">first part</a> of this
post I looked at a few celery backends and discovered they didn't meet my needs.
Why is the Celery stack slow? How slow is it actually?</p>

<h2>How slow is Celery in practice</h2>

<ul>
<li>Queue: 500`000 msg/sec</li>
<li>Kombu:  14`000 msg/sec</li>
<li>Celery:  2`000 msg/sec</li>
</ul>


<h2>Detailed test description</h2>

<p>There are three main components of the Celery stack:</p>

<ul>
<li>Celery itself</li>
<li>Kombu which handles the transport layer</li>
<li>Python Queue()'s underlying everything</li>
</ul>


<p>Using the <a href="https://gist.github.com/atodorov/2bc1fcd34531ad260ed7">Queue and Kombu tests</a>
run for 1 000 000 messages I got the following results:</p>

<ul>
<li>Raw Python Queue: Msgs per sec: 500`000</li>
<li>Raw Kombu without Celery where <code>kombu/utils/__init__.py:uuid()</code> is set to return 0

<ul>
<li>with json serializer: Msgs per sec: 5`988</li>
<li>with pickle serializer: Msgs per sec: 12`820</li>
<li>with the custom mem_serializer from <a href="/blog/2014/11/05/speeding-up-celery-backends/">part 1</a>:
Msgs per sec: 14`492</li>
</ul>
</li>
</ul>


<p><strong>Note:</strong> when the test is executed with 100K messages mem_serializer yielded
25`000 msg/sec then the performance is saturated. I've observed similar behavior
with raw Python Queue()'s. I saw some cache buffers being managed internally to avoid OOM
exceptions. This is probably the main reason performance becomes saturated over a longer
execution.</p>

<ul>
<li>Using <a href="https://gist.github.com/atodorov/0156cc41491a5e1ff953">celery_load_test.py</a> modified to
loop 1 000 000 times I got 1908.0 tasks created per sec.</li>
</ul>


<p>Another interesting this worth outlining - in the kombu test there are these lines:
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>with producers[connection].acquire(block=True) as producer:&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>for j in range(1000000):
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p></span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>If we swap them the performance drops down to 3875 msg/sec which is comparable with the
Celery results. Indeed inside Celery there's the same <code>with producer.acquire(block=True)</code>
construct which is executed every time a new task is published. Next I will be looking
into this to figure out exactly where the slowliness comes from.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Speeding up Celery Backends, Part 1]]></title>
    <link href="http://atodorov.org/blog/2014/11/05/speeding-up-celery-backends/"/>
    <updated>2014-11-05T15:20:00+02:00</updated>
    <id>http://atodorov.org/blog/2014/11/05/speeding-up-celery-backends</id>
    <content type="html"><![CDATA[<p>I'm working on an application which fires a lot of Celery tasks - the more
the better! Unfortunately Celery backends seem to be rather slow :(.
Using the <a href="https://gist.github.com/atodorov/0156cc41491a5e1ff953">celery_load_test.py</a>
command for Django I was able to capture some metrics:</p>

<ul>
<li>Amazon SQS backend: 2 or 3 tasks/sec</li>
<li>Filesystem backend: 2000 - 2500 tasks/sec</li>
<li>Memory backend: around 3000 tasks/sec</li>
</ul>


<p>Not bad but I need in the order of 10000 tasks created per sec!
The other noticeable thing is that memory backend isn't much faster compared to
the filesystem one! NB: all of these backends actually come from the kombu package.</p>

<h2>Why is Celery slow ?</h2>

<p>Using <code>celery_load_test.py</code> together with
<a href="/blog/2014/11/05/performance-profiling-in-python-with-cprofile/">cProfile</a> I
was able to pin-point some problematic areas:</p>

<ul>
<li><code>kombu/transports/virtual/__init__.py</code>: class Channel.basic_publish() - does
self.encode_body() into base64 encoded string. Fixed with custom transport backend
I called fastmemory which redefines the body_encoding property:</li>
</ul>


<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="nd">@cached_property</span>
</span><span class='line'><span class="k">def</span> <span class="nf">body_encoding</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class='line'>    <span class="k">return</span> <span class="bp">None</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<ul>
<li>Celery uses json or pickle (or other) serializers to serialize the data.
While json yields between 2000-3000 tasks/sec, pickle does around 3500 tasks/sec.
Replacing with a custom serializer which just returns
the objects (since we read/write from/to memory) yields about 4000 tasks/sec tops:
<div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">from</span> <span class="nn">kombu.serialization</span> <span class="kn">import</span> <span class="n">register</span><span class="o">&lt;/</span><span class="n">li</span><span class="o">&gt;</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">ul</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="k">def</span> <span class="nf">loads</span><span class="p">(</span><span class="n">s</span><span class="p">):</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="k">return</span> <span class="n">s</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="k">def</span> <span class="nf">dumps</span><span class="p">(</span><span class="n">s</span><span class="p">):</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="k">return</span> <span class="n">s</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">register</span><span class="p">(</span><span class="s">&#39;mem_serializer&#39;</span><span class="p">,</span> <span class="n">dumps</span><span class="p">,</span> <span class="n">loads</span><span class="p">,</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span>    <span class="n">content_type</span><span class="o">=</span><span class="s">&#39;application/x-memory&#39;</span><span class="p">,</span>
</span><span class='line'>    <span class="n">content_encoding</span><span class="o">=</span><span class="s">&#39;binary&#39;</span><span class="p">)</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<ul>
<li><code>kombu/utils/__init__.py</code>: def uuid() - generates random unique identifiers
which is a slow operation. Replacing it with <code>return "00000000"</code> boosts performance
to 7000 tasks/sec.</li>
</ul>


<p>It's clear that a constant UUID is not of any practical use but serves well to illustrate
how much does this function affect performance.</p>

<p><strong>Note:</strong>
Subsequent executions of <code>celery_load_test</code> seem to report degraded performance even with
the most optimized transport backend. I'm not sure why is this. One possibility is the random
UUID usage in other parts of the Celery/Kombu stack which drains entropy on the system and
generating more random numbers becomes slower. If you know better please tell me!</p>

<p>I will be looking for a better understanding
of these IDs in Celery and hope to be able to produce a faster uuid() function. Then I'll be
exploring the transport stack even more in order to reach the goal of 10000 tasks/sec.
If you have any suggestions or pointers please share them in the comments.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Performance Profiling in Python with cProfile]]></title>
    <link href="http://atodorov.org/blog/2014/11/05/performance-profiling-in-python-with-cprofile/"/>
    <updated>2014-11-05T14:40:00+02:00</updated>
    <id>http://atodorov.org/blog/2014/11/05/performance-profiling-in-python-with-cprofile</id>
    <content type="html"><![CDATA[<p>This is a quick reference on profiling Python applications with
<a href="https://docs.python.org/2/library/profile.html#module-cProfile">cProfile</a>:</p>

<pre><code>$ python -m cProfile -s time application.py
</code></pre>

<p>The output is sorted by execution time <code>-s time</code></p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code> 9072842 function calls (8882140 primitive calls) in 9.830 CPU seconds
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p>   Ordered by: internal time&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>   ncalls  tottime  percall  cumtime  percall filename:lineno(function)&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>61868    0.575    0.000    0.861    0.000 abstract.py:28(__init__)
</span><span class='line'>41250    0.527    0.000    0.660    0.000 uuid.py:101(__init__)
</span><span class='line'>61863    0.405    0.000    1.054    0.000 abstract.py:40(as_dict)
</span><span class='line'>41243    0.343    0.000    1.131    0.000 __init__.py:143(uuid4)
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p>   577388    0.338    0.000    0.649    0.000 abstract.py:46(&lt;genexpr>)&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>20622    0.289    0.000    8.824    0.000 base.py:331(send_task)
</span><span class='line'>61907    0.232    0.000    0.477    0.000 datastructures.py:467(__getitem__)
</span><span class='line'>20622    0.225    0.000    9.298    0.000 task.py:455(apply_async)
</span><span class='line'>61863    0.218    0.000    2.502    0.000 abstract.py:52(__copy__)
</span><span class='line'>20621    0.208    0.000    4.766    0.000 amqp.py:208(publish_task)
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p>   462640    0.193    0.000    0.247    0.000 {isinstance}
</span><span class='line'>   515525    0.162    0.000    0.193    0.000 abstract.py:41(f)&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>41246    0.153    0.000    0.633    0.000 entity.py:143(__init__)
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p></span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>In the example above (actual application) first line is kombu's
<code>abstract.py: class Object(object).__init__()</code>
and the second one is Python's
<code>uuid.py: class UUID().__init__()</code>.</p>
]]></content>
  </entry>
  
</feed>
