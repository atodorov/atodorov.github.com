<!DOCTYPE html>
<html lang="en">

<head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">

            <meta name="google-site-verification" content="XynqZtldWNBbmsynVQZremIxaaO8Wgs6AGR8UZ7KIkM">

        <title>atodorov.org - Tag QA</title>

        <link href="http://feeds.feedburner.com/atodorov" type="application/atom+xml" rel="alternate" title="atodorov.org Full Atom Feed" />
        <!-- Bootstrap Core CSS -->
        <link href="http://atodorov.org/theme/css/bootstrap.min.css" rel="stylesheet">

        <!-- Custom CSS -->
        <link href="http://atodorov.org/theme/css/clean-blog.min.css" rel="stylesheet">

        <!-- Code highlight color scheme -->
            <link href="http://atodorov.org/theme/css/code_blocks/github.css" rel="stylesheet">

            <!-- CSS specified by the user -->
            <link href="http://atodorov.org/override.css" rel="stylesheet">

        <!-- Custom Fonts -->
        <link href="http://atodorov.org/theme/css/font-awesome.min.css" rel="stylesheet" type="text/css">
        <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
        <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

        <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->

                <meta property="fb:admins" content="1616937247" >
                <meta property="og:locale" content="en_US">
		<meta property="og:site_name" content="atodorov.org">
            <meta name="twitter:card" content="summary_large_image">
            <meta name="twitter:site" content="@atodorov_">
            <meta name="twitter:title" content="atodorov.org">
            <meta name="twitter:description" content="you can logoff, but you can never leave">
                <meta name="twitter:image" content="http://atodorov.org//images/header_02.jpg">
</head>

<body>

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-custom navbar-fixed-top">
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="http://atodorov.org/">atodorov.org</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                        <li><a href="http://mrsenko.com/pylint-workshop/">Pylint Workshop</a></li>

                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

        <header class="intro-header" style="background-image: url('/images/header_02.jpg')">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <div class="page-heading">
                        <h1>Tag QA</h1>
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2019/04/05/the-art-of-unit-testing/" rel="bookmark" title="Permalink to The Art of [Unit] Testing">
                <h2 class="post-title">
                    The Art of [Unit] Testing
                </h2>
            </a>
                <p>A month ago I held a private discussional workshop for a friend's company in Sofia.
With people at executive positions on the tech &amp; business side we discussed
some of their current problems with respect to delivering a quality product.
Additionally I had a list of pre-compiled questions from members of the technical team,
young developers, mostly without formal background in software testing!
Some of the answers were inspired by
<a href="https://amzn.to/2VhvXox">The Art of Unit Testing by Roy Osherove</a> hence the title!</p>
<h2>Questions</h2>
<p><strong>Types of testing, general classification</strong></p>
<p>There are
<a href="https://www.softwaretestinghelp.com/types-of-software-testing/">many types of testing</a>!
Unit, Integration, System, Performance and Load, Mutation, Security, etc. Between
different projects we may use the same term to refer to slightly different types
of testing.</p>
<p>For example in <a href="http://kiwitcms.org">Kiwi TCMS</a> we generally test with a database deployed,
hit the application through its views (backend points that serve HTTP requests) and assert
on the response of these functions. The entire request-response cycle goes through the
application together with all of its settings and add-ons! In this project we are
more likely to classify this type of testing as Integration testing although at times
it is more closer to System testing.</p>
<p>The reason I think Kiwi TCMS is more closer to integration testing is because we execute
the tests against a running development version of the application! The test runner process
and the SUT process are in the same memory space (different threads sometimes).
In contrast full system testing for Kiwi TCMS will mean building and deploying the docker
container (a docker compose actually), hitting the application through the layer
exposed by Docker and asserting on the results. Here test runner and SUT are two distinctly
separate processes. Here we also have email integration, GitHub and Bugzilla integration,
additional 3rd party libraries that are installed in the Docker imaga, e.g. kerberos
authentication.</p>
<p>In another example for
<a href="https://github.com/MrSenko/pelican-ab/tree/master/tests">pelican-ab</a> we mostly have unit
tests which show the SUT as working. However pelican-ab for a static HTML generator
and if failed miserably with <code>DELETE_OUTPUT_DIRECTORY=True</code> setting! The problem here is that
<code>DELETE_OUTPUT_DIRECTORY</code> doesn't control anything in the SUT but does control
behavior in the outer software! This can only be detected with integration tests,
where we perform <em>testing of all integrated modules to verify the combined functionality</em>,
see <a href="http://atodorov.org/blog/2016/12/27/mutation-testing-vs-coverage/">here</a>.</p>
<p>As we don't depend on other services like a database I will classify this as pure integration
testing b/c we are testing a plugin + specific configuration of the larger system which enforces more
constraints.</p>
<p>My best advice is to:</p>
<p>1) have a general understanding of what the different terms mean in the industry
2) have a consensus within your team what do you mean when you say <em>X type of testing</em>
   and <em>Y type of testing</em> so that all of you speak the same language
3) try to speak a language which is closest to what the rest of the industry does,
   baring in mind that we people abuse and misuse language all the time!</p>
<p><strong>What is unit testing</strong></p>
<p>The classical definition is</p>
<blockquote>
<p>A unit test is a piece of code (usually a method) that invokes another piece of code
and checks the correctness of some assumptions afterwards. If the assumptions turn out
to be wrong the unit test has failed.
<strong>A unit is a method or function</strong>.</p>
</blockquote>
<p>Notice the emphasis above: a unit is method or a function - we exercise these in unit tests.
We should be examining their results or in a worse case the state of the class/module
which contains these methods! Now also notice that this definition is different from the
one available in the link above. For reference it is</p>
<blockquote>
<h1>42) Unit Testing</h1>
<p>Testing of an individual software component or module is termed as Unit Testing.</p>
</blockquote>
<p><em>Component</em> can be a single class which comes close to the definition for unit testing but
it can be several different classes, e.g. an authentication component handling several different
scenarios. Modules in the sense of modules in a programming language almost always contain
multiple classes and methods! Thus we unit test the classes and methods but we can rarely
speak about <em>unit testing</em> the module itself.</p>
<p>OTOH the second definition gets the following correctly:</p>
<blockquote>
<p>It is typically done by the programmer and not by testers, as it requires a detailed
knowledge of the internal program design and code.</p>
</blockquote>
<p>In my world, where everything is open source we testers can learn how the SUT and its
classes and methods work and we can also write pure unit tests. For example in
<a href="https://github.com/weldr/codec-rpm/commit/308c083afbe6f2f2ba64c83433d6a0262a5ab44c">codec-rpm</a>
I had the pleasure to add very pure unit tests - call a function and assert on its result,
nothing else in the system state changed (that's how the software was designed to work)!</p>
<p><strong>Important:</strong></p>
<p>Next questions ask about <em>how to ... unit test ...</em> and the term "unit test" in them is
used wrongly! I will drop this and only use "test" to answer!</p>
<p>Also important - <strong>make the difference between unit type test and another type of
test written with a unit testing framework</strong>! In most popular programming languages unit
testing frameworks are very powerful! They can automatically discover your test suite (discovery),
execute it (test runner), provide tooling for asserting conditions (equal, not equal, True,
has changed, etc) and tooling for reporting on the results (console log, HTML, etc).</p>
<p>For example Kiwi TCMS is a Django application and it uses the standard test framework
from Django which derives from Python's unittest! A tester can use pretty much any kind
of testing framework to automate pretty much any kind of test! Some frameworks just make
particular types of tests easier to implement than others.</p>
<p><strong>How to write our tests without touching the DB when almost all business logic is
contained within Active Record objects? Do we have to move this logic outside Active Record,
in pure PHP classes that don't touch DB?</strong></p>
<p>To answer the second part - it doesn't really matter. Separating logic from database is
a nicer design in general (loosely coupled) but not always feasible. Wrt testing you can either
mock calls to the database or perform your tests with the DB present.</p>
<p>For example Kiwi TCMS is a DB heavy applcation. Everything comes and goes to the
database, it hardly has any stand-alone logic. Thus the most natural way to test is together
with the database! Our framework provides tooling to load previously prepared test data
(db migrations, fixtures) and we also use <code>factoryboy</code> to speed up creation of ORM objects
only with the specific attributes that we need for the test!</p>
<p>Key here is speed and ease of development, not what is the best way in theory! In real-life
testing there are hardly any best practices IMO. Testing is always very context dependent.</p>
<p><strong>Is it good to test methods with Eloquent ORM/SQL statements and how to do it without a database?</strong></p>
<p><a href="https://laravel.com/docs/5.7/eloquent">Eloquent</a> is the ORM layer for Laravel thus the question
becomes the same as the previous one.! When the application is dependent on the DB, which in their
case is, then it makes sense to use a database during testing!</p>
<p><strong>For Feature tests isn't it better to to test them without a DB and b/c we have more business
logic there. For them we must be certain that we call the correct methods?</strong></p>
<p>Again, same as the previous one. Use the database when you have to! And two questions:</p>
<p>1) Does the database messes your testing up in some way? Does it prevent you from doing something?
   If yes, just debug the hell out of it, figure out what happens and then figure out how to
   fix it
2) What on Earth is <em>we must be certain that we call the correct methods</em> mean? (I am writing this
   as personal notes before the actual session took place). I suspect that this is the more general
   <em>am I testing for the right thing</em> question which inexperienced engineers ask. My rule of thumb
   is: check what do you assert on. Are you asserting that the record was created in the DB (so verifying
   explicitly permissions, DB setup, ORM correctness) or that the result of the operation mathes what
   the business logic expects (so verifying explicitly the expected behavior and implicitly that all
   the layers below managed to work so the data was actually written to disk)? At times both may be
   necessary (e.g. large system, lots of cachine, eventual consistency) but more often than not
   we need to actually assert on the business logic.</p>
<p>Example:</p>
<ul>
<li>technical validation: user tries to register an account, assert email was sent or</li>
<li>business/behavior validation: user tries to register an account, after confirming their intent
  they are able to login</li>
</ul>
<p><strong>Optimization for faster execution time, parallel execution</strong></p>
<p>Parallel testing is no, no, no in my book! If you do not understand why something is slow
trowing more instances at it increases your complexity and decreases the things you do
understand and subsequently are able to control and modify!</p>
<p>Check-out this excellent presentation by
<a href="https://www.youtube.com/watch?v=hbocBqOpuAo#t=3h18m25s">Emanuil Slavov</a> at
GTAC 2016. The most important thing Emanuil says is that a fast test suite is the result of many
conscious actions which introduced small improvements over time. His team had assigned
themselves the task to iteratively improve their test suite performance and at every step
of the way they analyzed the existing bottlenecks and experimented with possible solutions.</p>
<p>The steps in particular are (on a single machine):</p>
<ul>
<li>Execute tests in dedicated environment;</li>
<li>Start with empty database, not used by anything else; This also leads to
adjustments in your test suite architecture and DB setup procedures;</li>
<li>Simulate and stub external dependencies like 3rd party services;</li>
<li>Move to containers but beware of slow disk I/O;</li>
<li>Run database in memory not on disk because it is a temporary DB anyway;</li>
<li>Don't clean test data, just trash the entire DB once you're done; Will also require
  adjustments to tests, e.g. assert the actual object is there, not that there are
  now 2 objects;</li>
<li>Execute tests in parallel which should be the last thing to do!</li>
<li>Equalize workload between parallel threads for optimal performance;</li>
<li>Upgrade the hardware (RAM, CPU) aka vertical scaling; I would move this before
  parallel execution b/c test systems usually have less resources;</li>
<li>Add horizontal scaling (probably with a messaging layer);</li>
</ul>
<p>There are other more heuristical approaches like not running certain tests on
certain branches and/or using historical data to predict what and where to execute.
If you want to be fancy couple this with an ML algorithm but beware that
there are only so many companies in the world that will have any real benefit from this.
You and I probably won't. Read more about <a href="http://atodorov.org/blog/2016/11/30/highlights-from-ista-and-gtac-2016/">GTAC 2016</a>.</p>
<p><strong>Testing when touching the file system or working with 3rd party cloud providers</strong></p>
<p>If touching the filesystem is occasional and doesn't slow you down ignore it!
But also make sure you do have a fast disk, this is also true for DB access.
Try to push everything to memory, e.g. large DB buffers, filesystem mounted in memory,
all of this is very easy in Linux. Presumption here is that these are temporary objects
and you will destroy them after testing.</p>
<p>Now if the actual behavior that you want to test is working with a filesystem (e.g.
producing files on disk) or uploading files to a cloud provider there isn't much you
can do about it! This is a system type of test where you rely on <em>integration</em> with
a 3rd party solution.</p>
<p>For example for <a href="https://github.com/atodorov/django-s3-cache">django-s3-cache</a>
you need to provide your Amazon S3 authentication tokens before you can execute
the test suite. It will comminicate back and forth with AWS and possibly leave some
artifacts there when it is done!</p>
<p>Same thing for <a href="https://github.com/weldr/lorax/pull/584">lorax</a>, where the essence
of the SUT is to build Linux images ready to be deployed in the cloud! Checkout the
PR above and click the <code>View details</code> button at the bottom right to see the various
test statuses for this PR:</p>
<ul>
<li>Travis CI - pylint + unit test + some integration type tests (cli talks to API server)</li>
<li>very basic sanity tests (invoking the application cli via bash scripts). This hits
  the network to refresh with RPM package data from Fedora/CentOS repositories.</li>
<li>Jenkins jobs for AWS, Azure, OpenStack, Vmware, other (tar, Docker, stand-alone KVM).
  These will run the SUT, get busy for about 10 minutes to compose a cloud image of the
  chosen format, extract the file to a local directory, upload to the chosen cloud vendor,
  spin up a VM there and wait for it to initialize, ssh to the VM and perform final
  assertions, e.g. validating <em>it was able to boot as we expected it to</em>. This is for
  x86_64 and we need it for Power, s390x and ARM as well! I am having troubles even finding
  vendors that support all of these environments! Future releases will support even more
  cloud environments so rinse and repeat!</li>
</ul>
<p>My point is when your core functionality depends on a 3rd party provider your testing will
depend on that as well. In the above example I've had the scenario where VMs in Azure were
taking around 1hr to boot up. At the time we didn't know if that was due to us not integrating
with Azure properly (they don't use cloud-init/NetworkManager but their own code which we
had to install and configure inside the resulting Linux image) or because of infrastructure
issues. It turned out Azure was having networking trouble at the time when our team
was performing final testing before an important milestone. Sigh!</p>
<p><strong>With what tests (Feature or Unit) should I start before refactoring?</strong></p>
<p>So you know you are going to refactor something but it doesn't have [enough] tests?
How do you start? The answer will ellude most developers. You do not start by defining
the types of testing you should implement. You start with analyzing the existing behavior:
how it works, what conditions it expects, what input data, what constraints, etc. This is
very close to black-box testing techniques like decision tables, equivalence partitioning, etc
with the added bonus that you have access to the source code and can more accurately
figure out what is the actual behavior.</p>
<p>Then you write test scenarios (Given-When-Then or Actions 1, 2, 3 + expected results).
You evaluate these scenarios if they encompass all the previously identified behavior
and classify the risk assiciated with them. What if Scenario X fails after refactoring?
Cloud be the code is wrong, could be the scenario is incomplete. How does that affect
schedule, user experience, business risk (often money), etc.</p>
<p>Above is tipically the job of a true tester as illustrated by this picture from
Ingo Philipp, full presentation
<a href="https://assets.ctfassets.net/ut4a3ciohj8i/4ukPUn6tfiig8S4ASuaeoQ/670bba8e5498239a7fbbf404952beb08/Ingo_Philipp_Rediscover_Exploratory_Testing.pdf">here</a>
<img alt="'What is testing'" src="https://raw.githubusercontent.com/atodorov/qa-automation-ruby-101/master/module00/testing_knowledge_gap.png" /></p>
<p>Then and only then you sit down and figure out what types of tests are needed to
automate the identified scenarios, implement them and start refactoring.</p>
<p><strong>What are inexperienced developers missing most often when writing tests?
How to make my life easier if I am inexperienced and just starting with testing?</strong></p>
<p>See the picture above! Developers, even experienced ones have a different mind set
when they are working on fixing code or adding new features. What I've seen most oftenly is
adding tests only for happy paths/positive scenarios and not spending enough time to
evaluate and exercise all of the edge cases.</p>
<p>True 100% test coverage is impossible in practice and there are so many things that can
go wrong. Developers are typically not aware of all that because it is tipically not their
job to do it.</p>
<p>Also testing and development require different frame of mind. I myself am a tester but I do
have formal education in software engineering and regularly contribute as developer to various
projects (2000+ GitHub conributions as of late). When I revisit some tests I've written
I often find they are pointless and incorrect. This is because at the time I've been
thinking "how to make it work", not "how to test it and validate it actually works".</p>
<p>For an engineer without lots of experience in testing I would recommend to always start
with a BDD exercise. The reason is it will put you in a frame of mind to think about
expected behavior from the SUT and not think about implementation. This is the basis
for asking questions and defining good scenarios. Automation testing is a means of
expression, not a tool to find a solution to the testing problem!</p>
<p>Check-out <a href="http://atodorov.org/blog/2016/03/11/qa-switch-from-waterfall-to-bdd/">this BDD experiment I did</a>
and also the resources
<a href="https://github.com/atodorov/qa-automation-ruby-101/tree/master/module06">here</a>.</p>
<p><strong>Inside-out(Classi approach) vs Outside-in(Mockist approach)? When and why?</strong></p>
<p>These are terms associated with test driven development (TDD). A quick search reveals
<a href="https://8thlight.com/blog/georgina-mcfadyen/2016/06/27/inside-out-tdd-vs-outside-in.html">an excellent article explaining this question</a>.</p>
<blockquote>
<p>Inside Out TDD allows the developer to focus on one thing at a time.
Each entity (i.e. an individual module or single class) is created until the whole
application is built up. In one sense the individual entities could be deemed
worthless until they are working together, and wiring the system together at a
late stage may constitute higher risk. On the other hand, focussing on one entity at a time
helps parallelise development work within a team.</p>
</blockquote>
<p>This sounds to me is more suitable for less experienced teams but does require a strong
senior personel to control the deliverables and steer work in the right direction.</p>
<blockquote>
<p>Outside In TDD lends itself well to having a definable route through the system from the
very start, even if some parts are initially hardcoded.
The tests are based upon user-requested scenarios, and entities are wired together from
the beginning. This allows a fluent API to emerge and integration is proved from the start of development.
By focussing on a complete flow through the system from the start, knowledge of how different
parts of the system interact with each other is required. As entities emerge,
they are mocked or stubbed out, which allows their detail to be deferred until later.
This approach means the developer needs to know how to test interactions up front, either through
a mocking framework or by writing their own test doubles. The developer will then loop back,
providing the real implementation of the mocked or stubbed entities through new unit tests.</p>
</blockquote>
<p>I've seen this in practice in <a href="https://github.com/weldr/welder-web">welder-web</a>. This is the
web UI for the above mentioned cloud image builder. The application was developed iteratively
over the past 2 years and initially many of the screens and widgets were hard-coded.
Some of the interactions were not even existing, you click on a button and it does nothing.</p>
<p>This is more of an MVP, start-up approach, very convenient for frequent product demos
where you can demonstrate that some part of the system is now working and it shows
real data!</p>
<p>However this requires a relatively experienced team both testers and developers
and relatively well defined product vision. Individual steps (screens, interactions, components)
may not be so well defined but everybody needs to know where the product should go
so we can adjust our work and snap together.</p>
<p>As everything in testing the real answer is <em>it depends</em> and is often a mixture of the two.</p>
<p><strong>What is the difference between a double, stub, mock, fake and spy?</strong></p>
<p>These are classic unit testing terms defined by Gerard Meszaros in his book
<a href="https://amzn.to/2GYSse5">xUnit Test Patterns</a>, more precisely in
<a href="http://xunitpatterns.com/Test%20Double%20Patterns.html">Test Double Patterns</a>.
These terms are somewhat confusing and also used interchangeably in testing frameworks
so see below.</p>
<p>Background:</p>
<p>In most real-life software we have dependencies:
on other libraries, on filesystems, on database, on external API, on another class
(private and protected methods), etc.
Pure unit testing (see definition at the top) is not concerned with these because we
can't control them. Anytime we cross outside the class under test
(where the method which is unit tested is defined) we have a dependency that
we need to deal with. This may also apply to integration type tests, e.g. I don't want
to hit GitHub every time I want to test my code will not crash when we receive a
response from them.</p>
<p>From <em>xUnit Test Patterns</em></p>
<blockquote>
<p>For testing purposes we replace the real dependent component (DOC) with our <strong>Test Double</strong>.
Depending on the kind of test we are executing, we may hard-code the behavior of the Test Double
or we may configure it during the setup phase. When the SUT interacts with the Test Double,
it won't be aware that it isn't talking to the real McCoy,
but we will have achieved our goal of making impossible tests possible.</p>
</blockquote>
<p>Example: testing discount algorithm</p>
<ul>
<li>Replace the method figuring out what kind of discount the customer is eligible to with
  a hard-coded test double: e.g. -30% and validate the final price matches!</li>
<li>In another scenario use a second test double which applies 10% discount when you
  submit a coupon code. Verify the final price matches expectations!</li>
</ul>
<p>Here we don't care how the actual discount percentage is determined. This is a
dependency. We want to test that the discount is actually applied properly, e.g.
there may be 2 or 3 different discounts and only 1 applies or no discount policy
for items that are already on sale. This is what you are testing.</p>
<p><strong>Important:</strong> when the applying algorithm is tightly coupled with parts of the system
that select what types of discounts are available to the customer that means your code
needs refactoring since you will be not able to crate a test double (or it will be very hard
to do so).</p>
<blockquote>
<p>A <strong>Fake Object</strong> is a kind of Test Double that is similar to a Test Stub in many ways
including the need to install into the SUT a substitutable dependency but while a Test Stub
acts as a control point to inject indirect inputs into the SUT the Fake Object does not.
It merely provides a way for the interactions to occur in a self-consistent manner.</p>
</blockquote>
<p>Variations (see <a href="http://xunitpatterns.com/Fake%20Object.html">here</a>):</p>
<ul>
<li>Fake database;</li>
<li>In-memory database;</li>
<li>Fake web service (or fake web server in the case of Django);</li>
<li>Fake service layer;</li>
</ul>
<blockquote>
<p>Use of a <strong>Test Spy</strong> is a simple and intuitive way to implement an observation point that
exposes the indirect outputs of the SUT so they can be verified.
Before we exercise the SUT, we install a Test Spy as a stand-in for depended-on component (DOC)
used by the SUT. The Test Spy is designed to act as an <strong>observation point</strong> by recording the
method calls made to it by the SUT as it is exercised. During the result verification phase,
the test compares the actual values passed to the Test Spy by the SUT with the expected values.</p>
</blockquote>
<p><strong>Note:</strong> a test spy can be implemented via test double, exposing some of the functionality
to the test framework, e.g. expose internal log messages so we can validate them or can be
a very complex mock type of object.</p>
<p>From <em>The Art of Unit Testing</em></p>
<blockquote>
<p>A <em>stub</em> is a controllable replacement for an existing dependency (or collaborator)
 in the system. By using a stub, you can test your code without dealing with the dependency
 itself.</p>
<p>A <em>mock object</em> is a fake object in the system that decides whether the unit test
has passed or failed. It does so by verifying whether the object under test (e.g. a method)
interacted as expected with the fake object.</p>
<p>Stubs can NEVER fail a test! The asserts are aways against the class/method under test.
Mocks can fail a test! We can assert how the class/method under test interacted with
the mock.</p>
</blockquote>
<p>Example:</p>
<p>When testing a registration form, which will send a confirmation email:</p>
<ul>
<li>Checking that invalid input is not accepted - will not trigger <code>send_mail()</code> so
  we usually don't care about the dependency;</li>
<li>Checking valid input will create a new account in the DB - we stub-out <code>send_mail()</code>
  because we don't want to generate unnecessary email traffic to the outside world.</li>
<li>Checking if a banned email address/domain can register - we mock <code>send_mail()</code> so that
  we can assert that it was never called (together with other assertions that a correct
  error message was shown and no record was created in the database);</li>
<li>Checking that valid, non-banned email address can register - we mock <code>send_mail()</code> and
  later assert it was called with the actual address in question. This will verify that the
  system will attempt to deliver a confirmation email to the new user!</li>
</ul>
<p>To summarize:
- <strong>When using mocks, stubs and fake objects we should be replacing external
    dependencies of the software under test, not internal methods from the SUT!</strong>.
- Beware that many modern test framework use the singular term/class name Mock to
  refer to all of the things above. Depending on their behavior they can be true mocks
  or pure stubs.</p>
<p>More practical examples with code:</p>
<ul>
<li><a href="http://atodorov.org/blog/2014/02/27/mocking-django-auth-profile-module-without-database/">Mocking Django AUTH_PROFILE_MODULE without a Database</a></li>
<li><a href="http://atodorov.org/blog/2015/09/25/unit-testing-bad-stub-design-in-dnf/">Bad Stub Design in DNF</a></li>
<li><a href="http://atodorov.org/blog/2015/11/23/bad-stub-design-in-dnf/">Bad Stub Design in DNF, Pt.2</a></li>
<li><a href="http://atodorov.org/blog/2016/03/31/beware-of-double-stubs-in-rspec/">Beware of Double Stubs in RSpec</a></li>
</ul>
<p><strong>How do we test statistics where you have to create lots of records in different states to
make sure the aggregation algorithms work properly?</strong></p>
<p>Well there isn't much to do around this - create all the records and validate your queries!
Here the functionality is mostly filter records from the database, group and aggregate them
and display the results in table or chart form.</p>
<p>Depending on the complexity of what is displayed I may even go without actually automating
this. If we have a representative set of test data (e.g. all possible states and values)
then just make sure the generated charts and tables show the expected information.</p>
<p>In automation the only scenario I can think about is to re-implement the statistics
algorithm again! Doing a <code>select() &amp;&amp; stats()</code> and <code>assert stats(test_data) == stats()</code>
doesn't make a lot of sense becase we're using the result of one method to validate
itself! It will help discover problems with <code>select()</code> but not with the actual
calculation!</p>
<p>Once you reimplement every stats twice you will see why I tend to go for manual
testing here.</p>
<p><strong>How to test various filters and searches which need lots of data?</strong></p>
<p>First ask yourself the question - what do you need to test for?</p>
<ul>
<li>That all values from the webUI are passed down to the ORM</li>
<li>That the ORM will actually return the records in question (e.g. active really means active
  not the opposite)</li>
<li>which columns will be displayed (which is a UI thing)</li>
</ul>
<p>For Kiwi TCMS search pages we don't do any kind of automated testing! These are
very static HTML forms that pass their values to a JavaScript function which passes
them to an API call and then renders the results! When you change it you have to validate it
manually but nothing more really.</p>
<p>It is good to define test scenarios, especially based on customer bug reports but
essentially you are checking that a number of values are passed around which either
works or it doesn't. Not much logic and behavior to be tested there! <strong>Think like a tester, not
like a developer!</strong></p>
<p><strong>How to test an API? Should we use an API spec schema and assert the server side
and client side based on it?</strong></p>
<p>This is generally a good idea. The biggest troubles with APIs is that they change without
warning, sometimes in an incompatible way and clients are not aware of this. A few things you can do:</p>
<ul>
<li>Use API versioning and leave older versions arround for as long as necessary.
  Facebook for example keeps their older API versions around for several years.</li>
<li>Use some sort of contract testing/API specification to validate behavior.
  I find value here to have a test suite which explicitly exercises the external API in
  the desired ways (full coverage of what the application uses) so it can detect when
  something breaks. If this is not 100% all the time it will become useless very quickly.</li>
<li>Record and replay may be useful at scale, Twitter uses similar approach with
  anonimizing the actual values being sent around and also accounting for parameter types,
  e.g. an int X can receive only ints and if someone tries to send a string that was
  probably an error. Twitter however has access to their entire production data and can
  perform such kind of sampling.</li>
</ul>
<p><strong>What types of tests do QA people write?</strong> (I split this from the next question).</p>
<p>As should be evident by my many example nobody stops us from writing any kind of test
in any kind of programming language. This only depends on personal skills and the specifics of
the project we work on.</p>
<p>Please refer back to the codec-rpm, lorax and welder-web projects. These are components
from a larger product named Composer which builds Linux cloud images.</p>
<p>welder-web is
the front-end which integrates with Cockpit. This is written with React.js, includes some
component type tests (I think close to unit tests but I haven't worked on them), end-to-end
test suite (again JavaScript) similar to what you do with Selenium - fire up the browser
and click on widgets.</p>
<p>lorax is a Python based backend with unit and integration tests in Python. I mostly work
on testing the resulting cloud images which uses a test framework for Bash script,
ansible, Docker and a bunch of vendor specific cli/api tools.</p>
<p>codec-rpm is smaller component from another backend called BDCS which is written in Haskell.
As I showed you I've done some unit tests (and bug fixes even) and for bdcs-cli I did
work on similar cloud image tests in bash script. This component is now frozen but when/if
it picks up all the existing bash scripts will need to be ported plus any unit tests
which are missing will have to be reimplemented in Haskell. Whoever on the team is
free will get to do it.</p>
<p>At the very beginning we used to have a 3rd backend written in Rust but that was abandoned
relatively quickly.</p>
<p>To top this off a good QE person will often work on test related tooling to support their
team. I personally have worked on <a href="https://github.com/sixty-north/cosmic-ray">Cosmic-Ray</a> -
mutation testing tool for Python used by Amazon and others, I am the current maintainer of
<a href="https://github.com/PyCQA/pylint-django">pylint-django</a> - essentially a developer tool but
I like to stretch its usage with customized plugins and of course
<a href="http://kiwitcms.org">Kiwi TCMS</a> which is a test management tool.</p>
<p><strong>How do they (testers) know what classes I am going to create so they are able to
write tests for them beforehand?</strong></p>
<p>This comes from test driven development practices. In TDD (as everywhere in testing)
you will start with analisys what components are needed and how they will work.
Imagine that I want you to implement a class that represents a cash-desk which can
take money and store them, count them, etc. Imagine this is part of a banking application
where you can open accounts, transfer money between them, etc.</p>
<p>With TDD I start by implementing tests for the desired behavior. I will <code>import solution</code>
and I will create an object from the <code>Bill</code> class to represent a 5 BGN note.
I don't care how you want to name your classes! The tests serve to enforce the interface
I need you to implement: module name, classes in the module, method names, behavior.</p>
<p>Initially in TDD the tests will fail. Once functionality becomes to be implemented piece
by piece tests will start passing one by one! In TDD testers don't know, we expect developers
to do something otherwise tests fail and you can't merge!</p>
<p>In practice there is a back-and-forth process!</p>
<p>The above scenario is part of my training courses where I give students homework
assignments and I have already provided automated test suites for the classes and
modules they have to implement. Once the suite reports PASS I know the student
has at least done good enough implementation to meet the bare minimum of requirements.
See an example for the Cash-Desk and Bank-Account problems at
https://github.com/atodorov/qa-automation-python-selenium-101/tree/master/module04</p>
<p><strong>How to test functionality which is date/time dependent?</strong></p>
<p>For example a certain function should execute on week days but not on the weekend. How do we
test this? Very simple, we need to time travel, at least out tests do.</p>
<p>Check-out <a href="https://github.com/hnw/php-timecop">php-timecop</a> and
<a href="https://blog.trikoder.net/stub-php-date-and-the-crew-with-php-timecop-9a64a7d3b239">this introductory article</a>.
Now that we know what stubs are we simply use a suitable library and stub out
date/time utilities. This essentially gives you the ability to freeze the
system clock or time travel backwards and forwards in time so you can execute
your tests in the appropriate environment. There are many such time-travel/time-freeze
libraries for all popular programming languages.</p>
<p><strong>Given the two variations of the method below:</strong></p>
<div class="highlight"><pre><span class="x">public function updateStatusPaid()</span>
<span class="x">{</span>
<span class="x">    $this-&gt;update([</span>
<span class="x">        &#39;date_paid&#39; =&gt; now(),</span>
<span class="x">        &#39;status&#39; =&gt; &#39;paid&#39;</span>
<span class="x">    ]);</span>
<span class="x">}</span>

<span class="x">public function updateStatusPaid()</span>
<span class="x">{</span>
<span class="x">    $this-&gt;date_paid = now();</span>
<span class="x">    $this-&gt;status = &#39;paid&#39;;</span>
<span class="x">    $this-&gt;save();</span>
<span class="x">}</span>
</pre></div>


<p><strong>How do we create a test which validates this method without touching the database?
Also we want to be able to switch between method implementations without updating the test code!</strong></p>
<p>Let's examine this in details. Both of these methods change field values for the <code>$this</code> object
and commit that to storage! There is no indication what happened inside other than the
object fields being changed in the underlying storage medium.</p>
<p>Options:</p>
<p>1) Mock the <code>save()</code> method or spy the entire storage layer. This will give you
   faster speed of execution but more importantly will let you examine the values before
   they leave the process memory space. Your best bet here is replacing the entire
   backend portion of the ORM layer which talks to the database. Drawback is that data may not be persistent
   between test executions/different test methods (depending on how they are executed and how
   the new storage layer works) so chained tests, which depend on data created by other tests
   or other parts of the system may break.
2) Modify your method to provide more information which can be consumed by the tests. This is
   called engineering for testability. The trouble with this method is that it doesn't
   expose anything to the outside world so the only way we can check that something has
   changed is to actually fetch it from storage and assert that it is different.
3) Test with the database included. The OP presumes touching a database during testing is
   a bad thing. As I've already pointed out this is not necessarily the case. Unless your data
   is so big that it is spread around cluster nodes in several shards using a database for
   testing is probably the easiest thing you can do.</p>
<p>Now to the second part of the question: if your test is not tightly coupled with the method
implementation then it will not need to be changed once you change the implementation. That is
if you are asserting on independent system state then you should be fine.</p>
<h2>Current problems</h2>
<p>This is a list of problems we discussed, my views on them and similar items I've seen in the past.
They are valid across the board for many types of companies and teams and my only recommendation
here is to analyze the root of your problems and act to resolve them. IMO a lot of the times
the actual problems stem from not understanding the roots of what we are trying to validate,
not from technological limitations.</p>
<p>Background:</p>
<p>Company is delivering a digital product, over e-mail, without a required login procedure.
There are event ticket sites which work like this.</p>
<p><strong>Problem: email delivery fails, customer closes their browser and they can't get back to
what they paid for. Essentially customers locks themselves out of the product they
paid for.</strong></p>
<p>This is UX problem. Email is inherently unreliable and it can break at many steps along
the way. The product is not designed to be fault tolerant and to provide a way for the customer
to retrieve their digital products. Options include:</p>
<ul>
<li>Browser cookies to remember orders in the last X days</li>
<li>Well designed error/warning messages about possible data loss</li>
<li>Require login (email or social) or other means of backup delivery (mobile phone,
  second email address, etc)</li>
<li>Login is sometimes required by regulatory bodies (KYC practices) and is also a
  good starting point for additional marketting/relationship building activities</li>
<li>Monitoring of email delivery providers and their operation. This is a business critical
  functionality so it must be treated like that.</li>
</ul>
<p>Product needs enough input data from customer to produce a deliverable.
<strong>Problem: Sometimes <em>enough</em> may not be enough, that is the backend algorithm thinks it hass everything
and then it runs into some corner case from which it can't recover and is not able to
deliver its results to the customer.</strong></p>
<p>I see this essentially as an UX problem:</p>
<ul>
<li>Ask customer for more info at the beginning - annoying, slows down initial
  product adoption, may break the conversion funnel;</li>
<li>Calculate what we can and randomly pick options from DB (curated or based on statistics)
  and present them to customer;</li>
<li>Previous point + allow the customer to proceed or go back and refine the selection
  which was automatically made for them - this is managing the UX experience around
  the technological limitations</li>
</ul>
<p><strong>Infrastructure problems: site doesn't open (not accessible for some reason), big email queue,
many levels of cache (using varnish)</strong></p>
<p>Agressive monitoring of all of these items with alerts and combined charts. This is business
critical functionality and we need to always know what is the status of it. If you want to
be fancy couple this with an ML/AI algorithm which will predict failures in advance so you
can be on alert before that happens.</p>
<p>More importantly each problem in production must be followed by a post-mortem session (more on that later).</p>
<p><strong>Integration with payment processors: how do you test this in production ?</strong></p>
<p>Again agressive monitoring when/if these integrations are up and running, then:</p>
<p>Design a small test suite which goes directly on the website and examines if all payment options
are available. This will catch scenarios where you claim PayPal is supported but for some
reason the form didn't load. The problem may not be on your side! Check preferences per
country (may have been editted by admin on the backend), make sure what you promised is
always there.</p>
<p>I've used similar approach in a trading startup. We run the suite once every hour directly
agains prod. Results were charted in Grafana together with the rest of the monitoring metrics.
In the first two days we found that the HTML form provided by the payment processor was changing
all the time - this was supposed to be stable. In the first week we discovered the payment
processor had issues on their own and were down for couple of hours during the night our time zone.</p>
<p>There isn't much you can do when you rely on 3rd party services but you can either
- cache and retry later, masking the backend failures from the user at your own risk (payment may not be authorized later)
- do not accept payment or at least warn the customer if you are seeing/predicting 3rd party issues</p>
<p><strong>Problem: customers cancelling their payments after product was received</strong></p>
<p>Yes, in many countries you can do so many days after you paid and got access to something.
I have done so myself after non-delivery of items.</p>
<p>In case this is deliberate action from the customer there isn't much you can do. In case it is
because they were frustrated due to problems overzealous monitoring and communicating back to
the customers will probably help.</p>
<p><strong>Localization problems, missing translations, UI doesn't look good, missing images</strong></p>
<p>Unless your test team speaks the language they can't understand shit. Best options IMO:</p>
<ul>
<li>Allow translator team to preview their work before it is comitted to the current version;
  A simple staging server will work for this. This is easy to integrate with any translation
  system;</li>
<li>Use machine checks: missing format strings, unfilled data (e.g. missing translations),
  404 URLs. This is cheap to execute and can be done on Save and provide immediate feedback;</li>
<li>Many systems provide the option to Review &amp; Approve the work of another peer;</li>
<li>Some visual testing tools (I don't have much experience here but I know they exist) which
  will detect strings that are too long and do not fit inside buttons and other widgets.
  This is more in the category of visual layout testing.</li>
</ul>
<p><strong>Problem: on mobile version, after new feature was added the 'Buy' button was overlayed
by another widget and was not visible</strong></p>
<p>This means that:</p>
<ul>
<li>previously it was not defined what testing will be performed for the new feature;</li>
<li>also that this 'Buy' button was not considered business critical functionality,
  which it is;</li>
<li>the person who signed-off on this page was careless;</li>
</ul>
<p>Test management tools like <a href="http://kiwitcms.org">Kiwi TCMS</a> can help you with organizing
and documenting what needs to be tested. However, regardless of the system used, everything
starts with identifying which functionality is critical and must always be present! This is
the job of a tester!</p>
<p>Once identified as critical you could probably use some tools for visual comparison to
make sure this button is always available on this (other) pages. Again a person must
identify all the possible interactions we want to check for.</p>
<p><strong>Problem: we released at 18:30 on Friday and went home. We discovered email delivery
was broken at 10:00 the next day</strong></p>
<p>Obviously this wasn't well tested since it broke. The root cause must be analized and
a test for it added.</p>
<p>Also we are missing a minitoring metric here. If you are sending lots of emails then
a drop under, say 50K/hour probably means problems! What's the reason the existing monitoring
tools didn't trigger? Investigate and fix it.</p>
<p>Last - do not <em>push &amp; throw over the fence</em>. This is the silo mentality of the past.
A small team can allow itself to make these mistakes just a few times, then comapny goes out of
business and the people who didn't care enough to resolve the problems go out of a job.</p>
<p>Make a policy which gives you enough time to monitor production and revert in case of
problems. There are many reasons lots of companies don't release on Friday (while others do).
The point here is to put the policy and entire machinery in place so you can deal with
problems when they arise. If you are not equipped to deal with these problems
on late Friday night (or any other day/night of the week) you should not be making releases then.</p>
<p><strong>Problem: how do we follow-up after a blunder?</strong></p>
<p>In any growing team or company, especially a startup there is more demand to work on new
features than maintain existing code, resolve problems or work on non-visible items like
testing and monitoring which will help you the next time there are problems.</p>
<p><img alt="Swiss cheeze model framwork" src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/e8/Swiss_cheese_model_of_accident_causation.png/330px-Swiss_cheese_model_of_accident_causation.png" /></p>
<p>An evaluation framework like the
<a href="https://en.wikipedia.org/wiki/Swiss_cheese_model">Swiss cheese model</a> is a good place
to start. Prezi uses it extensively. Various sized holes are the different root causes which will lead to a problem:</p>
<ul>
<li>missing tests</li>
<li>undocumented release procedure</li>
<li>merged without code review</li>
<li>incomplete feature specification</li>
<li>too much work, task overload</li>
</ul>
<p>The cheese layers can be both technical and organizational. One of them can be
the business takeholders organization: wanting too much, not budgeting time for other
tasks, tight marketting schedule, etc.</p>
<p>Once a post-mortem is held and the issues at hand analyzed you need to come up
with a plan of action. These are your JIRA tickets about what to do next.
Some will have immediate priority others will be important 1 year from now.
Once the action items are entered into your task tracking software the only thing left
to do is priritizing them accordingly.</p>
<p><strong>Important:</strong> tests, monitoring, even talking about a post-mortem and other seemingly
non-visible tasks are still important. If the business doesn't budget time for their
completion it will ultimately fail! You can not sustain adding new features quickly
for an extended period of time without taking the time to resolve your architecture,
infrastructure, team and who knows what other issues.</p>
<p>Time and resources should be evaluated and assigned according to the importance of the task
and the various risks assiciated with it. This is no different from when we do
planning for new features. Consider having the ability to analyze, adapt and resolve
problems as the most important feature of your organization!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Fri 05 April 2019
            </p>
<p>There are <a href="http://atodorov.org/blog/2019/04/05/the-art-of-unit-testing/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2018/07/24/introducing-pylint-django-20/" rel="bookmark" title="Permalink to Introducing pylint-django 2.0">
                <h2 class="post-title">
                    Introducing pylint-django 2.0
                </h2>
            </a>
                <p>Today I have released pylint-django version 2.0 on PyPI.
The changes are centered around compatibility with the latest pylint 2.0 and
astroid 2.0 versions. I've also bumped pylint-django's version number to reflact
that.</p>
<p>A major component, class transformations, was updated so don't be surprised if
there are bugs. All the existing test cases pass but you never know what sort
of edge case there could be.</p>
<p>I'm also hosting a workshop/corporate training about writing pylint plugins.
If you are interested see this <a href="http://MrSenko.com/pylint-workshop/">page</a>!</p>
<p>Thanks for reading and happy testing!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Tue 24 July 2018
            </p>
<p>There are <a href="http://atodorov.org/blog/2018/07/24/introducing-pylint-django-20/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2018/07/06/upstream-rebuilds-with-jenkins-job-builder/" rel="bookmark" title="Permalink to Upstream rebuilds with Jenkins Job Builder">
                <h2 class="post-title">
                    Upstream rebuilds with Jenkins Job Builder
                </h2>
            </a>
                <p>I have been working on <a href="http://weldr.io/">Weldr</a> for some time now.
It is a multi-component software with several layers built on top of
each other as seen on the image below.</p>
<p><img alt="Weldr components" src="/images/welder_upstream.png" /></p>
<p>One of the risks that we face is introducing changes in
downstream components which are going to break something up the stack!
In this post I am going to show you how I have configured
Jenkins to trigger dependent rebuilds and report all of the statuses
back to the original GitHub PR. All of the code below is Jenkins Job Builder
yaml.</p>
<p><code>bdcs</code> is the first layer of our software stack. It provides command line
utilities. <code>codec-rpm</code> is a library component that facilitates working
with RPM packages (in Haskell). <code>bdcs</code> links to <code>codec-rpm</code> when it is compiled,
<code>bdcs</code> uses some functions and data types from <code>codec-rpm</code>.</p>
<p>When a pull request is opened against <code>codec-rpm</code> and testing completes successfully
I want to reuse that particular version of the <code>codec-rpm</code> library and
rebuild/test <code>bdcs</code> with that.</p>
<h2>YAML configuration</h2>
<p>All jobs have the following structure: -trigger -&gt; -provision -&gt; -runtest -&gt; -teardown.
This means that Jenkins will start executing a new job when it gets triggered by
an event in GitHub (commit to master branch or new pull request), then it will
provision a slave VM in OpenStack, execute the test suite on the slave and destroy
all of the resources at the end. This is repeated twice: for master branch and for
pull requests! Here's how the -runtest jobs look:</p>
<div class="highlight"><pre><span class="p-Indicator">-</span> <span class="l-Scalar-Plain">job-template</span><span class="p-Indicator">:</span>
    <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="s">&#39;{name}-provision&#39;</span>
    <span class="l-Scalar-Plain">node</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">master</span>
    <span class="l-Scalar-Plain">parameters</span><span class="p-Indicator">:</span>
      <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">string</span><span class="p-Indicator">:</span>
          <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">PROVIDER</span>
    <span class="l-Scalar-Plain">scm</span><span class="p-Indicator">:</span>
        <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">git</span><span class="p-Indicator">:</span>
            <span class="l-Scalar-Plain">url</span><span class="p-Indicator">:</span> <span class="s">&#39;https://github.com/weldr/{repo_name}.git&#39;</span>
            <span class="l-Scalar-Plain">refspec</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">${{git_refspec}}</span>
            <span class="l-Scalar-Plain">branches</span><span class="p-Indicator">:</span>
              <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">${{git_branch}}</span>
    <span class="l-Scalar-Plain">builders</span><span class="p-Indicator">:</span>
      <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">github-notifier</span>
      <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">shell</span><span class="p-Indicator">:</span> <span class="p-Indicator">|</span>
            <span class="no">#!/bin/bash -ex</span>
            <span class="no"># do the openstack provisioning here</span>
        <span class="c1"># NB: runtest_job is passed to us via the -trigger job</span>
      <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">trigger-builds</span><span class="p-Indicator">:</span>
          <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">project</span><span class="p-Indicator">:</span> <span class="s">&#39;${{runtest_job}}&#39;</span>
            <span class="l-Scalar-Plain">block</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">true</span>
            <span class="l-Scalar-Plain">current-parameters</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">true</span>
            <span class="l-Scalar-Plain">condition</span><span class="p-Indicator">:</span> <span class="s">&#39;SUCCESS&#39;</span>
            <span class="l-Scalar-Plain">fail-on-missing</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">true</span>


<span class="p-Indicator">-</span> <span class="l-Scalar-Plain">job-template</span><span class="p-Indicator">:</span>
    <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="s">&#39;{name}-master-runtest&#39;</span>
    <span class="l-Scalar-Plain">node</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">cinch-slave</span>
    <span class="l-Scalar-Plain">project-type</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">freestyle</span>
    <span class="l-Scalar-Plain">description</span><span class="p-Indicator">:</span> <span class="s">&#39;Build</span><span class="nv"> </span><span class="s">master</span><span class="nv"> </span><span class="s">branch</span><span class="nv"> </span><span class="s">of</span><span class="nv"> </span><span class="s">{name}!&#39;</span>
    <span class="l-Scalar-Plain">scm</span><span class="p-Indicator">:</span>
        <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">git</span><span class="p-Indicator">:</span>
            <span class="l-Scalar-Plain">url</span><span class="p-Indicator">:</span> <span class="s">&#39;https://github.com/weldr/{repo_name}.git&#39;</span>
            <span class="l-Scalar-Plain">branches</span><span class="p-Indicator">:</span>
                <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">master</span>
    <span class="l-Scalar-Plain">builders</span><span class="p-Indicator">:</span>
      <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">github-notifier</span>
      <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">conditional-step</span><span class="p-Indicator">:</span>
          <span class="l-Scalar-Plain">condition-kind</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">regex-match</span>
          <span class="l-Scalar-Plain">regex</span><span class="p-Indicator">:</span> <span class="s">&quot;^.+$&quot;</span>
          <span class="l-Scalar-Plain">label</span><span class="p-Indicator">:</span> <span class="s">&#39;${{UPSTREAM_BUILD}}&#39;</span>
          <span class="l-Scalar-Plain">on-evaluation-failure</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">dont-run</span>
          <span class="l-Scalar-Plain">steps</span><span class="p-Indicator">:</span>
            <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">copyartifact</span><span class="p-Indicator">:</span>
                <span class="l-Scalar-Plain">project</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">${{UPSTREAM_BUILD}}</span>
                <span class="l-Scalar-Plain">which-build</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">specific-build</span>
                <span class="l-Scalar-Plain">build-number</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">${{UPSTREAM_BUILD_NUMBER}}</span>
                <span class="l-Scalar-Plain">filter</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">${{UPSTREAM_ARTIFACT}}</span>
                <span class="l-Scalar-Plain">flatten</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">true</span>
      <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">shell</span><span class="p-Indicator">:</span> <span class="p-Indicator">|</span>
            <span class="no">#!/bin/bash -ex</span>
            <span class="no">make ci</span>
    <span class="l-Scalar-Plain">publishers</span><span class="p-Indicator">:</span>
      <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">trigger-parameterized-builds</span><span class="p-Indicator">:</span>
          <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">project</span><span class="p-Indicator">:</span> <span class="s">&#39;{name}-teardown&#39;</span>
            <span class="l-Scalar-Plain">current-parameters</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">true</span>
      <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">github-notifier</span>


<span class="p-Indicator">-</span> <span class="l-Scalar-Plain">job-template</span><span class="p-Indicator">:</span>
    <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="s">&#39;{name}-PR-runtest&#39;</span>
    <span class="l-Scalar-Plain">node</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">cinch-slave</span>
    <span class="l-Scalar-Plain">description</span><span class="p-Indicator">:</span> <span class="s">&#39;Build</span><span class="nv"> </span><span class="s">PRs</span><span class="nv"> </span><span class="s">for</span><span class="nv"> </span><span class="s">{name}!&#39;</span>
    <span class="l-Scalar-Plain">scm</span><span class="p-Indicator">:</span>
        <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">git</span><span class="p-Indicator">:</span>
            <span class="l-Scalar-Plain">url</span><span class="p-Indicator">:</span> <span class="s">&#39;https://github.com/weldr/{repo_name}.git&#39;</span>
            <span class="l-Scalar-Plain">refspec</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">+refs/pull/*:refs/remotes/origin/pr/*</span>
            <span class="l-Scalar-Plain">branches</span><span class="p-Indicator">:</span>
                <span class="c1"># builds the commit hash instead of a branch</span>
                <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">${{ghprbActualCommit}}</span>
    <span class="l-Scalar-Plain">builders</span><span class="p-Indicator">:</span>
      <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">github-notifier</span>
      <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">shell</span><span class="p-Indicator">:</span> <span class="p-Indicator">|</span>
            <span class="no">#!/bin/bash -ex</span>
            <span class="no">make ci</span>
      <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">conditional-step</span><span class="p-Indicator">:</span>
          <span class="l-Scalar-Plain">condition-kind</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">current-status</span>
          <span class="l-Scalar-Plain">condition-worst</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">SUCCESS</span>
          <span class="l-Scalar-Plain">condition-best</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">SUCCESS</span>
          <span class="l-Scalar-Plain">on-evaluation-failure</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">dont-run</span>
          <span class="l-Scalar-Plain">steps</span><span class="p-Indicator">:</span>
            <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">shell</span><span class="p-Indicator">:</span> <span class="p-Indicator">|</span>
                <span class="no">#!/bin/bash -ex</span>
                <span class="no">make after_success</span>
    <span class="l-Scalar-Plain">publishers</span><span class="p-Indicator">:</span>
      <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">archive</span><span class="p-Indicator">:</span>
          <span class="l-Scalar-Plain">artifacts</span><span class="p-Indicator">:</span> <span class="s">&#39;{artifacts_path}&#39;</span>
          <span class="l-Scalar-Plain">allow-empty</span><span class="p-Indicator">:</span> <span class="s">&#39;{artifacts_empty}&#39;</span>
      <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">conditional-publisher</span><span class="p-Indicator">:</span>
          <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">condition-kind</span><span class="p-Indicator">:</span> <span class="s">&#39;{execute_dependent_job}&#39;</span>
            <span class="l-Scalar-Plain">on-evaluation-failure</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">dont-run</span>
            <span class="l-Scalar-Plain">action</span><span class="p-Indicator">:</span>
              <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">trigger-parameterized-builds</span><span class="p-Indicator">:</span>
                <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">project</span><span class="p-Indicator">:</span> <span class="s">&#39;{dependent_job}&#39;</span>
                  <span class="l-Scalar-Plain">current-parameters</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">true</span>
                  <span class="l-Scalar-Plain">predefined-parameters</span><span class="p-Indicator">:</span> <span class="p-Indicator">|</span>
                    <span class="no">UPSTREAM_ARTIFACT={artifacts_path}</span>
                    <span class="no">UPSTREAM_BUILD=${{JOB_NAME}}</span>
                    <span class="no">UPSTREAM_BUILD_NUMBER=${{build_number}}</span>
                  <span class="l-Scalar-Plain">condition</span><span class="p-Indicator">:</span> <span class="s">&#39;SUCCESS&#39;</span>
      <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">trigger-parameterized-builds</span><span class="p-Indicator">:</span>
          <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">project</span><span class="p-Indicator">:</span> <span class="s">&#39;{name}-teardown&#39;</span>
            <span class="l-Scalar-Plain">current-parameters</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">true</span>
      <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">github-notifier</span>


<span class="p-Indicator">-</span> <span class="l-Scalar-Plain">job-group</span><span class="p-Indicator">:</span>
    <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="s">&#39;{name}-tests&#39;</span>
    <span class="l-Scalar-Plain">jobs</span><span class="p-Indicator">:</span>
    <span class="p-Indicator">-</span> <span class="s">&#39;{name}-provision&#39;</span>
    <span class="p-Indicator">-</span> <span class="s">&#39;{name}-teardown&#39;</span>
    <span class="p-Indicator">-</span> <span class="s">&#39;{name}-master-trigger&#39;</span>
    <span class="p-Indicator">-</span> <span class="s">&#39;{name}-master-runtest&#39;</span>
    <span class="p-Indicator">-</span> <span class="s">&#39;{name}-PR-trigger&#39;</span>
    <span class="p-Indicator">-</span> <span class="s">&#39;{name}-PR-runtest&#39;</span>


<span class="p-Indicator">-</span> <span class="l-Scalar-Plain">job</span><span class="p-Indicator">:</span>
    <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="s">&#39;codec-rpm-rebuild-bdcs&#39;</span>
    <span class="l-Scalar-Plain">node</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">master</span>
    <span class="l-Scalar-Plain">project-type</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">freestyle</span>
    <span class="l-Scalar-Plain">description</span><span class="p-Indicator">:</span> <span class="s">&#39;Rebuild</span><span class="nv"> </span><span class="s">bdcs</span><span class="nv"> </span><span class="s">after</span><span class="nv"> </span><span class="s">codec-rpm</span><span class="nv"> </span><span class="s">PR!&#39;</span>
    <span class="l-Scalar-Plain">scm</span><span class="p-Indicator">:</span>
        <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">git</span><span class="p-Indicator">:</span>
            <span class="l-Scalar-Plain">url</span><span class="p-Indicator">:</span> <span class="s">&#39;https://github.com/weldr/codec-rpm.git&#39;</span>
            <span class="l-Scalar-Plain">refspec</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">+refs/pull/*:refs/remotes/origin/pr/*</span>
            <span class="l-Scalar-Plain">branches</span><span class="p-Indicator">:</span>
                <span class="c1"># builds the commit hash instead of a branch</span>
                <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">${ghprbActualCommit}</span>
    <span class="l-Scalar-Plain">builders</span><span class="p-Indicator">:</span>
      <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">github-notifier</span>
      <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">trigger-builds</span><span class="p-Indicator">:</span>
          <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">project</span><span class="p-Indicator">:</span> <span class="s">&#39;bdcs-master-trigger&#39;</span>
            <span class="l-Scalar-Plain">block</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">true</span>
            <span class="l-Scalar-Plain">predefined-parameters</span><span class="p-Indicator">:</span> <span class="p-Indicator">|</span>
                <span class="no">UPSTREAM_ARTIFACT=${UPSTREAM_ARTIFACT}</span>
                <span class="no">UPSTREAM_BUILD=${UPSTREAM_BUILD}</span>
                <span class="no">UPSTREAM_BUILD_NUMBER=${UPSTREAM_BUILD_NUMBER}</span>
    <span class="l-Scalar-Plain">publishers</span><span class="p-Indicator">:</span>
      <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">github-notifier</span>


<span class="p-Indicator">-</span> <span class="l-Scalar-Plain">project</span><span class="p-Indicator">:</span>
    <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">codec-rpm</span>
    <span class="l-Scalar-Plain">dependent_job</span><span class="p-Indicator">:</span> <span class="s">&#39;{name}-rebuild-bdcs&#39;</span>
    <span class="l-Scalar-Plain">execute_dependent_job</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">always</span>
    <span class="l-Scalar-Plain">artifacts_path</span><span class="p-Indicator">:</span> <span class="s">&#39;dist/{name}-latest.tar.gz&#39;</span>
    <span class="l-Scalar-Plain">artifacts_empty</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">false</span>
    <span class="l-Scalar-Plain">jobs</span><span class="p-Indicator">:</span>
      <span class="p-Indicator">-</span> <span class="s">&#39;{name}-tests&#39;</span>
</pre></div>


<h2>Publishing artifacts</h2>
<p><code>make after_success</code> is responsible for creating a tarball if <code>codec-rpm</code> test suite
passed. This tarball gets uploaded as artifact into Jenkins and we can make use of it later!</p>
<p>Inside -master-runtest I have a <code>conditional-step</code> inside the <code>builders</code> section which
will copy the artifacts from the previous build if they are present. Notice that I copy
artifacts for a particular job number, which is the job for codec-rpm PR.</p>
<p>Making use of local artifacts is handled inside bdcs' <code>make ci</code> because it is
per-project specific and because I'd like to reuse my YAML templates.</p>
<h2>Reporting statuses to GitHub</h2>
<p>For <code>github-notifier</code> to be able to report statuses back to the pull request
the job needs to be configured with the git repository this pull request came from.
This is done by specifying the same <code>scm</code> section for all jobs that are related and
<code>current-parameters: true</code> to pass the revision information to the other jobs.</p>
<p>This also means that if I want to report status from <code>codec-rpm-rebuild-bdcs</code> then
it needs to be configured for the <code>codec-rpm</code> repository (see yaml) but somehow
it should trigger jobs for another repository!</p>
<p>When jobs are started via <code>trigger-parameterized-builds</code> their statuses are reported
separately to GitHub. When they are started via <code>trigger-builds</code> there should be only
one status reported.</p>
<h2>Trigger chain for dependency rebuilds</h2>
<p>With all of the above info we can now look at the <code>codec-rpm-rebuild-bdcs</code> job.</p>
<ul>
<li>It is configured for the codec-rpm repository so it will report its status to the PR</li>
<li>It is conditionally started after <code>codec-rpm-PR-runtest</code> finishes successfully</li>
<li>It triggers <code>bdcs-master-trigger</code> which in turn will rebuild &amp; retest the bdcs component.
  Additional parameters specify whether we're going to use locally built artifacts or
  attempt to download then from Hackage</li>
<li>It uses <code>block: true</code> so that the status of <code>codec-rpm-rebuild-bdcs</code> is dependent
  on the status of <code>bdcs-master-runtest</code> (everything in the job chain uses <code>block: true</code> because of this)</li>
</ul>
<h2>How this looks like in practice</h2>
<p>I have opened <a href="https://github.com/weldr/codec-rpm/pull/39">codec-rpm #39</a>
to validate my configuration. The chain of jobs that gets executed in Jenkins is:</p>
<div class="highlight"><pre><span class="gd">--- console.log for bdcs-master-runtest ---</span>
Started by upstream project &quot;bdcs-jslave-1-provision&quot; build number 267
originally caused by:
 Started by upstream project &quot;bdcs-master-trigger&quot; build number 133
 originally caused by:
  Started by upstream project &quot;codec-rpm-rebuild-bdcs&quot; build number 25
  originally caused by:
   Started by upstream project &quot;codec-rpm-PR-runtest&quot; build number 77
   originally caused by:
    Started by upstream project &quot;codec-rpm-jslave-1-provision&quot; build number 178
    originally caused by:
     Started by upstream project &quot;codec-rpm-PR-trigger&quot; build number 118
     originally caused by:
      GitHub pull request #39 of commit b00c923065e367afd5b7a7cc068b049bb1ed25e1, no merge conflicts.
</pre></div>


<p>Statuses are reported on GitHub as follows:</p>
<p><img alt="example of PR statuses" src="/images/codec-rpm-pr-39.png" /></p>
<p><code>default</code> is coming from the provisioning step and I think this is some sort of a bug
or misconfiguration of the provisioning job. We don't really care about this.</p>
<p>On the picture you can see that <code>codec-rpm-PR-runtest</code> was successful but
<code>codec-rpm-rebuild-bdcs</code> was not. The actual error when compiling bdcs is:</p>
<div class="highlight"><pre><span class="n">src</span><span class="o">/</span><span class="n">BDCS</span><span class="o">/</span><span class="n">Import</span><span class="o">/</span><span class="n">RPM</span><span class="p">.</span><span class="nl">hs</span><span class="p">:</span><span class="mi">110</span><span class="o">:</span><span class="mi">24</span><span class="o">:</span> <span class="nl">error</span><span class="p">:</span>
    <span class="o">*</span> <span class="n">Couldn</span><span class="err">&#39;</span><span class="n">t</span> <span class="n">match</span> <span class="n">type</span> <span class="err">`</span><span class="n">Entry</span><span class="err">&#39;</span> <span class="n">with</span> <span class="err">`</span><span class="n">C8</span><span class="p">.</span><span class="n">ByteString</span><span class="err">&#39;</span>
      <span class="n">Expected</span> <span class="nl">type</span><span class="p">:</span> <span class="n">conduit</span><span class="o">-</span><span class="mf">1.2.13.1</span><span class="o">:</span><span class="n">Data</span><span class="p">.</span><span class="n">Conduit</span><span class="p">.</span><span class="n">Internal</span><span class="p">.</span><span class="n">Conduit</span><span class="p">.</span><span class="n">ConduitM</span>
                       <span class="n">C8</span><span class="p">.</span><span class="n">ByteString</span>
                       <span class="n">Data</span><span class="p">.</span><span class="n">Void</span><span class="p">.</span><span class="n">Void</span>
                       <span class="n">Data</span><span class="p">.</span><span class="n">ContentStore</span><span class="p">.</span><span class="n">CsMonad</span>
                       <span class="p">([</span><span class="n">T</span><span class="p">.</span><span class="n">Text</span><span class="p">],</span> <span class="p">[</span><span class="n">Maybe</span> <span class="n">ObjectDigest</span><span class="p">])</span>
        <span class="n">Actual</span> <span class="nl">type</span><span class="p">:</span> <span class="n">conduit</span><span class="o">-</span><span class="mf">1.2.13.1</span><span class="o">:</span><span class="n">Data</span><span class="p">.</span><span class="n">Conduit</span><span class="p">.</span><span class="n">Internal</span><span class="p">.</span><span class="n">Conduit</span><span class="p">.</span><span class="n">ConduitM</span>
                       <span class="n">Entry</span>
                       <span class="n">Data</span><span class="p">.</span><span class="n">Void</span><span class="p">.</span><span class="n">Void</span>
                       <span class="n">Data</span><span class="p">.</span><span class="n">ContentStore</span><span class="p">.</span><span class="n">CsMonad</span>
                       <span class="p">([</span><span class="n">T</span><span class="p">.</span><span class="n">Text</span><span class="p">],</span> <span class="p">[</span><span class="n">Maybe</span> <span class="n">ObjectDigest</span><span class="p">])</span>
    <span class="o">*</span> <span class="n">In</span> <span class="n">the</span> <span class="n">second</span> <span class="n">argument</span> <span class="n">of</span> <span class="err">`</span><span class="p">(.</span><span class="o">|</span><span class="p">)</span><span class="err">&#39;</span><span class="p">,</span> <span class="n">namely</span>
        <span class="err">`</span><span class="n">getZipConduit</span>
           <span class="p">((,)</span> <span class="o">&lt;</span><span class="err">$</span><span class="o">&gt;</span> <span class="n">ZipConduit</span> <span class="n">filenames</span> <span class="o">&lt;*&gt;</span> <span class="n">ZipConduit</span> <span class="n">digests</span><span class="p">)</span><span class="err">&#39;</span>
      <span class="n">In</span> <span class="n">the</span> <span class="n">second</span> <span class="n">argument</span> <span class="n">of</span> <span class="err">`</span><span class="p">(</span><span class="err">$</span><span class="p">)</span><span class="err">&#39;</span><span class="p">,</span> <span class="n">namely</span>
        <span class="err">`</span><span class="n">src</span>
           <span class="p">.</span><span class="o">|</span>
             <span class="n">getZipConduit</span>
               <span class="p">((,)</span> <span class="o">&lt;</span><span class="err">$</span><span class="o">&gt;</span> <span class="n">ZipConduit</span> <span class="n">filenames</span> <span class="o">&lt;*&gt;</span> <span class="n">ZipConduit</span> <span class="n">digests</span><span class="p">)</span><span class="err">&#39;</span>
      <span class="n">In</span> <span class="n">the</span> <span class="n">second</span> <span class="n">argument</span> <span class="n">of</span> <span class="err">`</span><span class="p">(</span><span class="err">$</span><span class="p">)</span><span class="err">&#39;</span><span class="p">,</span> <span class="n">namely</span>
        <span class="err">`</span><span class="n">runConduit</span>
           <span class="err">$</span> <span class="n">src</span>
               <span class="p">.</span><span class="o">|</span>
                 <span class="n">getZipConduit</span>
                   <span class="p">((,)</span> <span class="o">&lt;</span><span class="err">$</span><span class="o">&gt;</span> <span class="n">ZipConduit</span> <span class="n">filenames</span> <span class="o">&lt;*&gt;</span> <span class="n">ZipConduit</span> <span class="n">digests</span><span class="p">)</span><span class="err">&#39;</span>
    <span class="o">|</span>
<span class="mi">110</span> <span class="o">|</span>                     <span class="p">.</span><span class="o">|</span> <span class="n">getZipConduit</span> <span class="p">((,)</span> <span class="o">&lt;</span><span class="err">$</span><span class="o">&gt;</span> <span class="n">ZipConduit</span> <span class="n">filenames</span>
    <span class="o">|</span>                        <span class="o">^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</span><span class="p">...</span>
</pre></div>


<p>That is because PR #39 changes the return type of <code>Codec.RPM.Conduit::payloadContentsC</code>
from <code>Entry</code> to <code>C8.ByteString</code>.</p>
<p>Thanks for reading and happy testing!</p>
<p><em>social image CC by https://pxhere.com/en/photo/226978</em></p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Fri 06 July 2018
            </p>
<p>There are <a href="http://atodorov.org/blog/2018/07/06/upstream-rebuilds-with-jenkins-job-builder/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2018/01/22/introducing-pylint-django-080/" rel="bookmark" title="Permalink to Introducing pylint-django 0.8.0">
                <h2 class="post-title">
                    Introducing pylint-django 0.8.0
                </h2>
            </a>
                <p>Since my previous post was about
<a href="http://atodorov.org/blog/2018/01/05/how-to-write-pylint-checker-plugins/">writing pylint plugins</a>
I figured I'd let you know that I've released
<a href="https://github.com/landscapeio/pylint-django">pylint-django</a> version 0.8.0
over the weekend. This release merges all pull requests which were
pending till now so make sure to read the change log.</p>
<p>Starting with this release Colin Howe and myself are the new
maintainers of this package. My immediate goal is to triage all of the
open issue and figure out if they still reproduce. If yes try to
come up with fixes for them or at least get the conversation going again.</p>
<p>My next goal is to integrate pylint-django with
<a href="http://kiwitcms.org">Kiwi TCMS</a> and start resolving all the 4000+
errors and warnings that it produces.</p>
<p>You are welcome to contribute of course. I'm also interested in hosting a
workshop on the topic of pylint plugins.</p>
<p>Thanks for reading and happy testing!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Mon 22 January 2018
            </p>
<p>There are <a href="http://atodorov.org/blog/2018/01/22/introducing-pylint-django-080/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2018/01/05/how-to-write-pylint-checker-plugins/" rel="bookmark" title="Permalink to How to write pylint checker plugins">
                <h2 class="post-title">
                    How to write pylint checker plugins
                </h2>
            </a>
                <p>In this post I will walk you through the process of learning how to write
additional checkers for pylint!</p>
<h2>Prerequisites</h2>
<ol>
<li>
<p>Read
   <a href="https://pylint.readthedocs.io/en/latest/development_guide/contribute.html">Contributing to pylint</a>
   to get basic knowledge of how to execute the test suite and how it is structured.
   Basically call <code>tox -e py36</code>. Verify that all tests <strong>PASS</strong> locally!</p>
</li>
<li>
<p>Read pylint's
   <a href="https://pylint.readthedocs.io/en/latest/how_tos/index.html">How To Guides</a>,
   in particular the section about writing a new checker. A plugin is usually
   a Python module that registers a new checker.</p>
</li>
<li>
<p>Most of pylint checkers are AST based, meaning they operate on the
   abstract syntax tree of the source code. You will have to familiarize
   yourself with the AST node reference for the <code>astroid</code> and <code>ast</code> modules.
   Pylint uses Astroid for parsing and augmenting the AST.</p>
<p><strong>NOTE:</strong> there is compact and excellent documentation provided by the
   <em>Green Tree Snakes</em> project. I would recommend the
   <a href="http://greentreesnakes.readthedocs.io/en/latest/nodes.html">Meet the Nodes</a>
   chapter.</p>
<p>Astroid also provides exhaustive documentation and
   <a href="http://astroid.readthedocs.io/en/latest/api/astroid.nodes.html">node API reference</a>.  </p>
<p><strong>WARNING:</strong> sometimes Astroid node class names don't match the ones from ast!</p>
</li>
<li>
<p>Your interactive shell weapons are <code>ast.dump()</code>, <code>ast.parse()</code>, <code>astroid.parse()</code> and
   <code>astroid.extract_node()</code>. I use them inside an interactive Python shell to
   figure out how a piece of source code is parsed and converted back to AST nodes!
   You can also try this
   <a href="https://bitbucket.org/takluyver/greentreesnakes/src/default/astpp.py?fileviewer=file-view-default">ast node pretty printer</a>!
   I personally haven't used it.</p>
</li>
</ol>
<h2>How pylint processes the AST tree</h2>
<p>Every checker class may include special methods with names
<code>visit_xxx(self, node)</code> and <code>leave_xxx(self, node)</code> where xxx is the lowercase
name of the node class (as defined by astroid). These methods are executed
automatically when the parser iterates over nodes of the respective type.</p>
<p>All of the magic happens inside such methods. They are responsible for collecting
information about the context of specific statements or patterns that you wish to
detect. The hard part is figuring out how to collect all the information you need
because sometimes it can be spread across nodes of several different types (e.g.
more complex code patterns).</p>
<p>There is a special decorator called <code>@utils.check_messages</code>. You have to list
all message ids that your <code>visit_</code> or <code>leave_</code> method will generate!</p>
<h2>How to select message codes and IDs</h2>
<p>One of the most unclear things for me is message codes. pylint
<a href="https://pylint.readthedocs.io/en/latest/how_tos/custom_checkers.html">docs</a> say</p>
<blockquote>
<blockquote>
<p>The message-id should be a 5-digit number, prefixed with a message category.
There are multiple message categories, these being <code>C</code>, <code>W</code>, <code>E</code>, <code>F</code>, <code>R</code>,
standing for <code>Convention</code>, <code>Warning</code>, <code>Error</code>, <code>Fatal</code> and <code>Refactoring</code>.
The rest of the 5 digits should not conflict with existing checkers and they
should be consistent across the checker. For instance, the first two digits should
not be different across the checker.</p>
</blockquote>
</blockquote>
<p>I'm usually having troubles with the numbering part so you will have to get creative
or look at existing checker codes.</p>
<h2>Practical example</h2>
<p>In <a href="http://kiwitcms.org">Kiwi TCMS</a> there's legacy code that looks like this:</p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">add_cases</span><span class="p">(</span><span class="n">run_ids</span><span class="p">,</span> <span class="n">case_ids</span><span class="p">):</span>
    <span class="n">trs</span> <span class="o">=</span> <span class="n">TestRun</span><span class="o">.</span><span class="n">objects</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">run_id__in</span><span class="o">=</span><span class="n">pre_process_ids</span><span class="p">(</span><span class="n">run_ids</span><span class="p">))</span>
    <span class="n">tcs</span> <span class="o">=</span> <span class="n">TestCase</span><span class="o">.</span><span class="n">objects</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">case_id__in</span><span class="o">=</span><span class="n">pre_process_ids</span><span class="p">(</span><span class="n">case_ids</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">tr</span> <span class="ow">in</span> <span class="n">trs</span><span class="o">.</span><span class="n">iterator</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">tc</span> <span class="ow">in</span> <span class="n">tcs</span><span class="o">.</span><span class="n">iterator</span><span class="p">():</span>
            <span class="n">tr</span><span class="o">.</span><span class="n">add_case_run</span><span class="p">(</span><span class="n">case</span><span class="o">=</span><span class="n">tc</span><span class="p">)</span>

    <span class="k">return</span>
</pre></div>


<p>Notice the dangling <code>return</code> statement at the end! It is useless because when missing
the default return value of this function will still be <code>None</code>. So I've decided to
create a plugin for that.</p>
<p>Armed with the knowledge above I first try the ast parser in the console:</p>
<div class="highlight"><pre><span class="n">Python</span> <span class="mf">3.6</span><span class="o">.</span><span class="mi">3</span> <span class="p">(</span><span class="n">default</span><span class="p">,</span> <span class="n">Oct</span>  <span class="mi">5</span> <span class="mi">2017</span><span class="p">,</span> <span class="mi">20</span><span class="p">:</span><span class="mi">27</span><span class="p">:</span><span class="mi">50</span><span class="p">)</span> 
<span class="p">[</span><span class="n">GCC</span> <span class="mf">4.8</span><span class="o">.</span><span class="mi">5</span> <span class="mi">20150623</span> <span class="p">(</span><span class="n">Red</span> <span class="n">Hat</span> <span class="mf">4.8</span><span class="o">.</span><span class="mi">5</span><span class="o">-</span><span class="mi">11</span><span class="p">)]</span> <span class="n">on</span> <span class="n">linux</span>
<span class="n">Type</span> <span class="s">&quot;help&quot;</span><span class="p">,</span> <span class="s">&quot;copyright&quot;</span><span class="p">,</span> <span class="s">&quot;credits&quot;</span> <span class="ow">or</span> <span class="s">&quot;license&quot;</span> <span class="k">for</span> <span class="n">more</span> <span class="n">information</span><span class="o">.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">ast</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">astroid</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">ast</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">ast</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="s">&#39;def func():</span><span class="se">\n</span><span class="s">    return&#39;</span><span class="p">))</span>
<span class="s">&quot;Module(body=[FunctionDef(name=&#39;func&#39;, args=arguments(args=[], vararg=None, kwonlyargs=[], kw_defaults=[], kwarg=None, defaults=[]), body=[Return(value=None)], decorator_list=[], returns=None)])&quot;</span>
<span class="o">&gt;&gt;&gt;</span> 
<span class="o">&gt;&gt;&gt;</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">node</span> <span class="o">=</span> <span class="n">astroid</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="s">&#39;def func():</span><span class="se">\n</span><span class="s">    return&#39;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">node</span>
<span class="o">&lt;</span><span class="n">Module</span> <span class="n">l</span><span class="o">.</span><span class="mi">0</span> <span class="n">at</span> <span class="mh">0x7f5b04621b38</span><span class="o">&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">node</span><span class="o">.</span><span class="n">body</span>
<span class="p">[</span><span class="o">&lt;</span><span class="n">FunctionDef</span><span class="o">.</span><span class="n">func</span> <span class="n">l</span><span class="o">.</span><span class="mi">1</span> <span class="n">at</span> <span class="mh">0x7f5b046219e8</span><span class="o">&gt;</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">node</span><span class="o">.</span><span class="n">body</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="o">&lt;</span><span class="n">FunctionDef</span><span class="o">.</span><span class="n">func</span> <span class="n">l</span><span class="o">.</span><span class="mi">1</span> <span class="n">at</span> <span class="mh">0x7f5b046219e8</span><span class="o">&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">node</span><span class="o">.</span><span class="n">body</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">body</span>
<span class="p">[</span><span class="o">&lt;</span><span class="n">Return</span> <span class="n">l</span><span class="o">.</span><span class="mi">2</span> <span class="n">at</span> <span class="mh">0x7f5b04621c18</span><span class="o">&gt;</span><span class="p">]</span>
</pre></div>


<p>As you can see there is a <code>FunctionDef</code> node representing the function and it has
a <code>body</code> attribute which is a list of all statements inside the function. The last
element is <code>.body[-1]</code> and it is of type <code>Return</code>! The <code>Return</code> node also has an
attribute called <code>.value</code> which is the return value! The complete code will look
like this:</p>
<div class="highlight"><span class="filename">uselessreturn.py</span><pre><span class="kn">import</span> <span class="nn">astroid</span>

<span class="kn">from</span> <span class="nn">pylint</span> <span class="kn">import</span> <span class="n">checkers</span>
<span class="kn">from</span> <span class="nn">pylint</span> <span class="kn">import</span> <span class="n">interfaces</span>
<span class="kn">from</span> <span class="nn">pylint.checkers</span> <span class="kn">import</span> <span class="n">utils</span>


<span class="k">class</span> <span class="nc">UselessReturnChecker</span><span class="p">(</span><span class="n">checkers</span><span class="o">.</span><span class="n">BaseChecker</span><span class="p">):</span>
    <span class="n">__implements__</span> <span class="o">=</span> <span class="n">interfaces</span><span class="o">.</span><span class="n">IAstroidChecker</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s">&#39;useless-return&#39;</span>

    <span class="n">msgs</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s">&#39;R2119&#39;</span><span class="p">:</span> <span class="p">(</span><span class="s">&quot;Useless return at end of function or method&quot;</span><span class="p">,</span>
                  <span class="s">&#39;useless-return&#39;</span><span class="p">,</span>
                  <span class="s">&#39;Emitted when a bare return statement is found at the end of &#39;</span>
                  <span class="s">&#39;function or method definition&#39;</span>
                  <span class="p">),</span>
        <span class="p">}</span>


    <span class="nd">@utils.check_messages</span><span class="p">(</span><span class="s">&#39;useless-return&#39;</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">visit_functiondef</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            Checks for presence of return statement at the end of a function</span>
<span class="sd">            &quot;return&quot; or &quot;return None&quot; are useless because None is the default</span>
<span class="sd">            return type if they are missing</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c"># if the function has empty body then return</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">node</span><span class="o">.</span><span class="n">body</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="n">last</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">body</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">last</span><span class="p">,</span> <span class="n">astroid</span><span class="o">.</span><span class="n">Return</span><span class="p">):</span>
            <span class="c"># e.g. &quot;return&quot;</span>
            <span class="k">if</span> <span class="n">last</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">add_message</span><span class="p">(</span><span class="s">&#39;useless-return&#39;</span><span class="p">,</span> <span class="n">node</span><span class="o">=</span><span class="n">node</span><span class="p">)</span>
            <span class="c"># e.g. &quot;return None&quot;</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">last</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">astroid</span><span class="o">.</span><span class="n">Const</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">last</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">add_message</span><span class="p">(</span><span class="s">&#39;useless-return&#39;</span><span class="p">,</span> <span class="n">node</span><span class="o">=</span><span class="n">node</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">register</span><span class="p">(</span><span class="n">linter</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;required method to auto register this checker&quot;&quot;&quot;</span>
    <span class="n">linter</span><span class="o">.</span><span class="n">register_checker</span><span class="p">(</span><span class="n">UselessReturnChecker</span><span class="p">(</span><span class="n">linter</span><span class="p">))</span>
</pre></div>


<p>Here's how to execute the new plugin:</p>
<div class="highlight"><pre><span class="nv">$ PYTHONPATH</span><span class="o">=</span>./myplugins pylint --load-plugins<span class="o">=</span>uselessreturn tcms/xmlrpc/api/testrun.py <span class="p">|</span> grep useless-return
W: 40, 0: Useless <span class="k">return</span> at end of <span class="k">function</span> or method <span class="o">(</span>useless-return<span class="o">)</span>
W:117, 0: Useless <span class="k">return</span> at end of <span class="k">function</span> or method <span class="o">(</span>useless-return<span class="o">)</span>
W:242, 0: Useless <span class="k">return</span> at end of <span class="k">function</span> or method <span class="o">(</span>useless-return<span class="o">)</span>
W:495, 0: Useless <span class="k">return</span> at end of <span class="k">function</span> or method <span class="o">(</span>useless-return<span class="o">)</span>
</pre></div>


<p><strong>NOTES:</strong></p>
<ul>
<li>
<p>If you contribute this code upstream and pylint releases it you will get a traceback:</p>
<div class="highlight"><pre>pylint.exceptions.InvalidMessageError: Message symbol &#39;useless-return&#39; is already defined
</pre></div>


<p>this means your checker has been released in the latest version and you can drop the custom
plugin!</p>
</li>
<li>
<p>This is example is fairly simple because the AST tree provides the information we
  need in a very handy way. Take a look at some of
  <a href="https://github.com/PyCQA/pylint/pulls/atodorov">my other checkers</a> to get a feeling
  of what a more complex checker looks like!</p>
</li>
<li>
<p>Write and run tests for your new checkers, especially if contributing upstream.
  Have in mind that the new checker will be executed against existing code and in
  combination with other checkers which could lead to some interesting results.
  I will leave the testing to yourself, all is written in the documentation.</p>
</li>
</ul>
<p>This particular example I've contributed as
<a href="https://github.com/PyCQA/pylint/pull/1821">PR #1821</a> which happened to contradict
an existing checker. The update, raising warnings only when there's a single return
statement in the function body, is <a href="https://github.com/PyCQA/pylint/pull/1823">PR #1823</a>.</p>
<h2>Workshop around the corner</h2>
<p>I will be working together with <a href="http://hacksoft.io">HackSoft</a> on an in-house
workshop/training for writing pylint plugins. I'm also looking at reviving
<a href="https://github.com/landscapeio/pylint-django/">pylint-django</a> so we can
write more plugins specifically for Django based projects.</p>
<p>If you are interested in workshop and training on the topic let me know!</p>
<p>Thanks for reading and happy testing!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Fri 05 January 2018
            </p>
<p>There are <a href="http://atodorov.org/blog/2018/01/05/how-to-write-pylint-checker-plugins/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/10/05/the-arcs-model-of-motivational-design/" rel="bookmark" title="Permalink to The ARCS model of motivational design">
                <h2 class="post-title">
                    The ARCS model of motivational design
                </h2>
            </a>
                <p><img alt="Motivation" src="/images/motivation.jpg" title="Motivation" /></p>
<p>The ARCS model is an instructional design method developed by John Keller
that focuses on motivation. ARCS is based on a research into best practices
and successful teachers and gives you tactics on how to evaluate your
lessons in order to build motivation right into them.</p>
<p>I have conducted and oversaw quite a few trainings and I have not been impressed
with the success rate of those so this topic is very dear to me.
Success for me measures in the ability to complete the training and
learn the basis of a technical topic. And then gather the initial
momentum to continue developing your skills within the chosen field.
This is what I've been doing for myself and this is what I'd like to
see my students do.</p>
<p>In his paper (I have a year 2000 printed copy from Cuba)
Keller argues that motivation is a product of four factors:
<strong>Attention, Relevance, Confidence and Satisfaction</strong>. You need all of them
incorporated in your lessons and learning materials for them to be motivational.
I could argue that you need the same characteristics at work in order to
motivate people to do their job as you wish.</p>
<p>Once you start a lesson you need to grab the audience <strong>Attention</strong> so they
can listen to you. Then the topic needs to be <strong>relevant</strong> to the audience
so they will continue listening to the end. This makes for a good start
but is not enough. <strong>Confidence</strong> means for the audience to feel confident
they can perform all the necessary tasks on their own, that they have
what it takes to learn (and you have to build that). If they think they
can't make it from the start then it is a lost battle. And <strong>Satisfaction</strong>
means the person feels that achievements are due to their own abilities and
hard work not due to external factors (work not demanding enough, luck, etc).</p>
<p>If all of the above 4 factors are true then the audience should feel
personally motivated to learn because they can clearly understand the
benefit for themselves and they realize that everything depends on them.</p>
<p>ARCS gives you a model to evaluate your target audience and lesson properties
and figure out tactics by which to address any shortcomings in the above 4 areas.</p>
<p>Last Friday I hosted 2 training sessions: a Python and Selenium workshop
at HackConf and then a lecture about test case management and demo of
<a href="http://kiwitcms.org">Kiwi TCMS</a> before students at Pragmatic IT academy.
For both of them I used the simplified ARCS evaluation matrix.</p>
<p>In this matrix the columns map to the ARCS areas while the rows map to
different parts of the lesson: audience, presentation media, exercise, etc.
Here's how I used them (I've mostly analyzed the audience).</p>
<h2>Python &amp; Selenium workshop</h2>
<ul>
<li>Attention<ul>
<li>(+) this is an elective workshop</li>
<li>(+) the topic is clear and the curricula is on GitHub</li>
<li>(+) the title is catchy (Learn Python &amp; Selenium in 6 hours)</li>
<li>(+) I am well known in the industry</li>
</ul>
</li>
<li>Relevance<ul>
<li>(+) Basic Python practical skills, being able to write small programs,
  knowing the basic building blocks</li>
<li>(+) Basic Selenium skills: finding and using elements</li>
<li>(+) Basic Python test automation skills: writing simple tests and asserts</li>
</ul>
</li>
<li>Confidence<ul>
<li>(+) each task has tests which need to report PASS at the end</li>
<li>(-) need to use PyCharm IDE, unfamiliar with IDEs</li>
<li>(-) not enough experience with programming or Linux</li>
<li>(-) not enough experience with (automation) testing</li>
<li>(-) all materials and exercises are in English</li>
</ul>
</li>
<li>Satisfaction<ul>
<li>(-) not being able to create a simple program</li>
</ul>
</li>
</ul>
<p>From the above it was clear that I didn't need to spend much time on building
attention or relevance. The topic itself and the fact that these are skill which
can be immediately applied at work gave the workshop a huge boost. During the
opening part of my workshop I've stated "this training takes around 2 months,
I've seen some of you forking my GitHub repo so I know you are prepared. Let's
see how much you can do in 6 hours" which sets the challenge and was my attention
building moment. Then I reiterated that all skills are directly applicable in
daily work confirming the relevance part.</p>
<p>I did need a confidence building strategy though. So having all the tests ready
meant evaluation was quick and easy. Anton (my assistant) and I promised to help
with the IDE and all other questions to counter the other items on the list.
During the course of the workshop I did quick code review of all participants
that managed to complete their tasks within the hour giving them quick tips on
how to perform or highlighting pieces of code/approaches that were different
from mine or that I found elegant or interesting. This was my confidence building
strategy. Code review and verbal praising also touches on the satisfaction
area, i.e. the participant gets the feeling they are doing well.</p>
<p>My Satisfaction building strategy was kind of mixed. Before I read about ARCS
I wanted to give penalty points to participants who didn't complete on time and then
send them home after 3 fails. At the end I only said I will do this but didn't
do it.</p>
<p>Instead I used the challenge statement from the attention phase and
turned that into a competition. The first 3 participants to complete their module tasks on time
were rewarded chocolates. With the agreement of the entire group the grand prize
was set to be a small box of the same chocolates and this would be awarded to
the person with the most chocolates (e.g. the one who's been in top 3 the most times).</p>
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Bistra is our winner. 4/5 times in top 3 <a href="https://twitter.com/hashtag/Python?src=hash&amp;ref_src=twsrc%5Etfw">#Python</a> <a href="https://twitter.com/hashtag/Selenium?src=hash&amp;ref_src=twsrc%5Etfw">#Selenium</a> <a href="https://twitter.com/hashtag/testing?src=hash&amp;ref_src=twsrc%5Etfw">#testing</a> <a href="https://twitter.com/hashtag/HC17?src=hash&amp;ref_src=twsrc%5Etfw">#HC17</a> <a href="https://t.co/vXrPhElbbW">pic.twitter.com/vXrPhElbbW</a></p>&mdash; Alexander Todorov (@atodorov_) <a href="https://twitter.com/atodorov_/status/913787872032980993?ref_src=twsrc%5Etfw">September 29, 2017</a></blockquote>

<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>I don't know if ARCS had anything to do with it but this workshop
was the most successful training I've ever done. 40% of the participants
managed to get at least one chocolate and at least 50% have completed all of
their tasks within the hour. Normally a passing rate on such training is
around 10 to 20 %.</p>
<p>During the workshop we had 5 different modules which consisted of 10-15 minutes
explanation of Python basics (e.g. loops or if conditions), quick Q&amp;A session
and around 30 minutes for working alone and code review. I don't think I was following
ARCS for each of the separate modules because I didn't have time to analyze them
individually. I gambled all my money on the introductory 10 minutes!</p>
<h2>TCMS lecture</h2>
<p>My second lecture for the day was about test case management. The audience was
students who are aspiring to become software testers and attending the
Software Testing training at Pragmatic. In my lecture (around 1 hour) I wanted
to explain what test management is, why it is important and also demo the
tool I'm working on - <a href="http://kiwitcms.org">Kiwi TCMS</a>. The analysis looks like:</p>
<ul>
<li>Attention<ul>
<li>(+) the entire training was elective but</li>
<li>(-) that particular lecture was mandatory. Students were not able to select
      what they are going to study</li>
</ul>
</li>
<li>Relevance<ul>
<li>(-) it may not be clear what TCMS is and why we need it</li>
<li>(+) however students may sense that this is something work related since
      the entire training is</li>
</ul>
</li>
<li>Confidence<ul>
<li>(-) unknown UI, generally unfamiliar workflow</li>
<li>(-) not enough knowledge how to write a Test Plan document or test cases</li>
</ul>
</li>
<li>Satisfaction<ul>
<li>(-) how to make sure new skills can be applied in practice</li>
</ul>
</li>
</ul>
<p>So I was in a medium need of a strategy to build attention. My opening was by introducing
myself to establish my professional level and introducing <a href="http://kiwitcms.org">Kiwi TCMS</a>
by saying it is the best open source test case management system to which I'm one of the
core maintainers.</p>
<p>Then I had a medium need of a relevance building strategy. I did this by explaining what
test management is and why it is important. I've talked briefly about QA managers trying to
indirectly inspire the audience to aim for this position. I finished this part by telling
the students how a TCMS system helps the ordinary guy in their daily work - namely by
giving you a dashboard where you can monitor all the work you need to do, check your
progress, etc.</p>
<p>I was in a strong need to build confidence. I did a 20-30 minutes demonstration where
I was writing a Test Plan and test cases and then pretending to execute them and marking bugs
and test results in the system. I told the students "you are my boss for today, tell me what
I need to test". So they instructed me to test the login functionality of the system
and we agreed on 5 different test cases. I described all of these into Kiwi TCMS and began
executing them. During execution I opened another browser window and did exactly what the
test case steps were asking for. There were some bugs so I promptly marked them as such and
I promised I will fix them.</p>
<p>To build satisfaction I was planning on having the students write one test plan and some
test cases but we didn't have time for this. Their instructor promised they will be doing
more exercises and using Kiwi TCMS in the next 2 months but this remains to be seen.
I've wrapped my lecture by giving advise to use Kiwi TCMS as a portfolio building tool.
Since these students are newcomers to the QA industry their next priority will be looking
for a job. I've advised them to document their test plans and test cases into Kiwi TCMS
and then present these artifacts to future employers.
I've also told them they are more than welcome to test and report bugs against Kiwi TCMS
on GitHub and add these bugs to their portfolio!</p>
<p>This is how I've applied ARCS for the first time. I like it and will continue to use it for
my trainings and workshops. I will try harder to make the application process more iterative
and apply the method not only to my opening speech but for all submodules as well!</p>
<p>One thing that bothers me is can I apply the ARCS principles when doing a technical
presentation and how do they play together or clash with storytelling, communication style and
rhetoric (all topics I'm exploring for my public speaking). If you do have more experience
with these please share it in the comments below.</p>
<p>Thanks for reading and happy testing!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Thu 05 October 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/10/05/the-arcs-model-of-motivational-design/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/10/03/storytelling-for-test-professionals/" rel="bookmark" title="Permalink to Storytelling for test professionals">
                <h2 class="post-title">
                    Storytelling for test professionals
                </h2>
            </a>
                <p>This is a very condensed brief of an 8 hour workshop I visited
earlier this year held by Huib Schoots. You can find the
<a href="https://www.dropbox.com/s/b2si3swfthtwtpi/Workshop%20Storytelling%20-%20RTC%202017%20-%20Huib%20Schoots.pdf?dl=1">slides here</a>.</p>
<p>Storytelling is the form in which people naturally communicate.
Understanding the building blocks of a story will help us
understand other people's motivations, serve as map for actions and emotions,
help uncover unknown perspectives and serve as source for inspiration.</p>
<p>Stories stand on their own and have a beginning, middle and an end. There is a
main character and a storyline with development. Stories are authentic and
personal and often provocative and evoke emotions.</p>
<h2>7 basic story plots</h2>
<ol>
<li>Overcoming the Monster</li>
<li>Rags to Riches</li>
<li>The Quest</li>
<li>Voyage and return</li>
<li>Comedy</li>
<li>Tragedy</li>
<li>Rebirth</li>
</ol>
<p>From these we can derive the following
<a href="http://annettesimmons.com/the-six-kinds-of-stories/">types of stories</a>.</p>
<h2>6 types of stories</h2>
<ol>
<li>Who am I (identity stories)</li>
<li>Why am I here (motive and mission stories)</li>
<li>Vision stories (the big picture)</li>
<li>Future scenarios (imagining the future)</li>
<li>Product stories (branding)</li>
<li>Culture stories (a sum of other stories)</li>
</ol>
<h2>12 Common Archetypes</h2>
<p>Each story needs a hero and there are
<a href="http://www.soulcraft.co/essays/the_12_common_archetypes.html">12 common archetypes</a>
of heroes. More importantly you can also find these archetypes within your team and
organization. Read the link above to find out what their motto, core desire, goals,
fears and motives are. The 12 types are</p>
<ol>
<li>Innocent</li>
<li>Everyman</li>
<li>Hero</li>
<li>Caregiver</li>
<li>Explorer</li>
<li>Rebel</li>
<li>Lover</li>
<li>Creator</li>
<li>Jester</li>
<li>Sage</li>
<li>Magician</li>
<li>Ruler</li>
</ol>
<h2>6 key elements of a story</h2>
<ol>
<li>Who's the hero?</li>
<li>What is their desire?</li>
<li>What is stopping them?</li>
<li>What is the turning point?</li>
<li>What are their insights?</li>
<li>What is the solution?</li>
</ol>
<h2>Dramatic structure and Freytag's pyramid</h2>
<p><img alt="&quot;Freytag's pyramid&quot;" src="https://upload.wikimedia.org/wikipedia/commons/a/af/Freytags_pyramid.svg" title="Freytag's pyramid" /></p>
<p>One of the most commonly used storytelling structures is the Freytag's Pyramid.
According to it each story has an exposition, rising action, climax, falling action
and resolution. I think this can be applied directly when preparing presentations
even technical ones.</p>
<h2>The Hero's journey</h2>
<p>Successful stories follow the 12 steps of the hero's journey</p>
<ol>
<li>Ordinary world</li>
<li>Call to adventure</li>
<li>Refusal of the Call</li>
<li>Meeting the mentor</li>
<li>Crossing the threshold (after which the hero enters the Special world)</li>
<li>Tests, allies and enemies</li>
<li>Approach</li>
<li>Ordeal, death &amp; rebirth</li>
<li>Rewards, seizing the sword</li>
<li>The road back (to the ordinary world)</li>
<li>Resurrection</li>
<li>Return with elixir</li>
</ol>
<p>As part of the workshop we worked in groups and created a completely made up
story. Every person in the group was contributing couple of sentences from
their own experiences, trying to describe the particular step in the hero's journey.
At the end we told a story from the point of view of a single hero which was
a complete mash-up of moments that had nothing to do with each other. Still it
sounded very realistic and plausible.</p>
<h2>Storytelling techniques</h2>
<p><a href="https://booksinbusiness.wordpress.com/2013/04/30/make-your-story-sticky-using-6-principles-s-u-c-c-e-s/">SUCCESS</a>
means Simple, Unexpected, Concrete, Credible, Emotional, Stories. To use this
technique find the core of your idea, grab people's attention by
surprising them and make sure the idea can be understood and remembered later.
Find a way to make people believe in the idea so they can test it for themselves,
make them feel something to understand why this idea is important. Tell stories and
empower people to use an idea through narrative.</p>
<p><a href="https://tweakyourslides.wordpress.com/tag/something-theyll-always-remember/">STAR</a> means
Something They will Always Remember. A STAR Moment should be
Simple, Transferable, Audience-centered, Repeatable, and Meaningful.
There are
<a href="http://www.wiley.com/legacy/email_templates/images/resonate.pdf">5 types of STAR moments</a>:
memorable dramatization, repeatable sound bites, evocative visuals,
emotive storytelling, shocking statistics.</p>
<p>To enhance our stories and presentations we should appeal to senses
(smell, sounds, sight, touch, taste) and make it visual.</p>
<p>I will be using some of these techniques combined with others in my future
presentations and workshops. I'd love to be able to summarize all of them
into a short guide targeted at IT professionals but I don't know if this
is even possible.</p>
<p>Anyway if you do try some of these techniques in your public speaking please
let me know how it goes. I want to hear what works for you and your audience
and what doesn't.</p>
<p>Thanks for reading and happy testing!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Tue 03 October 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/10/03/storytelling-for-test-professionals/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/10/02/more-tests-for-login-forms/" rel="bookmark" title="Permalink to More tests for login forms">
                <h2 class="post-title">
                    More tests for login forms
                </h2>
            </a>
                <p><img alt="&quot;Telenor's login form&quot;" src="/images/telenor_login.png" title="Telenor's login form" /></p>
<p>By now I probably have documented more test cases for login forms than anyone
else. You can check out my previous posts on the topic
<a href="http://atodorov.org/blog/2016/04/12/how-to-hire-software-testers-pt-1/">here</a> and
<a href="http://atodorov.org/blog/2017/06/14/vmwares-favorite-login-form/">here</a>. I give you a few more
examples.</p>
<p>Test 01 and 02:
First of all let's start by saying that a "Remember me" checkbox should actually
remember the user and login them automatically on the next visit if checked. The
other way around if not checked. I don't think this has been mentioned previously!</p>
<p>Test 03:
When there is a "Remember me" checkbox it should be selectable both with the mouse
and the keyboard. On my.telenor.bg the checkbox changes its image only when
clicked with the mouse. Also clicking the login button with Space doesn't work!</p>
<p>Interestingly enough when I don't select "Remember me" at all and close then
revisit the page I am still able to access the internal pages of my account!
At this point I'm not quite sure what this checkbox does!</p>
<p>Test 04:
Testing two factor authentication. I had the case where GitHub SMS didn't
arrive for over 24 hrs and I wasn't able to login. After requesting a new code
you can see the UI updating but I didn't receive another message. In this particular
case I received only one message with an already invalid code. So test for:</p>
<ul>
<li>how long does it take for the codes to expire</li>
<li>is there a visual feedback indicating how many codes have been requested</li>
<li>do latest code invalidates all the previous ones or all that have been unused
  still work</li>
<li>what happens if I'm already logged in and somebody tries to access my account
  requesting additional codes which may or may not invalidate my login session?</li>
</ul>
<p>Test 05:
Check that confirmation codes, links, etc will actually expire after their
configured time. Kiwi TCMS had this problem which has been fixed in
<a href="https://github.com/kiwitcms/Kiwi/commit/92162112bf2214b8eacf37ba3a796414b129a700#diff-353aa238f7ee459b1236e2a21f1142ba">version 3.32</a>.</p>
<p>Test 06:
Is this a social-network-login only site? Then which of my profiles did I use?
Check that there is a working
<a href="http://atodorov.org/blog/2013/03/14/django-social-auth-tip-reminder-of-login-provider/">social auth provider reminder</a>.</p>
<p>Test 07:
Check that there is an error message visible (e.g. wrong login credentials).
After the redesign Kiwi TCMS had stopped displaying this message and instead
presents the user with the login form again!</p>
<p>Also checkout these
<a href="http://testingchallenges.thetestingmap.org/index.php">testing challenges</a>
by Claudiu Draghia where you can see many cases related to input field
validation! For example empty field, value too long, special characters in field, etc.
All of these can lead to issues depending on how login is implemented.</p>
<p>Thanks for reading and happy testing!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Mon 02 October 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/10/02/more-tests-for-login-forms/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/09/08/xiaomis-selfie-bug/" rel="bookmark" title="Permalink to Xiaomi's selfie bug">
                <h2 class="post-title">
                    Xiaomi's selfie bug
                </h2>
            </a>
                <p>Recently I've been exploring the user interface of a Xiaomi Redmi Note 4X
phone and noticed a peculiar bug, adding to my collection of
<a href="http://atodorov.org/blog/2013/03/19/bug-in-nokia-software-shows-wrong-caller-id/">obscure phone bugs</a>.
Sometimes when taking selfies the images
will not be saved in the correct orientation. Instead they will be saved as
if looking in the mirror and this is a bug!</p>
<p><img alt="&quot;Samsung S5 front screen&quot;" src="/images/samsung_s5_front_screen.jpg" title="Samsung S5 front screen" /></p>
<p>While taking the selfie the display correctly acts as a mirror, see my personal
Samsung S5 (black) and the Xiaomi device (white).</p>
<p><img alt="&quot;Xiaomi front screen&quot;" src="/images/xiaomi_front_screen.jpg" title="Xiaomi front screen" /></p>
<p>However when the image is saved and then viewed through the gallery application
there is a difference. The image below is taken with the Xiaomi device and there
have been no effects added to it except scaling and cropping. As you can see
the letters on the cereal box are mirrored!</p>
<p><img alt="&quot;Xiaomi mirrored image&quot;" src="/images/xiaomi_adi_mirrored.jpeg" title="Xiaomi mirrored image" /></p>
<p>The symptoms of the bug are not quite clear as of yet. I've managed to reproduce at
around 50% rate so far. I've tried taking pictures during the day in direct sunlight
and in the shade, also in the evening under bad artificial lighting.
Taking photo of a child's face and then child plus varying number of adults.
Then photo of only 1 or more adults, heck I even made a picture of myself. I though that
lighting or the number of faces and their age have something to do with this bug
but so far I'm not getting consistent results. Sometimes the images turn out OK
and other times they don't regardless of what I take a picture of.</p>
<p>I also took a picture of the same cereal box, under the same conditions as above but
not capturing the child's face and the image came out not mirrored. The only clue
that seems to hold true so far is that you need to have people's faces in the picture
for this bug to reproduce but that isn't an edge case when taking selfies, right?</p>
<p>I've also compared the results with my Samsung S5 (Android version 6.0.1) and BlackBerry Z10 devices
and both work as expected: while taking the picture the display acts as a mirror
but when viewing the saved image it appears in normal orientation. On S5 there is
also a clearly visible "Processing" progress bar while the picture is being saved!</p>
<p>For reference the system information is below:</p>
<div class="highlight"><pre>Model number: Redmi Note 4X
Android version: 6.0 MRA58K
Android security patch level: 2017-03-01
Kernel version: 3.18.22+
</pre></div>


<p>I'd love if somebody
from Xiaomi's engineering department looks into this and sends me a root cause analysis
of the problem.</p>
<p>Thanks for reading and happy testing! Oh and btw this is my breakfast, not hers!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Fri 08 September 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/09/08/xiaomis-selfie-bug/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/08/30/speeding-up-rust-builds-inside-docker/" rel="bookmark" title="Permalink to Speeding up Rust builds inside Docker">
                <h2 class="post-title">
                    Speeding up Rust builds inside Docker
                </h2>
            </a>
                <p>Currently <a href="https://github.com/rust-lang/cargo/pull/3567">it is not possible</a>
to instruct <code>cargo</code>, the Rust package manager, to build only the dependencies
of the software you are compiling! This means you can't easily pre-install
build dependencies. Luckily you can workaround this with <code>cargo build -p</code>!
I've been using this Python script to parse <code>Cargo.toml</code>:</p>
<div class="highlight"><span class="filename">parse-cargo-toml.py</span><pre><span class="c">#!/usr/bin/env python</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">toml</span>

<span class="n">_pwd</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">__file__</span><span class="p">))</span>
<span class="n">cargo</span> <span class="o">=</span> <span class="n">toml</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">_pwd</span><span class="p">,</span> <span class="s">&#39;Cargo.toml&#39;</span><span class="p">),</span> <span class="s">&#39;r&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>

<span class="k">for</span> <span class="n">section</span> <span class="ow">in</span> <span class="p">[</span><span class="s">&#39;dependencies&#39;</span><span class="p">,</span> <span class="s">&#39;dev-dependencies&#39;</span><span class="p">]:</span>
    <span class="k">for</span> <span class="n">dep</span><span class="p">,</span> <span class="n">version</span> <span class="ow">in</span> <span class="n">cargo</span><span class="p">[</span><span class="n">section</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">print</span><span class="p">(</span><span class="s">&#39;cargo build -p </span><span class="si">%s</span><span class="s">&#39;</span> <span class="o">%</span> <span class="n">dep</span><span class="p">)</span>
</pre></div>


<p>and then inside my <code>Dockerfile</code>:</p>
<div class="highlight"><pre><span class="x">RUN mkdir /bdcs-api-rs/</span>
<span class="x">COPY parse-cargo-toml.py /bdcs-api-rs/</span>

<span class="err">#</span><span class="x"> Manually install cargo dependencies before building</span>
<span class="err">#</span><span class="x"> so we can have a reusable intermediate container.</span>
<span class="err">#</span><span class="x"> This workaround is needed until cargo can do this by itself:</span>
<span class="err">#</span><span class="x"> https://github.com/rust-lang/cargo/issues/2644</span>
<span class="err">#</span><span class="x"> https://github.com/rust-lang/cargo/pull/3567</span>
<span class="x">COPY Cargo.toml /bdcs-api-rs/</span>
<span class="x">WORKDIR /bdcs-api-rs/</span>
<span class="x">RUN python ./parse-cargo-toml.py | while read cmd; do \</span>
<span class="x">        </span><span class="p">$</span><span class="nv">cmd</span><span class="x">;                                    \</span>
<span class="x">    done</span>
</pre></div>


<p>It doesn't take into account the version constraints specified in <code>Cargo.toml</code> but
is still able to produce an intermediate docker layer which I can use to
<a href="http://atodorov.org/blog/2017/08/07/faster-travis-ci-tests-with-docker-cache/">speed-up my tests by caching the dependency compilation part</a>.</p>
<p>As seen in the <a href="https://travis-ci.org/weldr/bdcs-api-rs/builds/268489460#L1173">build log</a>,
lines 1173-1182, when doing <code>cargo build</code> it downloads and compiles <code>chrono v0.3.0</code> and
<code>toml v0.3.2</code>. The rest of the dependencies are already available. The logs also show
that after Job #285 the build times dropped from 16 minutes down to 3-4 minutes due to
Docker caching. This would be even less if the cache is kept locally!</p>
<p>Thanks for reading and happy testing!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Wed 30 August 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/08/30/speeding-up-rust-builds-inside-docker/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/08/12/code-coverage-from-nightmarejs-tests/" rel="bookmark" title="Permalink to Code coverage from Nightmare.js tests">
                <h2 class="post-title">
                    Code coverage from Nightmare.js tests
                </h2>
            </a>
                <p>In this article I'm going to walk you through the steps required
to collect code coverage when running an end-to-end test suite
against a React.js application.</p>
<p>The application under test looks like this</p>
<div class="highlight"><pre><span class="cp">&lt;!doctype html&gt;</span>
<span class="nt">&lt;html</span> <span class="na">lang=</span><span class="s">&quot;en-us&quot;</span> <span class="na">class=</span><span class="s">&quot;layout-pf layout-pf-fixed&quot;</span><span class="nt">&gt;</span>
  <span class="nt">&lt;head&gt;</span>
    <span class="c">&lt;!-- js dependencies skipped --&gt;</span>
  <span class="nt">&lt;/head&gt;</span>
  <span class="nt">&lt;body&gt;</span>
    <span class="nt">&lt;div</span> <span class="na">id=</span><span class="s">&quot;main&quot;</span><span class="nt">&gt;&lt;/div&gt;</span>
    <span class="nt">&lt;script </span><span class="na">src=</span><span class="s">&quot;./dist/main.js?0ca4cedf3884d3943762&quot;</span><span class="nt">&gt;&lt;/script&gt;</span>
  <span class="nt">&lt;/body&gt;</span>
<span class="nt">&lt;/html&gt;</span>
</pre></div>


<p>It is served as an <code>index.html</code> file and a <code>main.js</code> file which intercepts
all interactions from the user and sends requests to the backend API when
needed.</p>
<p>There is an existing unit-test suite which loads the individual components
and tests them in isolation.
<a href="https://twitter.com/atodorov_/status/886881560754102272">Apparently people do this</a>!</p>
<p>There is also an end-to-end test suite which does the majority of the testing.
It fires up a browser instance and interacts with the application. Everything
runs inside Docker containers providing a full-blown production-like environment.
They look like this</p>
<div class="highlight"><pre><span class="nx">test</span><span class="p">(</span><span class="s1">&#39;should switch to Edit Recipe page - recipe creation success&#39;</span><span class="p">,</span> <span class="p">(</span><span class="nx">done</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
  <span class="kr">const</span> <span class="nx">nightmare</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">Nightmare</span><span class="p">();</span>
  <span class="nx">nightmare</span>
    <span class="p">.</span><span class="kr">goto</span><span class="p">(</span><span class="nx">recipesPage</span><span class="p">.</span><span class="nx">url</span><span class="p">)</span>
    <span class="p">.</span><span class="nx">wait</span><span class="p">(</span><span class="nx">recipesPage</span><span class="p">.</span><span class="nx">btnCreateRecipe</span><span class="p">)</span>
    <span class="p">.</span><span class="nx">click</span><span class="p">(</span><span class="nx">recipesPage</span><span class="p">.</span><span class="nx">btnCreateRecipe</span><span class="p">)</span>
    <span class="p">.</span><span class="nx">wait</span><span class="p">(</span><span class="nx">page</span> <span class="o">=&gt;</span> <span class="nb">document</span><span class="p">.</span><span class="nx">querySelector</span><span class="p">(</span><span class="nx">page</span><span class="p">.</span><span class="nx">dialogRootElement</span><span class="p">).</span><span class="nx">style</span><span class="p">.</span><span class="nx">display</span> <span class="o">===</span> <span class="s1">&#39;block&#39;</span>
      <span class="p">,</span> <span class="nx">createRecipePage</span><span class="p">)</span>
    <span class="p">.</span><span class="nx">insert</span><span class="p">(</span><span class="nx">createRecipePage</span><span class="p">.</span><span class="nx">inputName</span><span class="p">,</span> <span class="nx">createRecipePage</span><span class="p">.</span><span class="nx">varRecName</span><span class="p">)</span>
    <span class="p">.</span><span class="nx">insert</span><span class="p">(</span><span class="nx">createRecipePage</span><span class="p">.</span><span class="nx">inputDescription</span><span class="p">,</span> <span class="nx">createRecipePage</span><span class="p">.</span><span class="nx">varRecDesc</span><span class="p">)</span>
    <span class="p">.</span><span class="nx">click</span><span class="p">(</span><span class="nx">createRecipePage</span><span class="p">.</span><span class="nx">btnSave</span><span class="p">)</span>
    <span class="p">.</span><span class="nx">wait</span><span class="p">(</span><span class="nx">editRecipePage</span><span class="p">.</span><span class="nx">componentListItemRootElement</span><span class="p">)</span>
    <span class="p">.</span><span class="nx">exists</span><span class="p">(</span><span class="nx">editRecipePage</span><span class="p">.</span><span class="nx">componentListItemRootElement</span><span class="p">)</span>
    <span class="p">.</span><span class="nx">end</span><span class="p">()</span> <span class="c1">// remove this!</span>
    <span class="p">.</span><span class="nx">then</span><span class="p">((</span><span class="nx">element</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
      <span class="nx">expect</span><span class="p">(</span><span class="nx">element</span><span class="p">).</span><span class="nx">toBe</span><span class="p">(</span><span class="kc">true</span><span class="p">);</span>
      <span class="c1">// here goes coverage collection helper</span>
      <span class="nx">done</span><span class="p">();</span> <span class="c1">// remove this!</span>
    <span class="p">});</span>
<span class="p">},</span> <span class="nx">timeout</span><span class="p">);</span>
</pre></div>


<p>The browser interaction is handled by Nightmare.js (sort of like Selenium) and
the test runner is Jest.</p>
<h2>Code instrumentation</h2>
<p>The first thing we need is to instrument the application code to provide coverage
statistics. This is done via <code>babel-plugin-istanbul</code>. Because unit-tests are
executed a bit differently we want to enable conditional instrumentation. In reality
for unit tests we use <code>jest --coverage</code> which enables istanbul on the fly and having
the code already instrumented breaks this. So I have the following in <code>webpack.config.js</code></p>
<div class="highlight"><pre><span class="k">if</span> <span class="p">(</span><span class="nx">process</span><span class="p">.</span><span class="nx">argv</span><span class="p">.</span><span class="nx">includes</span><span class="p">(</span><span class="s1">&#39;--with-coverage&#39;</span><span class="p">))</span> <span class="p">{</span>
  <span class="nx">babelConfig</span><span class="p">.</span><span class="nx">plugins</span><span class="p">.</span><span class="nx">push</span><span class="p">(</span><span class="s1">&#39;istanbul&#39;</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>


<p>and then build my application with <code>node run build --with-coverage</code>.</p>
<p>You can execute <code>node run start --with-coverage</code>, open the JavaScript console
in your browser and inspect the <code>window.__coverage__</code> variable. If this is defined
then the application is instrumented correctly.</p>
<h2>Fetching coverage information from within the tests</h2>
<p>Remember that <code>main.js</code> from the beginning of this post? It lives inside <code>index.html</code>
which means everything gets downloaded to the client side and executed there.
When running the end-to-end test suite that is the browser instance which is controlled
via Nightmare. <strong>You have to pass <code>window.__coverage__</code> from the browser scope back to
nodejs scope via <code>nightmare.evaluate()</code></strong>! I opted to directly save the coverage data
on the file system and make it available to coverage reporting tools later!</p>
<p>My coverage collecting snippet looks like this</p>
<div class="highlight"><pre><span class="nx">nightmare</span>
  <span class="p">.</span><span class="nx">evaluate</span><span class="p">(()</span> <span class="o">=&gt;</span> <span class="nb">window</span><span class="p">.</span><span class="nx">__coverage__</span><span class="p">)</span> <span class="c1">// this executes in browser scope</span>
  <span class="p">.</span><span class="nx">end</span><span class="p">()</span> <span class="c1">// terminate the Electron (browser) process</span>
  <span class="p">.</span><span class="nx">then</span><span class="p">((</span><span class="nx">cov</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
    <span class="c1">// this executes in Node scope</span>
    <span class="c1">// handle the data passed back to us from browser scope</span>
    <span class="kr">const</span> <span class="nx">strCoverage</span> <span class="o">=</span> <span class="nx">JSON</span><span class="p">.</span><span class="nx">stringify</span><span class="p">(</span><span class="nx">cov</span><span class="p">);</span>
    <span class="kr">const</span> <span class="nx">hash</span> <span class="o">=</span> <span class="nx">require</span><span class="p">(</span><span class="s1">&#39;crypto&#39;</span><span class="p">).</span><span class="nx">createHmac</span><span class="p">(</span><span class="s1">&#39;sha256&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
      <span class="p">.</span><span class="nx">update</span><span class="p">(</span><span class="nx">strCoverage</span><span class="p">)</span>
      <span class="p">.</span><span class="nx">digest</span><span class="p">(</span><span class="s1">&#39;hex&#39;</span><span class="p">);</span>
    <span class="kr">const</span> <span class="nx">fileName</span> <span class="o">=</span> <span class="err">`</span><span class="o">/</span><span class="nx">tmp</span><span class="o">/</span><span class="nx">coverage</span><span class="o">-</span><span class="nx">$</span><span class="p">{</span><span class="nx">hash</span><span class="p">}.</span><span class="nx">json</span><span class="err">`</span><span class="p">;</span>
    <span class="nx">require</span><span class="p">(</span><span class="s1">&#39;fs&#39;</span><span class="p">).</span><span class="nx">writeFileSync</span><span class="p">(</span><span class="nx">fileName</span><span class="p">,</span> <span class="nx">strCoverage</span><span class="p">);</span>

    <span class="nx">done</span><span class="p">();</span> <span class="c1">// the callback from the test</span>
  <span class="p">})</span>
<span class="p">.</span><span class="k">catch</span><span class="p">(</span><span class="nx">err</span> <span class="o">=&gt;</span> <span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="nx">err</span><span class="p">));</span>
</pre></div>


<p>Nightmare returns <code>window.__coverage__</code> from browser scope back to nodejs scope
and we save it under <code>/tmp</code> using a hash value of the coverage data as the file
name.</p>
<p><em>Side note:</em> I do have about 40% less coverage files than number of test cases.
This means some test scenarios exercise the same code paths. Storing the individual
coverage reports under a hashed file name makes this very easy to see!</p>
<p>Note that in my coverage handling code I also call <code>.end()</code> which will terminate
the browser instance and also execute the <code>done()</code> callback which is being passed
as parameter to the test above! This is important because it means we had to update
the way tests were written. In particular the Nightmare method sequence doesn't
have to call <code>.end()</code> and <code>done()</code> except in the coverage handling code. The
coverage helper must be the last code executed inside the body of the last
<code>.then()</code> method. This is usually after all assertions (expectations) have been met!</p>
<p>Now this coverage helper needs to be part of every single test case so I
wanted it to be a one line function, easy to copy&amp;paste! All my attempts to
move this code inside a module have been futile. I can get the module loaded
but it kept failing with
<code>Unhandled promise rejection (rejection id: 1): cov_23rlop1885 is not defined</code>;`</p>
<p>At the end I've resorted to this simple hack</p>
<div class="highlight"><pre><span class="nb">eval</span><span class="p">(</span><span class="nx">fs</span><span class="p">.</span><span class="nx">readFileSync</span><span class="p">(</span><span class="s1">&#39;utils/coverage.js&#39;</span><span class="p">).</span><span class="nx">toString</span><span class="p">());</span>
</pre></div>


<p>Shout-out to <a href="http://krasimirtsonev.com/">Krasimir Tsonev</a> who joined me on a two
days pairing session to figure this stuff out. Too bad we couldn't quite figure it
out. If you do please send me a pull request!</p>
<h2>Reporting the results</h2>
<p>All of these <code>coverage-*.json</code> files are directly consumable by <code>nyc</code> - the
coverage reporting tool that comes with the Istanbul suite! I mounted
<code>.nyc_output/</code> directly under <code>/tmp</code> inside my Docker container so I could</p>
<div class="highlight"><pre>nyc report
nyc report --reporter=lcov | codecov
</pre></div>


<p>We can also modify the unit-test command to
<code>jest --coverage --coverageReporters json --coverageDirectory .nyc_output</code> so it
produces a <code>coverage-final.json</code> file for <code>nyc</code>. Use this if you want to combine
the coverage reports from both test suites.</p>
<p>Because I'm using Travis CI the two test suites are executed independently and
there is no easy way to share information between them. Instead I've switched
from Coveralls to CodeCov which is smart enough to merge coverage submissions
coming from multiple jobs on the same git commits. You can compare the commit
<a href="https://codecov.io/gh/atodorov/welder-web/commit/46556808e42a21f48d008ced2d53ffe176c01b6d">submitting only unit-test results</a>
with the one
<a href="https://codecov.io/gh/atodorov/welder-web/commit/15f437477c17b63797cdb2455f1371336d7dc0e5">submitting coverage from both test suites</a>.</p>
<p>All of the above steps are put into practice in
<a href="https://github.com/weldr/welder-web/pull/136">PR #136</a> if you want to check them out!</p>
<p>Thanks for reading and happy testing!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Sat 12 August 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/08/12/code-coverage-from-nightmarejs-tests/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/08/07/faster-travis-ci-tests-with-docker-cache/" rel="bookmark" title="Permalink to Faster Travis CI tests with Docker cache">
                <h2 class="post-title">
                    Faster Travis CI tests with Docker cache
                </h2>
            </a>
                <p>For a while now I've been running tests on Travis CI using Docker
containers to build the project and execute the tests inside. In this
post I will explain how to speed up execution times.</p>
<p>A Docker image is a filesystem snapshot similar to a virtual machine
image. From these images we build containers (e.g. we run the container X
from the image Y). The construction of Docker images is controlled via
<code>Dockerfile</code> which contains a set of instructions how to build the image.
For example:</p>
<div class="highlight"><pre>FROM welder/web-nodejs:latest
MAINTAINER Brian C. Lane &lt;bcl@redhat.com&gt;
RUN dnf install -y nginx

CMD nginx -g &quot;daemon off;&quot;
EXPOSE 3000

## Do the things more likely to change below here. ##

COPY ./docker/nginx.conf /etc/nginx/

# Update node dependencies only if they have changed
COPY ./package.json /welder/package.json
RUN cd /welder/ &amp;&amp; npm install

# Copy the rest of the UI files over and compile them
COPY . /welder/
RUN cd /welder/ &amp;&amp; node run build

COPY entrypoint.sh /usr/local/bin/entrypoint.sh
ENTRYPOINT [&quot;/usr/local/bin/entrypoint.sh&quot;]
</pre></div>


<p><code>docker build</code> is smart enough to actually build intermediate layers for each
command and store them on your computer. Each command is hashed and it is rebuilt
only if it has been changed. Thus the stuff which doesn't change often goes first
(like setting up a web server or a DB) and the stuff that changes (like the project source code)
goes at the end. All of this is beautifully explained by <a href="https://www.youtube.com/watch?v=3a0gVrfmWC8">Stefan Kanev in
this video</a> (in Bulgarian).</p>
<h2>Travis and Docker</h2>
<p>While intermediate layer caching is a standard feature for Docker it is disabled
by default in Travis CI and any other CI service I was able to find. To be frank
Circles CI offer this as a premium feature but their pricing plans on that aren't
clear at all.</p>
<p>However you can enable the use of caching following a few simple steps:</p>
<ol>
<li>Make your Docker images publicly available (e.g. Docker Hub or Amazon EC2 Container Service)</li>
<li>Before starting the test job do a <code>docker pull my/image:latest</code></li>
<li>When building your Docker images in Travis add <code>--cache-from my/image:latest</code> to <code>docker build</code></li>
<li>After successful execution <code>docker tag</code> the latest image with the build job number and
   <code>docker push</code> it again to the hub!</li>
</ol>
<p><strong>NOTES:</strong></p>
<ul>
<li>Everything you do will become public so take care not to expose internal code.
  Alternatively you may configure a private docker registry (e.g. Amazon EC2 CS)
  and use encrypted passwords for Travis to access your images;</li>
<li><code>docker pull</code> will download all layers that it needs. If your hosting is slow
  this will negatively impact execution times;</li>
<li><code>docker push</code> will upload only the layers that have been changed;</li>
<li>I only push images coming from the master branch which are not from a pull request
  build job. This prevents me from accidentally messing something up.</li>
</ul>
<p>If you examine the logs of <a href="https://travis-ci.org/weldr/welder-web/jobs/260970675">Job #247.4</a>
and <a href="https://travis-ci.org/weldr/welder-web/jobs/261732264">Job #254.4</a> you will notice
that almost all intermediate layers were re-used from cache:</p>
<div class="highlight"><pre>Step 3/12 : RUN dnf install -y nginx
 ---&gt; Using cache
 ---&gt; 25311f052381
Step 4/12 : CMD nginx -g &quot;daemon off;&quot;
 ---&gt; Using cache
 ---&gt; 858606811c85
Step 5/12 : EXPOSE 3000
 ---&gt; Using cache
 ---&gt; d778cbbe0758
Step 6/12 : COPY ./docker/nginx.conf /etc/nginx/
 ---&gt; Using cache
 ---&gt; 56bfa3fa4741
Step 7/12 : COPY ./package.json /welder/package.json
 ---&gt; Using cache
 ---&gt; 929f20da0fc1
Step 8/12 : RUN cd /welder/ &amp;&amp; npm install
 ---&gt; Using cache
 ---&gt; 68a30a4aa5c6
</pre></div>


<p>Here the slowest operations are <code>dnf install</code> and <code>npm install</code> which on normal execution
will take around 5 minutes.</p>
<p>You can check-out my
<a href="https://github.com/weldr/welder-web/blob/master/.travis.yml">.travis.yml</a> for more info.</p>
<h2>First time cache</h2>
<p>It is important to note that you need to have your docker images available in the
registry before you execute the first <code>docker pull</code> from CI. I do this by manually building
the images on my computer and uploading them before configuring CI integration. Afterwards
the CI system takes care of updating the images for me.</p>
<p>Initially you may not notice a significant improvement as seen in
<a href="https://travis-ci.org/weldr/bdcs-api-rs/builds/261510313">Job #262</a>, Step 18/22.
The initial image available on Docker Hub has all the build dependencies installed
and the code has not been changed when job #262 was executed.</p>
<p>The <code>COPY</code> command copies the entire contents of the directory, including filesystem metadata!
Things like uid/gid (file ownership), timestamps (not sure if taken into account)
and/or extended attributes (e.g. SELinux)
will cause the intermediate layers checksums to differ even though the actual
source code didn't change. This will resolve itself once your CI system starts automatically
pushing the latest images to the registry.</p>
<p>Thanks for reading and happy testing!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Mon 07 August 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/08/07/faster-travis-ci-tests-with-docker-cache/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/08/04/transactionmanagementerror-during-testing-with-django-110/" rel="bookmark" title="Permalink to TransactionManagementError during testing with Django 1.10">
                <h2 class="post-title">
                    TransactionManagementError during testing with Django 1.10
                </h2>
            </a>
                <p>During the past 3 weeks I've been debugging a weird error which
started happening after I migrated <a href="http://MrSenko.com/kiwi/">KiwiTestPad</a> to
Django 1.10.7. Here is the reason why this happened.</p>
<h2>Symptoms</h2>
<p>After migrating to Django 1.10 all tests appeared to be working locally
on SQLite however they
<a href="https://travis-ci.org/MrSenko/Kiwi/jobs/258309883">failed on MySQL</a> with</p>
<div class="highlight"><pre><span class="n">TransactionManagementError</span><span class="o">:</span> <span class="n">An</span> <span class="n">error</span> <span class="n">occurred</span> <span class="k">in</span> <span class="n">the</span> <span class="n">current</span> <span class="n">transaction</span><span class="o">.</span> <span class="n">You</span> <span class="n">can</span><span class="s1">&#39;t execute queries until the end of the &#39;</span><span class="n">atomic</span><span class="err">&#39;</span> <span class="n">block</span><span class="o">.</span>
</pre></div>


<p>The exact same test cases
<a href="https://travis-ci.org/MrSenko/Kiwi/jobs/258309884">failed on PostgreSQL</a> with:</p>
<div class="highlight"><pre><span class="n">InterfaceError</span><span class="o">:</span> <span class="n">connection</span> <span class="n">already</span> <span class="n">closed</span>
</pre></div>


<p>Since version 1.10 Django executes all tests inside transactions so my first
thoughts were related to the auto-commit mode. However upon closer inspection
we can see that the line which triggers the failure is</p>
<div class="highlight"><pre>self.assertTrue(users.exists())
</pre></div>


<p>which is essentially a <code>SELECT</code> query aka
<code>User.objects.filter(username=username).exists()</code>!</p>
<p><strong>My tests were failing on a SELECT query!</strong></p>
<p>Reading the numerous posts about <code>TransactionManagementError</code> I discovered it may
be caused by a run-away cursor. The application did use raw SQL statements which
I've converted promptly to ORM queries, that took me some time. Then I also fixed
a couple of places where it used <code>transaction.atomic()</code> as well. No luck!</p>
<p>Then, after numerous experiments and tons of logging inside Django's own code I was
able to figure out when the failure occurred and what events were in place. The test
code looked like this:</p>
<div class="highlight"><pre><span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&#39;/confirm/&#39;</span><span class="p">)</span>

<span class="n">user</span> <span class="o">=</span> <span class="n">User</span><span class="o">.</span><span class="n">objects</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">username</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">new_user</span><span class="o">.</span><span class="n">username</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">assertTrue</span><span class="p">(</span><span class="n">user</span><span class="o">.</span><span class="n">is_active</span><span class="p">)</span>
</pre></div>


<p><strong>The failure was happening after the view had been rendered upon the
first time I do a SELECT against the database!</strong></p>
<p><strong>The problem was that the connection to the database had been closed
midway during the transaction!</strong></p>
<p>In particular (after more debugging of course) the sequence of events was:</p>
<ol>
<li>execute <code>django/test/client.py::Client::get()</code></li>
<li>execute <code>django/test/client.py::ClientHandler::__call__()</code>, which takes
   care to disconnect/connect <code>signals.request_started</code> and <code>signals.request_finished</code>
   which are responsible for tearing down the DB connection, so problem not here</li>
<li>execute <code>django/core/handlers/base.py::BaseHandler::get_response()</code></li>
<li>execute <code>django/core/handlers/base.py::BaseHandler::_get_response()</code> which goes through
   the middleware (needless to say I did inspect all of it as well since there
   have been some changes in Django 1.10)</li>
<li>execute <code>response = wrapped_callback()</code> while still inside <code>BaseHandler._get_response()</code></li>
<li>
<p>execute <code>django/http/response.py::HttpResponseBase::close()</code> which looks like</p>
<div class="highlight"><pre><span class="c"># These methods partially implement the file-like object interface.</span>
<span class="c"># See https://docs.python.org/3/library/io.html#io.IOBase</span>
 
<span class="c"># The WSGI server must call this method upon completion of the request.</span>
<span class="c"># See http://blog.dscpl.com.au/2012/10/obligations-for-calling-close-on.html</span>
<span class="k">def</span> <span class="nf">close</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">closable</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_closable_objects</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">closable</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
        <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
            <span class="k">pass</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">closed</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="n">signals</span><span class="o">.</span><span class="n">request_finished</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">sender</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_handler_class</span><span class="p">)</span>
</pre></div>


</li>
<li>
<p><code>signals.request_finished</code> is fired</p>
</li>
<li><code>django/db/__init__.py::close_old_connections()</code> closes the connection!</li>
</ol>
<p><strong>IMPORTANT:</strong> On MySQL setting <code>AUTO_COMMIT=False</code> and <code>CONN_MAX_AGE=None</code> helps
workaround this problem but is not the solution for me because it didn't help on
PostgreSQL.</p>
<p>Going back to <code>HttpResponseBase::close()</code> I started wondering who calls this method.
The answer was it was getting called by the <code>@content.setter</code> method at
<code>django/http/response.py::HttpResponse::content()</code> which is even more weird because
we assign to <code>self.content</code> inside <code>HttpResponse::__init__()</code></p>
<h2>Root cause</h2>
<p>The root cause of my problem was precisely this <code>HttpResponse::__init__()</code> method
or rather the way we arrive at it inside the application. </p>
<p>The offending view last line was</p>
<div class="highlight"><pre><span class="k">return</span> <span class="n">HttpResponse</span><span class="p">(</span><span class="n">Prompt</span><span class="o">.</span><span class="n">render</span><span class="p">(</span>
     <span class="n">request</span><span class="o">=</span><span class="n">request</span><span class="p">,</span>
     <span class="n">info_type</span><span class="o">=</span><span class="n">Prompt</span><span class="o">.</span><span class="n">Info</span><span class="p">,</span>
     <span class="n">info</span><span class="o">=</span><span class="n">msg</span><span class="p">,</span>
     <span class="nb">next</span><span class="o">=</span><span class="n">request</span><span class="o">.</span><span class="n">GET</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&#39;next&#39;</span><span class="p">,</span> <span class="n">reverse</span><span class="p">(</span><span class="s">&#39;core-views-index&#39;</span><span class="p">))</span>
<span class="p">))</span>
</pre></div>


<p>and the Prompt class looks like this</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">django.shortcuts</span> <span class="kn">import</span> <span class="n">render</span>

<span class="k">class</span> <span class="nc">Prompt</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">render</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">request</span><span class="p">,</span> <span class="n">info_type</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">info</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="nb">next</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">render</span><span class="p">(</span><span class="n">request</span><span class="p">,</span> <span class="s">&#39;prompt.html&#39;</span><span class="p">,</span> <span class="p">{</span>
            <span class="s">&#39;type&#39;</span><span class="p">:</span> <span class="n">info_type</span><span class="p">,</span>
            <span class="s">&#39;info&#39;</span><span class="p">:</span> <span class="n">info</span><span class="p">,</span>
            <span class="s">&#39;next&#39;</span><span class="p">:</span> <span class="nb">next</span>
        <span class="p">})</span>
</pre></div>


<p>Looking back at the internals of <code>HttpResponse</code> we see that</p>
<ul>
<li>if content is a string we call <code>self.make_bytes()</code></li>
<li>if the content is an iterator then we assign it and if the object has a close method
  then it is executed.</li>
</ul>
<p><code>HttpResponse</code> itself is an iterator, inherits from <code>six.Iterator</code> so when we initialize
<code>HttpResponse</code> with another <code>HttpResponse</code> object (aka the content) we execute <code>content.close()</code>
which unfortunately happens to close the database connection as well.</p>
<p><strong>IMPORTANT:</strong> note that from the point of view of a person using the application the
HTML content is exactly the same regardless of whether we have nested <code>HttpResponse</code> objects
or not.
Also during normal execution the code doesn't run inside a transaction so we never notice
the problem in production.</p>
<p>The fix of course is very simple, just <code>return Prompt.render()</code>!</p>
<p>Thanks for reading and happy testing!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Fri 04 August 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/08/04/transactionmanagementerror-during-testing-with-django-110/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/06/27/producing-coverage-report-for-haskell-binaries/" rel="bookmark" title="Permalink to Producing coverage report for Haskell binaries">
                <h2 class="post-title">
                    Producing coverage report for Haskell binaries
                </h2>
            </a>
                <p>Recently I've started testing a Haskell application and a question I find
unanswered (or at least very poorly documented) is how to produce coverage
reports for binaries ?</p>
<h2>Understanding HPC &amp; cabal</h2>
<p><code>hpc</code> is the Haskell code coverage tool. It produces the following files:</p>
<ul>
<li>.mix - module index file, contains information about <em>tick boxes</em> - their type
  and location in the source code;</li>
<li>.tix - tick index file aka coverage report;</li>
<li>.pix - program index file, used only by <code>hpc trans</code>.</li>
</ul>
<p>The invocation to <code>hpc report</code> needs to know where to find the .mix files in order
to be able to translate the coverage information back to source and it needs to
know the location (full path or relative from pwd) to the tix file we want to
report.</p>
<p><code>cabal</code> is the package management tool for Haskell. Among other thing it can be used
to build your code, execute the test suite and produce the coverage report for you.
<code>cabal build</code> will produce module information in <code>dist/hpc/vanilla/mix</code> and
<code>cabal test</code> will store coverage information in <code>dist/hpc/vanilla/tix</code>!</p>
<p>A particular thing about Haskell is that you can only test code which can be
<code>import</code>ed, e.g. it is a library module. You can't test (via Hspec or Hunit) code which
lives inside a file that produces a binary (e.g. Main.hs). However you can still
execute these binaries (e.g. invoke them from the shell) and they will produce a
coverage report in the current directory (e.g. main.tix).</p>
<h2>Putting everything together</h2>
<ol>
<li>Using <code>cabal build</code> and <code>cabal test</code> build the project and execute your unit tests.
   This will create the necessary .mix files (including ones for binaries) and .tix
   files coming from unit testing;</li>
<li>Invoke your binaries passing appropriate data and examining the results (e.g. compare
   the output to a known value). A simple shell or Python script could do the job;</li>
<li>Copy the <code>binary.tix</code> file under <code>dist/hpc/vanilla/binary/binary.tix</code>!</li>
</ol>
<p>Produce coverage report with hpc:</p>
<div class="highlight"><pre>hpc markup --hpcdir=dist/hpc/vanilla/mix/lib --hpcdir=dist/hpc/vanilla/mix/binary  dist/hpc/vanilla/tix/binary/binary.tix
</pre></div>


<p>Convert the coverage report to JSON and send it to Coveralls.io:</p>
<div class="highlight"><pre>cabal install hpc-coveralls
~/.cabal/bin/hpc-coveralls --display-report tests binary
</pre></div>


<h2>Example</h2>
<p>Check out the <a href="https://github.com/weldr/haskell-rpm/pull/18">haskell-rpm</a> repository
for an example. See <a href="https://coveralls.io/builds/12131112">job #45</a> where there is now
coverage for the <code>inspect.hs</code>, <code>unrpm.hs</code> and <code>rpm2json.hs</code> files, producing binary executables.
Also notice that in
<a href="https://coveralls.io/builds/12131112/source?filename=.%2FRPM%2FParse.hs">RPM/Parse.hs</a>
the function <code>parseRPMC</code> is now covered, while it was not covered in the
<a href="https://coveralls.io/builds/12102486/source?filename=.%2FRPM%2FParse.hs">previous job #42</a>!</p>
<div class="highlight"><span class="filename">.travis.yml snippet</span><pre><span class="l-Scalar-Plain">script</span><span class="p-Indicator">:</span>
  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">~/.cabal/bin/hlint .</span>
  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">cabal install --dependencies-only --enable-tests</span>
  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">cabal configure --enable-tests --enable-coverage --ghc-option=-DTEST</span>
  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">cabal build</span>
  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">cabal test --show-details=always</span>

  <span class="c1"># tests to produce coverage for binaries</span>
  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">wget https://s3.amazonaws.com/atodorov/rpms/macbook/el7/x86_64/efivar-0.14-1.el7.x86_64.rpm</span>
  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">./tests/test_binaries.sh ./efivar-0.14-1.el7.x86_64.rpm</span>

  <span class="c1"># move .tix files in appropriate directories</span>
  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">mkdir ./dist/hpc/vanilla/tix/inspect/ ./dist/hpc/vanilla/tix/unrpm/ ./dist/hpc/vanilla/tix/rpm2json/</span>
  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">mv inspect.tix ./dist/hpc/vanilla/tix/inspect/</span>
  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">mv rpm2json.tix ./dist/hpc/vanilla/tix/rpm2json/</span>
  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">mv unrpm.tix ./dist/hpc/vanilla/tix/unrpm/</span>

<span class="l-Scalar-Plain">after_success</span><span class="p-Indicator">:</span>
  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">cabal install hpc-coveralls</span>
  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">~/.cabal/bin/hpc-coveralls --display-report tests inspect rpm2json unrpm</span>
</pre></div>


<p>Thanks for reading and happy testing!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Tue 27 June 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/06/27/producing-coverage-report-for-haskell-binaries/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/06/26/whats-the-bug-in-this-pseudo-code/" rel="bookmark" title="Permalink to What's the bug in this pseudo-code">
                <h2 class="post-title">
                    What's the bug in this pseudo-code
                </h2>
            </a>
                <p><img alt="Rails Girls Vratsa sticker" src="/images/bug_rails_girls_vratsa.jpg" title="Rails Girls Vratsa sticker" /></p>
<p>This is one of the stickers for the second edition of Rails Girls Vratsa which
was held yesterday. Let's explore some of the bug proposals submitted by the Bulgarian QA group:</p>
<blockquote>
<ol>
<li>sad() == true is ugly</li>
<li>sad() is not very nice, better make it if(isSad())</li>
<li>use sadStop(), and even better - stopSad()</li>
<li>there is an extra space character in beAwesome( )</li>
<li>the last curly bracket needs to be on a new line</li>
</ol>
<p>Lyudmil Latinov</p>
</blockquote>
<p>My friend Lu describes what I would call style issues. The style he refers to
is mostly Java oriented, especially with naming things. In Ruby we would probably
go with <code>sad?</code> instead of <code>isSad</code>. Style is important and there are many tools
to help us with that this will not cause a functional problem! While I'm at it let me say
the curly brackets are not the problem either. They are not valid in Ruby this is
a pseudo-code and they also fall in the style category.</p>
<p>The next interesting proposal comes from Tsveta Krasteva. She examines the possibility
of <code>sad()</code> returning an object or nil instead of boolean value. Her first question was
will the if statement still work, and the answer is yes. In Ruby everything is an object
and every object can be compared to <code>true</code> and <code>false</code>. See
<a href="http://www.skorks.com/2009/09/true-false-and-nil-objects-in-ruby/">Alan Skorkin's</a> blog
post on the subject.</p>
<p>Then Tsveta says the answer is to use <code>sad().stop()</code> with the warning that it may return
nil. In this context the <code>sad()</code> method returns on object indicating that the person
is feeling sad. If the method returns nil then the person is feeling OK.</p>
<div class="highlight"><span class="filename">example by Tsveta</span><pre><span class="k">class</span> <span class="nc">Csad</span>
  <span class="k">def</span> <span class="nf">stop</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;stop</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">);</span>
  <span class="k">end</span>
<span class="k">end</span>

<span class="k">def</span> <span class="nf">sad</span><span class="p">()</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;sad</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">);</span>
  <span class="no">Csad</span><span class="o">.</span><span class="n">new</span><span class="p">();</span>
<span class="k">end</span>

<span class="k">def</span> <span class="nf">beAwesome</span><span class="p">()</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;beAwesome</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">);</span>
<span class="k">end</span>

<span class="c1"># notice == true was removed</span>
<span class="k">if</span><span class="p">(</span><span class="n">sad</span><span class="p">())</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Yes, I am sad</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">);</span>
  <span class="n">sad</span><span class="o">.</span><span class="n">stop</span><span class="p">();</span>
  <span class="n">beAwesome</span><span class="p">(</span> <span class="p">);</span>
<span class="k">end</span>
</pre></div>


<p>While this is coming closer to a functioning solution something about it is bugging me.
In the if statement the developer has typed more characters than required (<code>== true</code>).
This sounds to me unlikely but is possible with less experienced developers.
The other issue is that we are using an object (of <code>class Csad</code>) to represent an internal
state in the system under test. There is one method to return the state (<code>sad()</code>) and
another one to alter the state (<code>Csad.stop()</code>). The two methods don't operate on
the same object! Not a very strong OOP design. On top of that we have to call the
method twice, first time in the if statement, the second time in the body of the
if statement, which may have unwanted side effects. It is best to assign the return
value to some variable instead.</p>
<p>IMO if we are to use this OOP approach the code should look something like:</p>
<div class="highlight"><pre><span class="k">class</span> <span class="n">Person</span>
  <span class="n">def</span> <span class="n">sad</span>?()
  <span class="nb">end</span>

  <span class="n">def</span> <span class="n">stopBeingSad</span>()
  <span class="nb">end</span>

  <span class="n">def</span> <span class="n">beAwesome</span>()
  <span class="nb">end</span>
<span class="nb">end</span>

<span class="n">p</span> = <span class="n">Person</span>.<span class="nb">new</span>
<span class="k">if</span> <span class="n">p</span>.<span class="n">sad</span>?
    <span class="n">p</span>.<span class="n">stopBeingSad</span>
    <span class="n">p</span>.<span class="n">beAwesome</span>
<span class="nb">end</span>
</pre></div>


<p>Let me return back to assuming we don't use classes here.
The first <em>obvious</em> mistake is the space in <code>sad stop();</code> first spotted by Peter Sabev*.
His proposal, backed by others is to use <code>sad.stop()</code>. However they
didn't use my hint asking what is the return value of <code>sad()</code> ?</p>
<p>If <code>sad()</code> returns boolean then we'll get
<code>undefined method 'stop' for true:TrueClass (NoMethodError)</code>!
Same thing if <code>sad()</code> returns nil, although we skip the if block in this case.</p>
<p>In Ruby we are allowed to skip parentheses when calling a method, like I've shown
above. If we ignore this fact for a second, then <code>sad?.stop()</code> will mean execute the
method named <code>stop()</code> which is a member of the <code>sad?</code> variable, which is of type method!
Again, methods don't have an attribute named <code>stop</code>!</p>
<p>The last two paragraphs are the semantic/functional mistake I see in this code. The only way
for it to work is to use an OOP variant which is further away from what the existing
clues give us.</p>
<p><strong>Note:</strong> The variant <code>sad? stop()</code> is syntactically correct. This means call the function <code>sad?</code>
with parameter the result of calling the method <code>stop()</code>, which depending on the outer scope of this program may or may not
be correct (e.g. <code>stop</code> is defined, <code>sad?</code> accepts optional parameters, <code>sad?</code> maintains
global state).</p>
<p>Thanks for reading and happy testing!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Mon 26 June 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/06/26/whats-the-bug-in-this-pseudo-code/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/06/14/vmwares-favorite-login-form/" rel="bookmark" title="Permalink to VMware's favorite login form">
                <h2 class="post-title">
                    VMware's favorite login form
                </h2>
            </a>
                <p><em>How do you test a login form?</em> is one of my favorite questions when
screening candidates for QA positions and also a good brain exercise
even for experienced testers. I've written about it
<a href="http://atodorov.org/blog/2016/04/12/how-to-hire-software-testers-pt-1/">last year</a>. In this
blog post I'd like to share a different perspective on this same question,
this time courtesy of my friend Rayna Stankova.</p>
<p><img alt="Login form" src="/images/login_form_vmware.png" title="Login form" /></p>
<h2>What bugs do you see above</h2>
<p>The series of images above is from a
<a href="https://www.meetup.com/Women-Who-Code-Sofia/events/239480974/">Women Who Code Sofia</a>
workshop where the participants were given printed copies and asked to find
as much defects as possible. Here they are (counting clock-wise from the top-left corner):</p>
<ol>
<li>Typo in "Registr" link at the bottom;</li>
<li>UI components are not aligned;</li>
<li>Missing "Forgot your password?" link</li>
<li>Backend credentials validation with empty password;
   plain text password field; Too specific information about incorrect credentials;</li>
<li>Too specific information about incorrect credentials with visual hint
   as to what exactly is not correct. In this case it looks like the password
   is OK, maybe it was one of
   <a href="https://www.youtube.com/watch?v=0Jx8Eay5fWQ">the 4 most commonly used passwords</a>,
   but the username is wrong which we can easily figure out;</li>
<li>In this case the error handling appears to be correct, not disclosing what
   exactly is wrong. The placement is somewhat wrong, it looks like an error
   message for one of the fields instead for the entire form. I'd move that to the top
   and even slightly update the wording to be more like <em>Login failed, bad credentials,
   try again</em>.</li>
</ol>
<h2>How do you test this</h2>
<p>Here is a list of possible test scenarios, proposed by Rayna. Notes are mine.</p>
<p><strong>UI Layer</strong></p>
<ul>
<li>Test 1: Verify Email (User ID) field has focus on page load</li>
<li>Test 2: Verify Empty Email (User ID) field and Password field</li>
<li>Test 3: Verify Empty Email (User ID) field</li>
<li>Test 4: Verify Empty Password field</li>
<li>Test 5: Verify Correct sign in</li>
<li>Test 6: Verify Incorrect sign in</li>
<li>Test 7: Verify Password Reset - working link</li>
<li>Test 8: Verify Password Reset - invalid emails</li>
<li>Test 9: Verify Password Reset - valid email</li>
<li>Test 10: Verify Password Reset - using new password</li>
<li>Test 11: Verify Password Reset - using old password</li>
<li>Test 12: Verify whether password text is hidden</li>
<li>Test 13: Verify text field limits - whether the browser accepts more than the allowed database limits</li>
<li>Test 14: Verify that validation message is displayed in case user exceeds the character limit of the username and password fields</li>
<li>Test 15: Verify if there is checkbox with label "remember password" in the login page</li>
<li>Test 16: Verify if it’s allowed the username to contain non printable characters? If not, this is invalid on the 'create user' section.</li>
<li>Test 17: Verify if the user must be logged in to access any other area of the site.</li>
</ul>
<p>Tests 10 and 11 are particularly relevant for Fedora Account System where
you need a really strong password and (at least in the past) had to change it more often
and couldn't reuse any of your old passwords. As a user I really hate this b/c I can't remember
my own password but it makes for a good test scenario.</p>
<p>13 and 14 are also something I rarely see and could make a nice case for
property based testing.</p>
<p>16 would have been the bread and butter of testing Emoj.li (the first emoji-only
social network).</p>
<p><strong>Keyboard Specific</strong></p>
<ul>
<li>Test 18: Verify Navigate to all fields</li>
<li>Test 19: Verify Enter submits on password focus</li>
<li>Test 20: Verify Space submits on login focus</li>
<li>Test 21: Verify Enter submits</li>
</ul>
<p>These are all so relevant with beautifully styled websites nowadays. The one I hate the most
is when space key doesn't trigger select/unselect for checkboxes which are actually
images!</p>
<p><strong>Security:</strong></p>
<ul>
<li>Test 22: Verify SQL Injections testing - password field</li>
<li>Test 23: Verify SQL Injections testing - username field</li>
<li>Test 24: Verify SQL Injections testing - reset password</li>
<li>Test 25: Verify Password/username not visible from URL login</li>
<li>Test 26: Verify For security point of view, in case of incorrect credentials user is displayed the message like
  "incorrect username or password" instead of exact message pointing at the field that is incorrect.
  As message like "incorrect username" will aid hacker in brute-forcing the fields one by one</li>
<li>Test 27: Verify the timeout of the login session</li>
<li>Test 28: Verify if the password can be copy-pasted or not</li>
<li>Test 29: Verify that once logged in, clicking back button doesn't logout user</li>
</ul>
<p>22, 23 and 24 are a bit generic and I guess can be collapsed into one. Better yet make
them more specific instead.</p>
<p>Test 28 may sound like nonsense but is not. I remember back in the days that
it was possible to copy and paste the password out of Windows dial-up credentials screen.
With heavily styled form fields it is possible to have this problem again so it is
a valid check IMO.</p>
<p><strong>Others:</strong></p>
<ul>
<li>Test 30: Verify that the password is in encrypted form when entered</li>
<li>Test 31: Verify the user must be logged in to call any web services.</li>
<li>Test 32: Verify if the username is allowed to contain non printable characters,
  the code handling login can deal with them and no error is thrown.</li>
</ul>
<p>I think Test 30 means to validate that the backend doesn't store passwords in plain text
but rather stores their hashes.</p>
<p>32 is a duplicate of 16. I also say why only the username? Password field is also
a good candidate for this.</p>
<p>If you check how I
<a href="http://atodorov.org/blog/2016/04/12/how-to-hire-software-testers-pt-1/">would test a login form</a> you will find
some similarities but there are also scenarios which are different. I'm interested to
see what other scenarios we've both missed, especially ones which have manifested themselves
as bugs in actual applications.</p>
<p>Thanks for reading and happy testing!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Wed 14 June 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/06/14/vmwares-favorite-login-form/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/06/12/monitoring-behavior-via-automated-tests/" rel="bookmark" title="Permalink to Monitoring behavior via automated tests">
                <h2 class="post-title">
                    Monitoring behavior via automated tests
                </h2>
            </a>
                <p>In my last several presentations I briefly talked about
using your tests as a monitoring tool. I've not been eating my own
dog food and stuff failed in production!</p>
<h2>What is monitoring via testing</h2>
<p>This is a technique I coined 6 months ago while working with Tradeo's team.
I'm not the first one to figure this out so if you know the proper
name for it please let me know in the comments.
So why not take a subset of your automated tests and run them regularly against
production? Let's say every hour?</p>
<p>In my particular case we
started with integration tests which interact with the product (a web app)
in a way that a living person would do. E.g. login, update their settings,
follow another user, chat with another user, try to deposit money, etc.
The results from these tests are logged into a database and then charted
(using Grafana). This way we can bring lots of data points together and easily
analyze them.</p>
<p>This technique has the added bonus that we can cover the most critical
test paths in a couple of minutes and do so regularly without human intervention.
Perusing the existing monitoring infrastructure of the devops team we can configure
alerts if need be. This makes it sort of early detection/warning system plus
it gives a degree of possibility to spot correlations between data points or
patterns.</p>
<p>As simple as it sounds I've heard about a handfull of companies doing this
sort of continuous testing against production. Maybe you can implement something
similar in your organization and we can talk more about the results?</p>
<h2>Why does it matter</h2>
<p>Anyway, everyone knows
<a href="http://atodorov.org/blog/2017/05/27/learn-python-selenium-automation-in-8-weeks/">how to write Selenium tests</a>
so I'm not going to bother you with the details. Why does this kind of
testing matter?</p>
<p>Do you remember a recent announcement by GitHub about Travis CI leaking some
authentication tokens into their public log files? I did receive an email about
this but didn't pay attention to it because I don't use GitHub tokens for
anything I do in Travis. However as a safety measure GitHub had went ahead and
wiped out my security tokens.</p>
<p>The result from this is that my
<a href="http://mrsenko.com/blog/mr-senko/2016/05/18/triggering-automatic-dependency-testing/">automated upstream testing infrastructure</a>
had stopped working! In particular my requests to the GitHub API stopped
working. And I didn't even know about it!</p>
<p>This means that since May 24th there have been at least 4 new
versions of libraries and frameworks on which some of my software depends
and I failed to test them! One of them was <em>Django 1.11.2</em>.</p>
<p>I have supplied a new GitHub token for my infra but if I had monitoring
I would have known about this problem well in advance. Next I'm off to write
some monitoring tests and also implement better failure detection in
<a href="https://github.com/MrSenko/strazar">Strazar</a> itself!</p>
<p>Thanks for reading and happy testing (in production)!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Mon 12 June 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/06/12/monitoring-behavior-via-automated-tests/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/06/10/semantically-invalid-input/" rel="bookmark" title="Permalink to Semantically Invalid Input">
                <h2 class="post-title">
                    Semantically Invalid Input
                </h2>
            </a>
                <div class="highlight"><span class="filename">Unsolvable Square example</span><pre><span class="o">.</span> <span class="o">.</span> <span class="mi">9</span> <span class="o">|</span> <span class="o">.</span> <span class="mi">2</span> <span class="mi">8</span> <span class="o">|</span> <span class="mi">7</span> <span class="o">.</span> <span class="o">.</span>
<span class="mi">8</span> <span class="o">.</span> <span class="mi">6</span> <span class="o">|</span> <span class="o">.</span> <span class="o">.</span> <span class="mi">4</span> <span class="o">|</span> <span class="o">.</span> <span class="o">.</span> <span class="mi">5</span>
<span class="o">.</span> <span class="o">.</span> <span class="mi">3</span> <span class="o">|</span> <span class="o">.</span> <span class="o">.</span> <span class="o">.</span> <span class="o">|</span> <span class="o">.</span> <span class="o">.</span> <span class="mi">4</span>
<span class="o">------+-------+------</span>
<span class="mi">6</span> <span class="o">.</span> <span class="o">.</span> <span class="o">|</span> <span class="o">.</span> <span class="o">.</span> <span class="o">.</span> <span class="o">|</span> <span class="o">.</span> <span class="o">.</span> <span class="o">.</span>
<span class="err">?</span> <span class="mi">2</span> <span class="o">.</span> <span class="o">|</span> <span class="mi">7</span> <span class="mi">1</span> <span class="mi">3</span> <span class="o">|</span> <span class="mi">4</span> <span class="mi">5</span> <span class="o">.</span>
<span class="o">.</span> <span class="o">.</span> <span class="o">.</span> <span class="o">|</span> <span class="o">.</span> <span class="o">.</span> <span class="o">.</span> <span class="o">|</span> <span class="o">.</span> <span class="o">.</span> <span class="mi">2</span>
<span class="o">------+-------+------</span>
<span class="mi">3</span> <span class="o">.</span> <span class="o">.</span> <span class="o">|</span> <span class="o">.</span> <span class="o">.</span> <span class="o">.</span> <span class="o">|</span> <span class="mi">5</span> <span class="o">.</span> <span class="o">.</span>
<span class="mi">9</span> <span class="o">.</span> <span class="o">.</span> <span class="o">|</span> <span class="mi">4</span> <span class="o">.</span> <span class="o">.</span> <span class="o">|</span> <span class="mi">8</span> <span class="o">.</span> <span class="mi">7</span>
<span class="o">.</span> <span class="o">.</span> <span class="mi">1</span> <span class="o">|</span> <span class="mi">2</span> <span class="mi">5</span> <span class="o">.</span> <span class="o">|</span> <span class="mi">3</span> <span class="o">.</span> <span class="o">.</span>
</pre></div>


<p>In a comment to a
<a href="http://atodorov.org/blog/2016/04/16/how-to-hire-software-testers-pt-2/">previous post</a>
<em>Flavio Poletti</em> proposed a very interesting test case for a function which solves
the Sudoku game - <em>semantically invalid input, i.e. an input that passes intermediate
validation checks (no duplicates in any row/col/9-square) but that cannot possibly
have a solution</em>.</p>
<p>Until then I thought that Sudoku was a completely deterministic game and if input
followed all validation checks then we always have a solution. Apparently I was wrong!
Reading more on the topic I discovered these
<a href="http://sudopedia.enjoysudoku.com/Test_Cases.html">Sudoku test cases from Sudopedia</a>.
Their <a href="http://sudopedia.enjoysudoku.com/Invalid_Test_Cases.html">Invalid Test Cases</a>
section lists several examples of semantically invalid input in Sudoku:</p>
<ul>
<li>Unsolvable Square;</li>
<li>Unsolvable Box;</li>
<li>Unsolvable Column;</li>
<li>Unsolvable Row;</li>
<li>Not Unique with examples having 2, 3, 4, 10 and 125 solutions</li>
</ul>
<p>The example above cannot be solved because the left-most
square of the middle row (r5c1) has no possible candidates.</p>
<p>Following the rule <em>non-repeating numbers from 1 to 9 in each row</em> for row 5 we're
left with numbers: 6, 8 and 9. For (r5c1) 6 is a no-go because it is already present
in the same square. Then 9 is a no-go because it is present in column 1. Which leaves
us with 8, which is also present in column 1! Pretty awesome, isn't it?</p>
<p>Also check the
<a href="http://sudopedia.enjoysudoku.com/Valid_Test_Cases.html">Valid Test Cases</a> section
which includes other interesting examples and definitely not ones which I have considered
previously when <a href="http://atodorov.org/blog/2016/04/16/how-to-hire-software-testers-pt-2/">testing Sudoku</a>.</p>
<p>On a more practical note I have been trying to remember a case from my QA practice
where we had input data that matched all conditions but is semantically invalid. I
can't remember of such a case. If you do have examples about semantically invalid
data in real software please let me know in the comments below!</p>
<p>Thanks for reading and happy testing!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Sat 10 June 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/06/10/semantically-invalid-input/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/05/27/learn-python-selenium-automation-in-8-weeks/" rel="bookmark" title="Permalink to Learn Python & Selenium Automation in 8 weeks">
                <h2 class="post-title">
                    Learn Python &amp; Selenium Automation in 8 weeks
                </h2>
            </a>
                <p>Couple of months ago I conducted a practical,
instructor lead training in Python and Selenium automation for
manual testers. You can find the materials at
<a href="https://github.com/atodorov/qa-automation-python-selenium-101">GitHub</a>.</p>
<p>The training consists of several basic modules and practical homework
assignments. The modules explain</p>
<ol>
<li>The basic structure of a Python program and functions</li>
<li>Commonly used data types</li>
<li>If statements and (for) loops</li>
<li>Classes and objects</li>
<li>The Python unit testing framework and its assertions</li>
<li>High-level introduction to Selenium with Python</li>
<li>High-level introduction to the Page Objects design pattern</li>
<li>Writing automated tests for real world scenarios
   without any help from the instructor.</li>
</ol>
<p>Every module is intended to be taken in the course of 1 week and begins with
links to preparatory materials and lots of reading. Then I help the students
understand the basics and explain with more examples, often writing code as
we go along. At the end there is the homework assignment for which I expect
a solution presented by the end of the week so I can comment and code-review it.</p>
<p>All assignments which require the student to implement functionality, not tests,
are paired with a test suite, which the student should use to validate their
solution.</p>
<h2>What worked well</h2>
<p>Despite everything I've written below I had 2 students (from a group of 8)
which showed very good progress. One of them was the absolute star, taking
active participation in every class and doing almost all homework assignments
on time, pretty much without errors. I think she'd had some previous training
or experience though.
She was in the USA, training was done remotely via Google Hangouts.</p>
<p>The other student was in Sofia, training was done in person. He is not on the
same level as the US student but is the best from the Bulgarian team. IMO he
lacks a little bit of motivation. He "cheated" a bit on some tasks providing
non-standard, easier solutions and made most of his assignments. After the first
Selenium session he started creating small scripts to extract results from
football sites or as helpers to be applied in the daily job.
The interesting
fact for me was that he created his programs as <code>unittest.TestCase</code> classes.
I guess because this was the way he knew how to run them!?!</p>
<p>There were another few students which had had some prior experience with
programming but weren't very active in class so I can't tell how their
careers will progress. If they put some more effort into it I'm sure they
can turn out to have decent programming skills.</p>
<h2>What didn't work well</h2>
<p>Starting from the beginning most students failed to read the preparatory
materials. Some of the students did read a little bit, others didn't read at all.
At the times when they came prepared I had the feeling the sessions progressed
more smoothly. I also had students joining late in the process, which for the
most part didn't participate at all in the training. I'd like to avoid that in
the future if possible.</p>
<p>Sometimes students complained about lack of example code, although
<em>Dive into Python</em> includes tons of examples. I've resorted to sending them
the example.py files which I produced during class.</p>
<p>The practical part of the training was mostly myself programming on a big
TV screen in front of everyone else. Several times someone from the students
took my place. There wasn't much active participation on their part and
unfortunately they didn't want to bring personal laptops to the training
(or maybe weren't allowed)! We did have a company provided laptop though.</p>
<p>When practicing functions and arithmetic operations the students struggled
with basic maths like breaking down a number into its digits or vice versa,
working with Fibonacci sequences and the like. In some cases they cheated
by converting to/from strings and then iterating over them. Also some
hard-coded the first few numbers of the Fibonacci sequence and returned
it directly. Maybe an in-place
explanation of the underlying maths would have been helpful but honestly
I was surprised by this. Somebody please explain or give me an advise here!</p>
<p>I am completely missing examples of the <code>datetime</code> and <code>timedelta</code> classes
which tuned out to be very handy in the practical Selenium tasks and we had
to go over them on the fly.</p>
<p>The OOP assignments went mostly undone, not to mention one of them had
bonus tasks which are easily solved using recursion. I think we could
skip some of the OOP practice (not sure how safe that is) because I really
need classes only for constructing the tests and we don't do anything fancy
there.</p>
<p>Page Object design pattern is also OOP based and I think that went somewhat
well granted that we are only passing values around and performing some actions.
I didn't put constraints nor provided guidance on what the classes should
look like and which methods go where. Maybe I should have made it easier.</p>
<p>Anyway, given that Page Objects is being replaced by Screenplay pattern,
I think we can safely stick to the all-in-one functional based Selenium
tests. Maybe utilize helper functions for repeated tasks (like login).
Indeed this is what I was using last year with Rspec &amp; Capybara!</p>
<h2>What students didn't understand</h2>
<p>Right until the end I had people who had troubles understanding function
signatures, function instances and calling/executing a function. Also
returning a value from a function vs. printing the (same) value on screen
or assigning to the same global variable (e.g. FIB_NUMBERS).</p>
<p>In the same category falls using method parameters vs. using global variables
(which happened to have the same value), using the parameters as arguments to
another function inside the body of the current function, using class attributes
(e.g. <code>self.name</code>) to store and pass values around vs. local variables in methods
vs. method parameters which have the same names.</p>
<p>I think there was some confusion about lists, dictionaries and tuples but
we did practice mostly with list structures so I don't have enough information.</p>
<p>I have the impression that object oriented programming (classes and instances,
we didn't go into inheritance) are generally confusing to beginners with zero
programming experience. The classical way to explain them is by using some
abstractions like animal -&gt; dog -&gt; a particular dog breed -&gt; a particular pet.
OOP was explained to me in a similar way back in school so these kinds of
abstractions are very natural for me. I have no idea if my explanation sucks or students are having hard time
wrapping their heads around the abstraction. I'd love to hear some feedback
from other instructors on this one.</p>
<p>I think there is some misunderstanding between a class (a definition of behavior)
and an instance/object of this class (something which exists into memory). This
may also explain the difficulty remembering or figuring out what <code>self</code> points to
and why do we need to use it inside method bodies.</p>
<p>For <code>unittest.TestCase</code> we didn't do lots of practice which is my fault.
The homework assignments request the students to go back to solutions
of previous modules and implement more tests for them. Next time I should
provide a module (possibly with non-obvious bugs) and request to write
a comprehensive test suite for it.</p>
<p>Because of the missing practice there was some confusion/misunderstanding
about the <code>setUpClass/tearDownClass</code> and the <code>setUp/tearDown</code> methods.
Also add to the mix that the first are <code>@classmethod</code> while the later
are not. "To be safe" students always defined both as class methods!</p>
<p>I have since corrected the training materials but we didn't have
good examples (nor practiced) explaining the difference between
<code>setUpClass</code> (executed once aka before suite) and <code>setUp</code>
(possibly executed multiple times aka before test method).</p>
<p>On the Selenium side I think it is mostly practice which students lack,
not understanding. The entire Selenium framework (any web test framework
for that matter) boils down to</p>
<ul>
<li>Load a page</li>
<li>Find element(s)</li>
<li>Click or hover (that one was tricky) element</li>
<li>Get element's attribute value or text</li>
<li>Wait for the proper page to load (or worst case AJAX calls)</li>
</ul>
<p>IMO finding the correct element on the page is on-par with waiting
(which also relies on locating elements) and took 80% of the time we spent
working with Selenium.</p>
<p>Thanks for reading and don't forget to comment and give me your feedback!</p>
<p><em>Image source: https://www.udemy.com/selenium-webdriver-with-python/</em></p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Sat 27 May 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/05/27/learn-python-selenium-automation-in-8-weeks/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/04/20/quality-assurance-according-2-einstein/" rel="bookmark" title="Permalink to Quality Assurance According 2 Einstein">
                <h2 class="post-title">
                    Quality Assurance According 2 Einstein
                </h2>
            </a>
                <p><img alt="logo" src="/images/qa_einstein.png" title="logo" /></p>
<p><a href="https://www.facebook.com/events/1887550261534408/">Quality Assurance According 2 Einstein</a>
is a talk which introduces several different ideas about how we need to think
and approach software testing. It touches on subjects like mutation testing,
pairwise testing, automatic test execution, smart test non-execution, using tests
as monitoring tools and team/process organization.</p>
<p>Because testing is more thinking than writing I have chosen a different format
for this presentation. It contains only slides with famous quotes from one
of the greatest thinkers of our time - Albert Einstein!</p>
<p>This blog post includes the accompanying links and references only! It is my first
iteration on the topic so expect it to be unclear and incomplete, use your imagination!
I will continue 
working and presenting on the same topic in the next few months
so you can expect updates from time to time. In the mean time I am happy to discuss
with you down in the comments.</p>
<blockquote>
<p>IMAGINATION
IS MORE
IMPORTANT
THAN KNOWLEDGE.</p>
</blockquote>
<ul>
<li><a href="http://atodorov.org/blog/2016/03/25/hello-world-qa-challenge/">Hello World bug challenge</a></li>
<li><a href="http://atodorov.org/blog/2016/04/16/how-to-hire-software-testers-pt-2/#disqus_thread">Testing a Sudoku</a></li>
<li><a href="https://github.com/weldr/welder-web/pull/56">https://github.com/weldr/welder-web/pull/56</a></li>
<li><a href="https://github.com/weldr/welder-web/pull/59">https://github.com/weldr/welder-web/pull/59</a></li>
</ul>
<blockquote>
<p>THE FASTER YOU GO,
THE SHORTER
YOU ARE.</p>
</blockquote>
<ul>
<li><a href="http://bit.ly/GTAC2016Unity3D">Using Statistics to Predict Which Tests to Run</a></li>
<li><a href="http://bit.ly/ISTA2016ExMachina">The framework that knows its bugs</a></li>
<li><a href="http://atodorov.org/blog/2017/04/14/testing-red-hat-enterprise-linux-the-microsoft-way/">Testing Red Hat Enterprise Linux the Microsoft way</a></li>
<li><a href="http://mrsenko.com/blog/mr-senko/2016/05/18/triggering-automatic-dependency-testing/">Automatic dependency testing with Strazar</a></li>
<li><a href="http://atodorov.org/blog/2017/04/15/automatic-cargo-update-pull-requests-for-rust-projects/">Automatic cargo update, test and pull request</a></li>
</ul>
<blockquote>
<p>IF THE FACTS
DON'T FIT
THE THEORY,
CHANGE THE FACTS.</p>
</blockquote>
<ul>
<li><a href="https://www.youtube.com/watch?v=sAfROROGujU&amp;list=PLFjlI7p-h1hxBP3cIjEqePSeoBDHud5Db&amp;index=47">Coverage is Not Strongly Correlated with Test Suite Effectiveness</a></li>
<li><a href="https://www.youtube.com/watch?v=NKEptA3KP08&amp;list=PLFjlI7p-h1hxBP3cIjEqePSeoBDHud5Db&amp;index=1">Code Coverage is a Strong Predictor of Test Suite Effectiveness</a></li>
<li><a href="http://atodorov.org/blog/2016/12/27/mutation-testing-vs-coverage/">Mutation testing vs. coverage, Pt. 1</a></li>
<li><a href="http://atodorov.org/blog/2017/04/05/mutation-testing-vs-coverage-pt-2/">Mutation testing vs. coverage, Pt. 2</a></li>
<li>There are 101 coverage metrics according to <a href="http://www.badsoftware.com/coverage.htm">Cem Kaner</a>.
  Which ones are you measuring and what conclusions are you making out of these metrics?</li>
</ul>
<blockquote>
<p>THE WHOLE OF
SCIENCE
IS NOTHING MORE
THAN A REFINEMENT
OF EVERYDAY
THINKING.</p>
</blockquote>
<ul>
<li><a href="https://github.com/HackBulgaria/QA-and-Automation-101/tree/master/lesson12">How to find 1000 bugs in 30 minutes</a></li>
<li><a href="https://www.youtube.com/watch?v=m5NfgXP76Vw&amp;index=1&amp;list=PLFjlI7p-h1hxBP3cIjEqePSeoBDHud5Db&amp;t=103s">How we found a million style and grammar errors in the English Wikipedia</a></li>
<li><a href="https://www.youtube.com/watch?v=56oNQf5oITw&amp;list=PLFjlI7p-h1hxBP3cIjEqePSeoBDHud5Db&amp;t=1300s&amp;index=47">Simple Testing Can Prevent Most Critical Failures</a></li>
<li><a href="https://www.youtube.com/watch?v=nCGBgI1MNwE&amp;list=PLFjlI7p-h1hxBP3cIjEqePSeoBDHud5Db&amp;index=60">Need it robust, make it fragile</a><ul>
<li>btw its me who asks the first question at the end :)</li>
</ul>
</li>
</ul>
<blockquote>
<p>INSANITY -
DOING THE SAME THING
OVER AND OVER
AND EXPECTING
DIFFERENT RESULTS.</p>
</blockquote>
<ul>
<li><a href="http://atodorov.org/blog/2016/12/28/4-quick-wins-to-manage-the-cost-of-software-testing/">4 Quick Wins to Manage the Cost of Software Testing</a></li>
</ul>
<p>This principle can be applied to any team/process within the organization.
The above link is reference to a nice book which was recommended to me but the
gist of it is that we always need to analyze, ask questions and change is we want
to achieve great results. A practicle example of what is possible if you follow
this principle is this talk
<a href="https://www.youtube.com/watch?v=khSsjjg2eSQ&amp;index=1&amp;list=PLFjlI7p-h1hxBP3cIjEqePSeoBDHud5Db">Accelerate Automation Tests From 3 Hours to 3 Minutes</a>.</p>
<blockquote>
<p>THE ONLY
REASON FOR TIME
IS SO THAT
EVERYTHING DOESN'T
HAPPEN AT ONCE.</p>
</blockquote>
<p>The topic here is "using tests as a monitoring tool".
This is something I started a while back ago, helping a prominent startup with their
production testing but my involvement ended very soon after the framework was
deployed live so I don't have lots of insight.</p>
<p>As the first few days this technique identified some unexpected behaviors,
for example a 3rd party service was updating very often. Once even they were
broken for a few hours - something nobody had information about.</p>
<p>Since then I've heard about 2 more companies using similar techniques to continuously
validate that production software continues to work without having a physical
person to verify it. In the event of failures there are alerts which are
delath with accordingly.</p>
<blockquote>
<p>NO PROBLEM
CAN BE SOLVED FROM
THE SAME LEVEL
OF CONSIOUSNESS
THAT CREATED IT.</p>
</blockquote>
<p>That much must be obvious to us quality engineers. What about the future however?</p>
<ul>
<li><a href="https://www.quora.com/How-do-I-implement-AI-in-test-automation">How do I implement AI in test automation</a></li>
<li><a href="http://sourced.tech/">source{d} - Building the first AI that understands code</a></li>
</ul>
<p>I don't have anything more concrete here. Just looking towards what is coming next!</p>
<blockquote>
<p>DO NOT WORRY ABOUT
YOUR DIFFICULTIES
IN MATHEMATICS.
I CAN ASSURE
YOU MINE ARE STILL
GREATER.</p>
</blockquote>
<p>Thanks for reading and happy testing!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Thu 20 April 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/04/20/quality-assurance-according-2-einstein/#disqus_thread">comments</a>.</p>        </div>

    <hr>
    <!-- Pager -->
    <ul class="pager">
        <li class="next">
                <a href="http://atodorov.org/blog/categories/qa/index2.html">Older Posts &rarr;</a>
        </li>
    </ul>
    Page 1 / 7
    <hr>
            </div>
        </div>
    </div>

    <hr>

    <!-- Footer -->
    <footer>
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <ul class="list-inline text-center">
                            <li>
                                <a href="https://twitter.com/atodorov_">
                                    <span class="fa-stack fa-lg">
                                        <i class="fa fa-circle fa-stack-2x"></i>
                                        <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li>
                                <a href="https://github.com/atodorov">
                                    <span class="fa-stack fa-lg">
                                        <i class="fa fa-circle fa-stack-2x"></i>
                                        <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li>
                                <a href="https://bg.linkedin.com/in/alextodorov">
                                    <span class="fa-stack fa-lg">
                                        <i class="fa fa-circle fa-stack-2x"></i>
                                        <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li>
                                <a href="http://feeds.feedburner.com/atodorov">
                                    <span class="fa-stack fa-lg">
                                        <i class="fa fa-circle fa-stack-2x"></i>
                                        <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li>
                                <a href="https://www.youtube.com/playlist?list=PLFjlI7p-h1hxBP3cIjEqePSeoBDHud5Db">
                                    <span class="fa-stack fa-lg">
                                        <i class="fa fa-circle fa-stack-2x"></i>
                                        <i class="fa fa-youtube fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li>
                                <a href="http://amzn.to/1ivu2q4">
                                    <span class="fa-stack fa-lg">
                                        <i class="fa fa-circle fa-stack-2x"></i>
                                        <i class="fa fa-amazon fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li>
                                <a href="http://mrsenko.com/?utm_source=atodorov.org&utm_medium=blog&utm_campaign=social_icon">
                                    <span class="fa-stack fa-lg">
                                        <i class="fa fa-circle fa-stack-2x"></i>
                                        <i class="fa fa-user-secret fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                    </ul>
<section>
    <p>
        I am the project lead for
        <a href="http://kiwitcms.org/?utm_source=atodorov.org&utm_medium=blog&utm_campaign=footer">Kiwi TCMS</a>
        and the current maintainer of <a href="http://MrSenko.com/pylint-workshop/">pylint-django</a>!
    </p>
    <p>
        <small>
            <em>
                Some of the links contained within this site have my referral id (e.g.,
                <a target="_blank" href="http://www.amazon.com/ref=as_li_ss_tl?_encoding=UTF8&camp=1789&creative=390957&linkCode=ur2&tag=atodorovorg-20&linkId=L6Q34XAXQS5RDMOY">Amazon</a><img src="https://ir-na.amazon-adsystem.com/e/ir?t=atodorovorg-20&l=ur2&o=1" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />),
                which provides me with a small commission for each sale. Thank you for your support.
            </em>
        </small>
    </p>
</section>

<form action="http://google.com/search" method="get" style="width:300px;margin:0 auto;">
    <fieldset role="search">
        <input type="hidden" name="sitesearch" value="http://atodorov.org" />
        <input class="search" type="text" name="q" placeholder="Search" style="width:100%"/>
    </fieldset>
</form>

<p class="copyright text-muted">
    <a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/deed.en_US">CC-BY-SA</a> &amp;
    <a rel="license" href="http://opensource.org/licenses/MIT">MIT</a>
    2011-2018 &diams; Alexander Todorov
</p>

<script type='text/javascript'>
window.__lo_site_id = 55936;
    (function() {
        var wa = document.createElement('script'); wa.type = 'text/javascript'; wa.async = true;
        wa.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://cdn') + '.luckyorange.com/w.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(wa, s);
      })();
</script>
                </div>
            </div>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="http://atodorov.org/theme/js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="http://atodorov.org/theme/js/bootstrap.min.js"></script>


    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-37979549-1']);
    _gaq.push(['_trackPageview']);
    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
    </script>
<script type="text/javascript">
    var disqus_shortname = 'atodorov';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
</body>

</html>