<!DOCTYPE html>
<html lang="en">

<head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">

            <meta name="google-site-verification" content="XynqZtldWNBbmsynVQZremIxaaO8Wgs6AGR8UZ7KIkM">

        <title>atodorov.org - Tag QA</title>

        <link href="http://feeds.feedburner.com/atodorov" type="application/atom+xml" rel="alternate" title="atodorov.org Full Atom Feed" />
        <!-- Bootstrap Core CSS -->
        <link href="http://atodorov.org/theme/css/bootstrap.min.css" rel="stylesheet">

        <!-- Custom CSS -->
        <link href="http://atodorov.org/theme/css/clean-blog.min.css" rel="stylesheet">

        <!-- Code highlight color scheme -->
            <link href="http://atodorov.org/theme/css/code_blocks/github.css" rel="stylesheet">

            <!-- CSS specified by the user -->
            <link href="http://atodorov.org/override.css" rel="stylesheet">

        <!-- Custom Fonts -->
        <link href="http://atodorov.org/theme/css/font-awesome.min.css" rel="stylesheet" type="text/css">
        <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
        <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

        <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->

                <meta property="fb:admins" content="1616937247" >
                <meta property="og:locale" content="en_US">
		<meta property="og:site_name" content="atodorov.org">
            <meta name="twitter:card" content="summary_large_image">
            <meta name="twitter:site" content="@atodorov_">
            <meta name="twitter:title" content="atodorov.org">
            <meta name="twitter:description" content="you can logoff, but you can never leave">
                <meta name="twitter:image" content="http://atodorov.org//images/header_02.jpg">
</head>

<body>

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-custom navbar-fixed-top">
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="http://atodorov.org/">atodorov.org</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                        <li><a href="http://mrsenko.com/?utm_source=atodorov.org&utm_medium=blog&utm_campaign=menu">Mr. Senko</a></li>

                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

        <header class="intro-header" style="background-image: url('/images/header_02.jpg')">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <div class="page-heading">
                        <h1>Tag QA</h1>
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/10/02/more-tests-for-login-forms/" rel="bookmark" title="Permalink to More tests for login forms">
                <h2 class="post-title">
                    More tests for login forms
                </h2>
            </a>
                <p><img alt="&quot;Telenor's login form&quot;" src="/images/telenor_login.png" title="Telenor's login form" /></p>
<p>By now I probably have documented more test cases for login forms than anyone
else. You can check out my previous posts on the topic
<a href="http://atodorov.org/blog/2016/04/12/how-to-hire-software-testers-pt-1/">here</a> and
<a href="http://atodorov.org/blog/2017/06/14/vmwares-favorite-login-form/">here</a>. I give you a few more
examples.</p>
<p>Test 01 and 02:
First of all let's start by saying that a "Remember me" checkbox should actually
remember the user and login them automatically on the next visit if checked. The
other way around if not checked. I don't think this has been mentioned previously!</p>
<p>Test 03:
When there is a "Remember me" checkbox it should be selectable both with the mouse
and the keyboard. On my.telenor.bg the checkbox changes its image only when
clicked with the mouse. Also clicking the login button with Space doesn't work!</p>
<p>Interestingly enough when I don't select "Remember me" at all and close then
revisit the page I am still able to access the internal pages of my account!
At this point I'm not quite sure what this checkbox does!</p>
<p>Test 04:
Testing two factor authentication. I had the case where GitHub SMS didn't
arrive for over 24 hrs and I wasn't able to login. After requesting a new code
you can see the UI updating but I didn't receive another message. In this particular
case I received only one message with an already invalid code. So test for:</p>
<ul>
<li>how long does it take for the codes to expire</li>
<li>is there a visual feedback indicating how many codes have been requested</li>
<li>do latest code invalidates all the previous ones or all that have been unused
  still work</li>
<li>what happens if I'm already logged in and somebody tries to access my account
  requesting additional codes which may or may not invalidate my login session?</li>
</ul>
<p>Test 05:
Check that confirmation codes, links, etc will actually expire after their
configured time. Kiwi TCMS had this problem which has been fixed in
<a href="https://github.com/kiwitcms/Kiwi/commit/92162112bf2214b8eacf37ba3a796414b129a700#diff-353aa238f7ee459b1236e2a21f1142ba">version 3.32</a>.</p>
<p>Test 06:
Is this a social-network-login only site? Then which of my profiles did I use?
Check that there is a working
<a href="http://atodorov.org/blog/2013/03/14/django-social-auth-tip-reminder-of-login-provider/">social auth provider reminder</a>.</p>
<p>Test 07:
Check that there is an error message visible (e.g. wrong login credentials).
After the redesign Kiwi TCMS had stopped displaying this message and instead
presents the user with the login form again!</p>
<p>Also checkout these
<a href="http://testingchallenges.thetestingmap.org/index.php">testing challenges</a>
by Claudiu Draghia where you can see many cases related to input field
validation! For example empty field, value too long, special characters in field, etc.
All of these can lead to issues depending on how login is implemented.</p>
<p>Thanks for reading and happy testing!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Mon 02 October 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/10/02/more-tests-for-login-forms/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/09/08/xiaomis-selfie-bug/" rel="bookmark" title="Permalink to Xiaomi's selfie bug">
                <h2 class="post-title">
                    Xiaomi's selfie bug
                </h2>
            </a>
                <p>Recently I've been exploring the user interface of a Xiaomi Redmi Note 4X
phone and noticed a peculiar bug, adding to my collection of
<a href="http://atodorov.org/blog/2013/03/19/bug-in-nokia-software-shows-wrong-caller-id/">obscure phone bugs</a>.
Sometimes when taking selfies the images
will not be saved in the correct orientation. Instead they will be saved as
if looking in the mirror and this is a bug!</p>
<p><img alt="&quot;Samsung S5 front screen&quot;" src="/images/samsung_s5_front_screen.jpg" title="Samsung S5 front screen" /></p>
<p>While taking the selfie the display correctly acts as a mirror, see my personal
Samsung S5 (black) and the Xiaomi device (white).</p>
<p><img alt="&quot;Xiaomi front screen&quot;" src="/images/xiaomi_front_screen.jpg" title="Xiaomi front screen" /></p>
<p>However when the image is saved and then viewed through the gallery application
there is a difference. The image below is taken with the Xiaomi device and there
have been no effects added to it except scaling and cropping. As you can see
the letters on the cereal box are mirrored!</p>
<p><img alt="&quot;Xiaomi mirrored image&quot;" src="/images/xiaomi_adi_mirrored.jpeg" title="Xiaomi mirrored image" /></p>
<p>The symptoms of the bug are not quite clear as of yet. I've managed to reproduce at
around 50% rate so far. I've tried taking pictures during the day in direct sunlight
and in the shade, also in the evening under bad artificial lighting.
Taking photo of a child's face and then child plus varying number of adults.
Then photo of only 1 or more adults, heck I even made a picture of myself. I though that
lighting or the number of faces and their age have something to do with this bug
but so far I'm not getting consistent results. Sometimes the images turn out OK
and other times they don't regardless of what I take a picture of.</p>
<p>I also took a picture of the same cereal box, under the same conditions as above but
not capturing the child's face and the image came out not mirrored. The only clue
that seems to hold true so far is that you need to have people's faces in the picture
for this bug to reproduce but that isn't an edge case when taking selfies, right?</p>
<p>I've also compared the results with my Samsung S5 (Android version 6.0.1) and BlackBerry Z10 devices
and both work as expected: while taking the picture the display acts as a mirror
but when viewing the saved image it appears in normal orientation. On S5 there is
also a clearly visible "Processing" progress bar while the picture is being saved!</p>
<p>For reference the system information is below:</p>
<div class="highlight"><pre>Model number: Redmi Note 4X
Android version: 6.0 MRA58K
Android security patch level: 2017-03-01
Kernel version: 3.18.22+
</pre></div>


<p>I'd love if somebody
from Xiaomi's engineering department looks into this and sends me a root cause analysis
of the problem.</p>
<p>Thanks for reading and happy testing! Oh and btw this is my breakfast, not hers!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Fri 08 September 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/09/08/xiaomis-selfie-bug/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/08/30/speeding-up-rust-builds-inside-docker/" rel="bookmark" title="Permalink to Speeding up Rust builds inside Docker">
                <h2 class="post-title">
                    Speeding up Rust builds inside Docker
                </h2>
            </a>
                <p>Currently <a href="https://github.com/rust-lang/cargo/pull/3567">it is not possible</a>
to instruct <code>cargo</code>, the Rust package manager, to build only the dependencies
of the software you are compiling! This means you can't easily pre-install
build dependencies. Luckily you can workaround this with <code>cargo build -p</code>!
I've been using this Python script to parse <code>Cargo.toml</code>:</p>
<div class="highlight"><span class="filename">parse-cargo-toml.py</span><pre><span class="c">#!/usr/bin/env python</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">toml</span>

<span class="n">_pwd</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">__file__</span><span class="p">))</span>
<span class="n">cargo</span> <span class="o">=</span> <span class="n">toml</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">_pwd</span><span class="p">,</span> <span class="s">&#39;Cargo.toml&#39;</span><span class="p">),</span> <span class="s">&#39;r&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>

<span class="k">for</span> <span class="n">section</span> <span class="ow">in</span> <span class="p">[</span><span class="s">&#39;dependencies&#39;</span><span class="p">,</span> <span class="s">&#39;dev-dependencies&#39;</span><span class="p">]:</span>
    <span class="k">for</span> <span class="n">dep</span><span class="p">,</span> <span class="n">version</span> <span class="ow">in</span> <span class="n">cargo</span><span class="p">[</span><span class="n">section</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">print</span><span class="p">(</span><span class="s">&#39;cargo build -p </span><span class="si">%s</span><span class="s">&#39;</span> <span class="o">%</span> <span class="n">dep</span><span class="p">)</span>
</pre></div>


<p>and then inside my <code>Dockerfile</code>:</p>
<div class="highlight"><pre><span class="x">RUN mkdir /bdcs-api-rs/</span>
<span class="x">COPY parse-cargo-toml.py /bdcs-api-rs/</span>

<span class="err">#</span><span class="x"> Manually install cargo dependencies before building</span>
<span class="err">#</span><span class="x"> so we can have a reusable intermediate container.</span>
<span class="err">#</span><span class="x"> This workaround is needed until cargo can do this by itself:</span>
<span class="err">#</span><span class="x"> https://github.com/rust-lang/cargo/issues/2644</span>
<span class="err">#</span><span class="x"> https://github.com/rust-lang/cargo/pull/3567</span>
<span class="x">COPY Cargo.toml /bdcs-api-rs/</span>
<span class="x">WORKDIR /bdcs-api-rs/</span>
<span class="x">RUN python ./parse-cargo-toml.py | while read cmd; do \</span>
<span class="x">        </span><span class="p">$</span><span class="nv">cmd</span><span class="x">;                                    \</span>
<span class="x">    done</span>
</pre></div>


<p>It doesn't take into account the version constraints specified in <code>Cargo.toml</code> but
is still able to produce an intermediate docker layer which I can use to
<a href="http://atodorov.org/blog/2017/08/07/faster-travis-ci-tests-with-docker-cache/">speed-up my tests by caching the dependency compilation part</a>.</p>
<p>As seen in the <a href="https://travis-ci.org/weldr/bdcs-api-rs/builds/268489460#L1173">build log</a>,
lines 1173-1182, when doing <code>cargo build</code> it downloads and compiles <code>chrono v0.3.0</code> and
<code>toml v0.3.2</code>. The rest of the dependencies are already available. The logs also show
that after Job #285 the build times dropped from 16 minutes down to 3-4 minutes due to
Docker caching. This would be even less if the cache is kept locally!</p>
<p>Thanks for reading and happy testing!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Wed 30 August 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/08/30/speeding-up-rust-builds-inside-docker/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/08/12/code-coverage-from-nightmarejs-tests/" rel="bookmark" title="Permalink to Code coverage from Nightmare.js tests">
                <h2 class="post-title">
                    Code coverage from Nightmare.js tests
                </h2>
            </a>
                <p>In this article I'm going to walk you through the steps required
to collect code coverage when running an end-to-end test suite
against a React.js application.</p>
<p>The application under test looks like this</p>
<div class="highlight"><pre><span class="cp">&lt;!doctype html&gt;</span>
<span class="nt">&lt;html</span> <span class="na">lang=</span><span class="s">&quot;en-us&quot;</span> <span class="na">class=</span><span class="s">&quot;layout-pf layout-pf-fixed&quot;</span><span class="nt">&gt;</span>
  <span class="nt">&lt;head&gt;</span>
    <span class="c">&lt;!-- js dependencies skipped --&gt;</span>
  <span class="nt">&lt;/head&gt;</span>
  <span class="nt">&lt;body&gt;</span>
    <span class="nt">&lt;div</span> <span class="na">id=</span><span class="s">&quot;main&quot;</span><span class="nt">&gt;&lt;/div&gt;</span>
    <span class="nt">&lt;script </span><span class="na">src=</span><span class="s">&quot;./dist/main.js?0ca4cedf3884d3943762&quot;</span><span class="nt">&gt;&lt;/script&gt;</span>
  <span class="nt">&lt;/body&gt;</span>
<span class="nt">&lt;/html&gt;</span>
</pre></div>


<p>It is served as an <code>index.html</code> file and a <code>main.js</code> file which intercepts
all interactions from the user and sends requests to the backend API when
needed.</p>
<p>There is an existing unit-test suite which loads the individual components
and tests them in isolation.
<a href="https://twitter.com/atodorov_/status/886881560754102272">Apparently people do this</a>!</p>
<p>There is also an end-to-end test suite which does the majority of the testing.
It fires up a browser instance and interacts with the application. Everything
runs inside Docker containers providing a full-blown production-like environment.
They look like this</p>
<div class="highlight"><pre><span class="nx">test</span><span class="p">(</span><span class="s1">&#39;should switch to Edit Recipe page - recipe creation success&#39;</span><span class="p">,</span> <span class="p">(</span><span class="nx">done</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
  <span class="kr">const</span> <span class="nx">nightmare</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">Nightmare</span><span class="p">();</span>
  <span class="nx">nightmare</span>
    <span class="p">.</span><span class="kr">goto</span><span class="p">(</span><span class="nx">recipesPage</span><span class="p">.</span><span class="nx">url</span><span class="p">)</span>
    <span class="p">.</span><span class="nx">wait</span><span class="p">(</span><span class="nx">recipesPage</span><span class="p">.</span><span class="nx">btnCreateRecipe</span><span class="p">)</span>
    <span class="p">.</span><span class="nx">click</span><span class="p">(</span><span class="nx">recipesPage</span><span class="p">.</span><span class="nx">btnCreateRecipe</span><span class="p">)</span>
    <span class="p">.</span><span class="nx">wait</span><span class="p">(</span><span class="nx">page</span> <span class="o">=&gt;</span> <span class="nb">document</span><span class="p">.</span><span class="nx">querySelector</span><span class="p">(</span><span class="nx">page</span><span class="p">.</span><span class="nx">dialogRootElement</span><span class="p">).</span><span class="nx">style</span><span class="p">.</span><span class="nx">display</span> <span class="o">===</span> <span class="s1">&#39;block&#39;</span>
      <span class="p">,</span> <span class="nx">createRecipePage</span><span class="p">)</span>
    <span class="p">.</span><span class="nx">insert</span><span class="p">(</span><span class="nx">createRecipePage</span><span class="p">.</span><span class="nx">inputName</span><span class="p">,</span> <span class="nx">createRecipePage</span><span class="p">.</span><span class="nx">varRecName</span><span class="p">)</span>
    <span class="p">.</span><span class="nx">insert</span><span class="p">(</span><span class="nx">createRecipePage</span><span class="p">.</span><span class="nx">inputDescription</span><span class="p">,</span> <span class="nx">createRecipePage</span><span class="p">.</span><span class="nx">varRecDesc</span><span class="p">)</span>
    <span class="p">.</span><span class="nx">click</span><span class="p">(</span><span class="nx">createRecipePage</span><span class="p">.</span><span class="nx">btnSave</span><span class="p">)</span>
    <span class="p">.</span><span class="nx">wait</span><span class="p">(</span><span class="nx">editRecipePage</span><span class="p">.</span><span class="nx">componentListItemRootElement</span><span class="p">)</span>
    <span class="p">.</span><span class="nx">exists</span><span class="p">(</span><span class="nx">editRecipePage</span><span class="p">.</span><span class="nx">componentListItemRootElement</span><span class="p">)</span>
    <span class="p">.</span><span class="nx">end</span><span class="p">()</span> <span class="c1">// remove this!</span>
    <span class="p">.</span><span class="nx">then</span><span class="p">((</span><span class="nx">element</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
      <span class="nx">expect</span><span class="p">(</span><span class="nx">element</span><span class="p">).</span><span class="nx">toBe</span><span class="p">(</span><span class="kc">true</span><span class="p">);</span>
      <span class="c1">// here goes coverage collection helper</span>
      <span class="nx">done</span><span class="p">();</span> <span class="c1">// remove this!</span>
    <span class="p">});</span>
<span class="p">},</span> <span class="nx">timeout</span><span class="p">);</span>
</pre></div>


<p>The browser interaction is handled by Nightmare.js (sort of like Selenium) and
the test runner is Jest.</p>
<h2>Code instrumentation</h2>
<p>The first thing we need is to instrument the application code to provide coverage
statistics. This is done via <code>babel-plugin-istanbul</code>. Because unit-tests are
executed a bit differently we want to enable conditional instrumentation. In reality
for unit tests we use <code>jest --coverage</code> which enables istanbul on the fly and having
the code already instrumented breaks this. So I have the following in <code>webpack.config.js</code></p>
<div class="highlight"><pre><span class="k">if</span> <span class="p">(</span><span class="nx">process</span><span class="p">.</span><span class="nx">argv</span><span class="p">.</span><span class="nx">includes</span><span class="p">(</span><span class="s1">&#39;--with-coverage&#39;</span><span class="p">))</span> <span class="p">{</span>
  <span class="nx">babelConfig</span><span class="p">.</span><span class="nx">plugins</span><span class="p">.</span><span class="nx">push</span><span class="p">(</span><span class="s1">&#39;istanbul&#39;</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>


<p>and then build my application with <code>node run build --with-coverage</code>.</p>
<p>You can execute <code>node run start --with-coverage</code>, open the JavaScript console
in your browser and inspect the <code>window.__coverage__</code> variable. If this is defined
then the application is instrumented correctly.</p>
<h2>Fetching coverage information from within the tests</h2>
<p>Remember that <code>main.js</code> from the beginning of this post? It lives inside <code>index.html</code>
which means everything gets downloaded to the client side and executed there.
When running the end-to-end test suite that is the browser instance which is controlled
via Nightmare. <strong>You have to pass <code>window.__coverage__</code> from the browser scope back to
nodejs scope via <code>nightmare.evaluate()</code></strong>! I opted to directly save the coverage data
on the file system and make it available to coverage reporting tools later!</p>
<p>My coverage collecting snippet looks like this</p>
<div class="highlight"><pre><span class="nx">nightmare</span>
  <span class="p">.</span><span class="nx">evaluate</span><span class="p">(()</span> <span class="o">=&gt;</span> <span class="nb">window</span><span class="p">.</span><span class="nx">__coverage__</span><span class="p">)</span> <span class="c1">// this executes in browser scope</span>
  <span class="p">.</span><span class="nx">end</span><span class="p">()</span> <span class="c1">// terminate the Electron (browser) process</span>
  <span class="p">.</span><span class="nx">then</span><span class="p">((</span><span class="nx">cov</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
    <span class="c1">// this executes in Node scope</span>
    <span class="c1">// handle the data passed back to us from browser scope</span>
    <span class="kr">const</span> <span class="nx">strCoverage</span> <span class="o">=</span> <span class="nx">JSON</span><span class="p">.</span><span class="nx">stringify</span><span class="p">(</span><span class="nx">cov</span><span class="p">);</span>
    <span class="kr">const</span> <span class="nx">hash</span> <span class="o">=</span> <span class="nx">require</span><span class="p">(</span><span class="s1">&#39;crypto&#39;</span><span class="p">).</span><span class="nx">createHmac</span><span class="p">(</span><span class="s1">&#39;sha256&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
      <span class="p">.</span><span class="nx">update</span><span class="p">(</span><span class="nx">strCoverage</span><span class="p">)</span>
      <span class="p">.</span><span class="nx">digest</span><span class="p">(</span><span class="s1">&#39;hex&#39;</span><span class="p">);</span>
    <span class="kr">const</span> <span class="nx">fileName</span> <span class="o">=</span> <span class="err">`</span><span class="o">/</span><span class="nx">tmp</span><span class="o">/</span><span class="nx">coverage</span><span class="o">-</span><span class="nx">$</span><span class="p">{</span><span class="nx">hash</span><span class="p">}.</span><span class="nx">json</span><span class="err">`</span><span class="p">;</span>
    <span class="nx">require</span><span class="p">(</span><span class="s1">&#39;fs&#39;</span><span class="p">).</span><span class="nx">writeFileSync</span><span class="p">(</span><span class="nx">fileName</span><span class="p">,</span> <span class="nx">strCoverage</span><span class="p">);</span>

    <span class="nx">done</span><span class="p">();</span> <span class="c1">// the callback from the test</span>
  <span class="p">})</span>
<span class="p">.</span><span class="k">catch</span><span class="p">(</span><span class="nx">err</span> <span class="o">=&gt;</span> <span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="nx">err</span><span class="p">));</span>
</pre></div>


<p>Nightmare returns <code>window.__coverage__</code> from browser scope back to nodejs scope
and we save it under <code>/tmp</code> using a hash value of the coverage data as the file
name.</p>
<p><em>Side note:</em> I do have about 40% less coverage files than number of test cases.
This means some test scenarios exercise the same code paths. Storing the individual
coverage reports under a hashed file name makes this very easy to see!</p>
<p>Note that in my coverage handling code I also call <code>.end()</code> which will terminate
the browser instance and also execute the <code>done()</code> callback which is being passed
as parameter to the test above! This is important because it means we had to update
the way tests were written. In particular the Nightmare method sequence doesn't
have to call <code>.end()</code> and <code>done()</code> except in the coverage handling code. The
coverage helper must be the last code executed inside the body of the last
<code>.then()</code> method. This is usually after all assertions (expectations) have been met!</p>
<p>Now this coverage helper needs to be part of every single test case so I
wanted it to be a one line function, easy to copy&amp;paste! All my attempts to
move this code inside a module have been futile. I can get the module loaded
but it kept failing with
<code>Unhandled promise rejection (rejection id: 1): cov_23rlop1885 is not defined</code>;`</p>
<p>At the end I've resorted to this simple hack</p>
<div class="highlight"><pre><span class="nb">eval</span><span class="p">(</span><span class="nx">fs</span><span class="p">.</span><span class="nx">readFileSync</span><span class="p">(</span><span class="s1">&#39;utils/coverage.js&#39;</span><span class="p">).</span><span class="nx">toString</span><span class="p">());</span>
</pre></div>


<p>Shout-out to <a href="http://krasimirtsonev.com/">Krasimir Tsonev</a> who joined me on a two
days pairing session to figure this stuff out. Too bad we couldn't quite figure it
out. If you do please send me a pull request!</p>
<h2>Reporting the results</h2>
<p>All of these <code>coverage-*.json</code> files are directly consumable by <code>nyc</code> - the
coverage reporting tool that comes with the Istanbul suite! I mounted
<code>.nyc_output/</code> directly under <code>/tmp</code> inside my Docker container so I could</p>
<div class="highlight"><pre>nyc report
nyc report --reporter=lcov | codecov
</pre></div>


<p>We can also modify the unit-test command to
<code>jest --coverage --coverageReporters json --coverageDirectory .nyc_output</code> so it
produces a <code>coverage-final.json</code> file for <code>nyc</code>. Use this if you want to combine
the coverage reports from both test suites.</p>
<p>Because I'm using Travis CI the two test suites are executed independently and
there is no easy way to share information between them. Instead I've switched
from Coveralls to CodeCov which is smart enough to merge coverage submissions
coming from multiple jobs on the same git commits. You can compare the commit
<a href="https://codecov.io/gh/atodorov/welder-web/commit/46556808e42a21f48d008ced2d53ffe176c01b6d">submitting only unit-test results</a>
with the one
<a href="https://codecov.io/gh/atodorov/welder-web/commit/15f437477c17b63797cdb2455f1371336d7dc0e5">submitting coverage from both test suites</a>.</p>
<p>All of the above steps are put into practice in
<a href="https://github.com/weldr/welder-web/pull/136">PR #136</a> if you want to check them out!</p>
<p>Thanks for reading and happy testing!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Sat 12 August 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/08/12/code-coverage-from-nightmarejs-tests/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/08/07/faster-travis-ci-tests-with-docker-cache/" rel="bookmark" title="Permalink to Faster Travis CI tests with Docker cache">
                <h2 class="post-title">
                    Faster Travis CI tests with Docker cache
                </h2>
            </a>
                <p>For a while now I've been running tests on Travis CI using Docker
containers to build the project and execute the tests inside. In this
post I will explain how to speed up execution times.</p>
<p>A Docker image is a filesystem snapshot similar to a virtual machine
image. From these images we build containers (e.g. we run the container X
from the image Y). The construction of Docker images is controlled via
<code>Dockerfile</code> which contains a set of instructions how to build the image.
For example:</p>
<div class="highlight"><pre>FROM welder/web-nodejs:latest
MAINTAINER Brian C. Lane &lt;bcl@redhat.com&gt;
RUN dnf install -y nginx

CMD nginx -g &quot;daemon off;&quot;
EXPOSE 3000

## Do the things more likely to change below here. ##

COPY ./docker/nginx.conf /etc/nginx/

# Update node dependencies only if they have changed
COPY ./package.json /welder/package.json
RUN cd /welder/ &amp;&amp; npm install

# Copy the rest of the UI files over and compile them
COPY . /welder/
RUN cd /welder/ &amp;&amp; node run build

COPY entrypoint.sh /usr/local/bin/entrypoint.sh
ENTRYPOINT [&quot;/usr/local/bin/entrypoint.sh&quot;]
</pre></div>


<p><code>docker build</code> is smart enough to actually build intermediate layers for each
command and store them on your computer. Each command is hashed and it is rebuilt
only if it has been changed. Thus the stuff which doesn't change often goes first
(like setting up a web server or a DB) and the stuff that changes (like the project source code)
goes at the end. All of this is beautifully explained by <a href="https://www.youtube.com/watch?v=3a0gVrfmWC8">Stefan Kanev in
this video</a> (in Bulgarian).</p>
<h2>Travis and Docker</h2>
<p>While intermediate layer caching is a standard feature for Docker it is disabled
by default in Travis CI and any other CI service I was able to find. To be frank
Circles CI offer this as a premium feature but their pricing plans on that aren't
clear at all.</p>
<p>However you can enable the use of caching following a few simple steps:</p>
<ol>
<li>Make your Docker images publicly available (e.g. Docker Hub or Amazon EC2 Container Service)</li>
<li>Before starting the test job do a <code>docker pull my/image:latest</code></li>
<li>When building your Docker images in Travis add <code>--cache-from my/image:latest</code> to <code>docker build</code></li>
<li>After successful execution <code>docker tag</code> the latest image with the build job number and
   <code>docker push</code> it again to the hub!</li>
</ol>
<p><strong>NOTES:</strong></p>
<ul>
<li>Everything you do will become public so take care not to expose internal code.
  Alternatively you may configure a private docker registry (e.g. Amazon EC2 CS)
  and use encrypted passwords for Travis to access your images;</li>
<li><code>docker pull</code> will download all layers that it needs. If your hosting is slow
  this will negatively impact execution times;</li>
<li><code>docker push</code> will upload only the layers that have been changed;</li>
<li>I only push images coming from the master branch which are not from a pull request
  build job. This prevents me from accidentally messing something up.</li>
</ul>
<p>If you examine the logs of <a href="https://travis-ci.org/weldr/welder-web/jobs/260970675">Job #247.4</a>
and <a href="https://travis-ci.org/weldr/welder-web/jobs/261732264">Job #254.4</a> you will notice
that almost all intermediate layers were re-used from cache:</p>
<div class="highlight"><pre>Step 3/12 : RUN dnf install -y nginx
 ---&gt; Using cache
 ---&gt; 25311f052381
Step 4/12 : CMD nginx -g &quot;daemon off;&quot;
 ---&gt; Using cache
 ---&gt; 858606811c85
Step 5/12 : EXPOSE 3000
 ---&gt; Using cache
 ---&gt; d778cbbe0758
Step 6/12 : COPY ./docker/nginx.conf /etc/nginx/
 ---&gt; Using cache
 ---&gt; 56bfa3fa4741
Step 7/12 : COPY ./package.json /welder/package.json
 ---&gt; Using cache
 ---&gt; 929f20da0fc1
Step 8/12 : RUN cd /welder/ &amp;&amp; npm install
 ---&gt; Using cache
 ---&gt; 68a30a4aa5c6
</pre></div>


<p>Here the slowest operations are <code>dnf install</code> and <code>npm install</code> which on normal execution
will take around 5 minutes.</p>
<p>You can check-out my
<a href="https://github.com/weldr/welder-web/blob/master/.travis.yml">.travis.yml</a> for more info.</p>
<h2>First time cache</h2>
<p>It is important to note that you need to have your docker images available in the
registry before you execute the first <code>docker pull</code> from CI. I do this by manually building
the images on my computer and uploading them before configuring CI integration. Afterwards
the CI system takes care of updating the images for me.</p>
<p>Initially you may not notice a significant improvement as seen in
<a href="https://travis-ci.org/weldr/bdcs-api-rs/builds/261510313">Job #262</a>, Step 18/22.
The initial image available on Docker Hub has all the build dependencies installed
and the code has not been changed when job #262 was executed.</p>
<p>The <code>COPY</code> command copies the entire contents of the directory, including filesystem metadata!
Things like uid/gid (file ownership), timestamps (not sure if taken into account)
and/or extended attributes (e.g. SELinux)
will cause the intermediate layers checksums to differ even though the actual
source code didn't change. This will resolve itself once your CI system starts automatically
pushing the latest images to the registry.</p>
<p>Thanks for reading and happy testing!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Mon 07 August 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/08/07/faster-travis-ci-tests-with-docker-cache/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/08/04/transactionmanagementerror-during-testing-with-django-110/" rel="bookmark" title="Permalink to TransactionManagementError during testing with Django 1.10">
                <h2 class="post-title">
                    TransactionManagementError during testing with Django 1.10
                </h2>
            </a>
                <p>During the past 3 weeks I've been debugging a weird error which
started happening after I migrated <a href="http://MrSenko.com/kiwi/">KiwiTestPad</a> to
Django 1.10.7. Here is the reason why this happened.</p>
<h2>Symptoms</h2>
<p>After migrating to Django 1.10 all tests appeared to be working locally
on SQLite however they
<a href="https://travis-ci.org/MrSenko/Kiwi/jobs/258309883">failed on MySQL</a> with</p>
<div class="highlight"><pre><span class="n">TransactionManagementError</span><span class="o">:</span> <span class="n">An</span> <span class="n">error</span> <span class="n">occurred</span> <span class="k">in</span> <span class="n">the</span> <span class="n">current</span> <span class="n">transaction</span><span class="o">.</span> <span class="n">You</span> <span class="n">can</span><span class="s1">&#39;t execute queries until the end of the &#39;</span><span class="n">atomic</span><span class="err">&#39;</span> <span class="n">block</span><span class="o">.</span>
</pre></div>


<p>The exact same test cases
<a href="https://travis-ci.org/MrSenko/Kiwi/jobs/258309884">failed on PostgreSQL</a> with:</p>
<div class="highlight"><pre><span class="n">InterfaceError</span><span class="o">:</span> <span class="n">connection</span> <span class="n">already</span> <span class="n">closed</span>
</pre></div>


<p>Since version 1.10 Django executes all tests inside transactions so my first
thoughts were related to the auto-commit mode. However upon closer inspection
we can see that the line which triggers the failure is</p>
<div class="highlight"><pre>self.assertTrue(users.exists())
</pre></div>


<p>which is essentially a <code>SELECT</code> query aka
<code>User.objects.filter(username=username).exists()</code>!</p>
<p><strong>My tests were failing on a SELECT query!</strong></p>
<p>Reading the numerous posts about <code>TransactionManagementError</code> I discovered it may
be caused by a run-away cursor. The application did use raw SQL statements which
I've converted promptly to ORM queries, that took me some time. Then I also fixed
a couple of places where it used <code>transaction.atomic()</code> as well. No luck!</p>
<p>Then, after numerous experiments and tons of logging inside Django's own code I was
able to figure out when the failure occurred and what events were in place. The test
code looked like this:</p>
<div class="highlight"><pre><span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&#39;/confirm/&#39;</span><span class="p">)</span>

<span class="n">user</span> <span class="o">=</span> <span class="n">User</span><span class="o">.</span><span class="n">objects</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">username</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">new_user</span><span class="o">.</span><span class="n">username</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">assertTrue</span><span class="p">(</span><span class="n">user</span><span class="o">.</span><span class="n">is_active</span><span class="p">)</span>
</pre></div>


<p><strong>The failure was happening after the view had been rendered upon the
first time I do a SELECT against the database!</strong></p>
<p><strong>The problem was that the connection to the database had been closed
midway during the transaction!</strong></p>
<p>In particular (after more debugging of course) the sequence of events was:</p>
<ol>
<li>execute <code>django/test/client.py::Client::get()</code></li>
<li>execute <code>django/test/client.py::ClientHandler::__call__()</code>, which takes
   care to disconnect/connect <code>signals.request_started</code> and <code>signals.request_finished</code>
   which are responsible for tearing down the DB connection, so problem not here</li>
<li>execute <code>django/core/handlers/base.py::BaseHandler::get_response()</code></li>
<li>execute <code>django/core/handlers/base.py::BaseHandler::_get_response()</code> which goes through
   the middleware (needless to say I did inspect all of it as well since there
   have been some changes in Django 1.10)</li>
<li>execute <code>response = wrapped_callback()</code> while still inside <code>BaseHandler._get_response()</code></li>
<li>
<p>execute <code>django/http/response.py::HttpResponseBase::close()</code> which looks like</p>
<div class="highlight"><pre><span class="c"># These methods partially implement the file-like object interface.</span>
<span class="c"># See https://docs.python.org/3/library/io.html#io.IOBase</span>
 
<span class="c"># The WSGI server must call this method upon completion of the request.</span>
<span class="c"># See http://blog.dscpl.com.au/2012/10/obligations-for-calling-close-on.html</span>
<span class="k">def</span> <span class="nf">close</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">closable</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_closable_objects</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">closable</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
        <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
            <span class="k">pass</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">closed</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="n">signals</span><span class="o">.</span><span class="n">request_finished</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">sender</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_handler_class</span><span class="p">)</span>
</pre></div>


</li>
<li>
<p><code>signals.request_finished</code> is fired</p>
</li>
<li><code>django/db/__init__.py::close_old_connections()</code> closes the connection!</li>
</ol>
<p><strong>IMPORTANT:</strong> On MySQL setting <code>AUTO_COMMIT=False</code> and <code>CONN_MAX_AGE=None</code> helps
workaround this problem but is not the solution for me because it didn't help on
PostgreSQL.</p>
<p>Going back to <code>HttpResponseBase::close()</code> I started wondering who calls this method.
The answer was it was getting called by the <code>@content.setter</code> method at
<code>django/http/response.py::HttpResponse::content()</code> which is even more weird because
we assign to <code>self.content</code> inside <code>HttpResponse::__init__()</code></p>
<h2>Root cause</h2>
<p>The root cause of my problem was precisely this <code>HttpResponse::__init__()</code> method
or rather the way we arrive at it inside the application. </p>
<p>The offending view last line was</p>
<div class="highlight"><pre><span class="k">return</span> <span class="n">HttpResponse</span><span class="p">(</span><span class="n">Prompt</span><span class="o">.</span><span class="n">render</span><span class="p">(</span>
     <span class="n">request</span><span class="o">=</span><span class="n">request</span><span class="p">,</span>
     <span class="n">info_type</span><span class="o">=</span><span class="n">Prompt</span><span class="o">.</span><span class="n">Info</span><span class="p">,</span>
     <span class="n">info</span><span class="o">=</span><span class="n">msg</span><span class="p">,</span>
     <span class="nb">next</span><span class="o">=</span><span class="n">request</span><span class="o">.</span><span class="n">GET</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&#39;next&#39;</span><span class="p">,</span> <span class="n">reverse</span><span class="p">(</span><span class="s">&#39;core-views-index&#39;</span><span class="p">))</span>
<span class="p">))</span>
</pre></div>


<p>and the Prompt class looks like this</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">django.shortcuts</span> <span class="kn">import</span> <span class="n">render</span>

<span class="k">class</span> <span class="nc">Prompt</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">render</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">request</span><span class="p">,</span> <span class="n">info_type</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">info</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="nb">next</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">render</span><span class="p">(</span><span class="n">request</span><span class="p">,</span> <span class="s">&#39;prompt.html&#39;</span><span class="p">,</span> <span class="p">{</span>
            <span class="s">&#39;type&#39;</span><span class="p">:</span> <span class="n">info_type</span><span class="p">,</span>
            <span class="s">&#39;info&#39;</span><span class="p">:</span> <span class="n">info</span><span class="p">,</span>
            <span class="s">&#39;next&#39;</span><span class="p">:</span> <span class="nb">next</span>
        <span class="p">})</span>
</pre></div>


<p>Looking back at the internals of <code>HttpResponse</code> we see that</p>
<ul>
<li>if content is a string we call <code>self.make_bytes()</code></li>
<li>if the content is an iterator then we assign it and if the object has a close method
  then it is executed.</li>
</ul>
<p><code>HttpResponse</code> itself is an iterator, inherits from <code>six.Iterator</code> so when we initialize
<code>HttpResponse</code> with another <code>HttpResponse</code> object (aka the content) we execute <code>content.close()</code>
which unfortunately happens to close the database connection as well.</p>
<p><strong>IMPORTANT:</strong> note that from the point of view of a person using the application the
HTML content is exactly the same regardless of whether we have nested <code>HttpResponse</code> objects
or not.
Also during normal execution the code doesn't run inside a transaction so we never notice
the problem in production.</p>
<p>The fix of course is very simple, just <code>return Prompt.render()</code>!</p>
<p>Thanks for reading and happy testing!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Fri 04 August 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/08/04/transactionmanagementerror-during-testing-with-django-110/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/06/27/producing-coverage-report-for-haskell-binaries/" rel="bookmark" title="Permalink to Producing coverage report for Haskell binaries">
                <h2 class="post-title">
                    Producing coverage report for Haskell binaries
                </h2>
            </a>
                <p>Recently I've started testing a Haskell application and a question I find
unanswered (or at least very poorly documented) is how to produce coverage
reports for binaries ?</p>
<h2>Understanding HPC &amp; cabal</h2>
<p><code>hpc</code> is the Haskell code coverage tool. It produces the following files:</p>
<ul>
<li>.mix - module index file, contains information about <em>tick boxes</em> - their type
  and location in the source code;</li>
<li>.tix - tick index file aka coverage report;</li>
<li>.pix - program index file, used only by <code>hpc trans</code>.</li>
</ul>
<p>The invocation to <code>hpc report</code> needs to know where to find the .mix files in order
to be able to translate the coverage information back to source and it needs to
know the location (full path or relative from pwd) to the tix file we want to
report.</p>
<p><code>cabal</code> is the package management tool for Haskell. Among other thing it can be used
to build your code, execute the test suite and produce the coverage report for you.
<code>cabal build</code> will produce module information in <code>dist/hpc/vanilla/mix</code> and
<code>cabal test</code> will store coverage information in <code>dist/hpc/vanilla/tix</code>!</p>
<p>A particular thing about Haskell is that you can only test code which can be
<code>import</code>ed, e.g. it is a library module. You can't test (via Hspec or Hunit) code which
lives inside a file that produces a binary (e.g. Main.hs). However you can still
execute these binaries (e.g. invoke them from the shell) and they will produce a
coverage report in the current directory (e.g. main.tix).</p>
<h2>Putting everything together</h2>
<ol>
<li>Using <code>cabal build</code> and <code>cabal test</code> build the project and execute your unit tests.
   This will create the necessary .mix files (including ones for binaries) and .tix
   files coming from unit testing;</li>
<li>Invoke your binaries passing appropriate data and examining the results (e.g. compare
   the output to a known value). A simple shell or Python script could do the job;</li>
<li>Copy the <code>binary.tix</code> file under <code>dist/hpc/vanilla/binary/binary.tix</code>!</li>
</ol>
<p>Produce coverage report with hpc:</p>
<div class="highlight"><pre>hpc markup --hpcdir=dist/hpc/vanilla/mix/lib --hpcdir=dist/hpc/vanilla/mix/binary  dist/hpc/vanilla/tix/binary/binary.tix
</pre></div>


<p>Convert the coverage report to JSON and send it to Coveralls.io:</p>
<div class="highlight"><pre>cabal install hpc-coveralls
~/.cabal/bin/hpc-coveralls --display-report tests binary
</pre></div>


<h2>Example</h2>
<p>Check out the <a href="https://github.com/weldr/haskell-rpm/pull/18">haskell-rpm</a> repository
for an example. See <a href="https://coveralls.io/builds/12131112">job #45</a> where there is now
coverage for the <code>inspect.hs</code>, <code>unrpm.hs</code> and <code>rpm2json.hs</code> files, producing binary executables.
Also notice that in
<a href="https://coveralls.io/builds/12131112/source?filename=.%2FRPM%2FParse.hs">RPM/Parse.hs</a>
the function <code>parseRPMC</code> is now covered, while it was not covered in the
<a href="https://coveralls.io/builds/12102486/source?filename=.%2FRPM%2FParse.hs">previous job #42</a>!</p>
<div class="highlight"><span class="filename">.travis.yml snippet</span><pre><span class="l-Scalar-Plain">script</span><span class="p-Indicator">:</span>
  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">~/.cabal/bin/hlint .</span>
  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">cabal install --dependencies-only --enable-tests</span>
  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">cabal configure --enable-tests --enable-coverage --ghc-option=-DTEST</span>
  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">cabal build</span>
  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">cabal test --show-details=always</span>

  <span class="c1"># tests to produce coverage for binaries</span>
  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">wget https://s3.amazonaws.com/atodorov/rpms/macbook/el7/x86_64/efivar-0.14-1.el7.x86_64.rpm</span>
  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">./tests/test_binaries.sh ./efivar-0.14-1.el7.x86_64.rpm</span>

  <span class="c1"># move .tix files in appropriate directories</span>
  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">mkdir ./dist/hpc/vanilla/tix/inspect/ ./dist/hpc/vanilla/tix/unrpm/ ./dist/hpc/vanilla/tix/rpm2json/</span>
  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">mv inspect.tix ./dist/hpc/vanilla/tix/inspect/</span>
  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">mv rpm2json.tix ./dist/hpc/vanilla/tix/rpm2json/</span>
  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">mv unrpm.tix ./dist/hpc/vanilla/tix/unrpm/</span>

<span class="l-Scalar-Plain">after_success</span><span class="p-Indicator">:</span>
  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">cabal install hpc-coveralls</span>
  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">~/.cabal/bin/hpc-coveralls --display-report tests inspect rpm2json unrpm</span>
</pre></div>


<p>Thanks for reading and happy testing!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Tue 27 June 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/06/27/producing-coverage-report-for-haskell-binaries/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/06/26/whats-the-bug-in-this-pseudo-code/" rel="bookmark" title="Permalink to What's the bug in this pseudo-code">
                <h2 class="post-title">
                    What's the bug in this pseudo-code
                </h2>
            </a>
                <p><img alt="Rails Girls Vratsa sticker" src="/images/bug_rails_girls_vratsa.jpg" title="Rails Girls Vratsa sticker" /></p>
<p>This is one of the stickers for the second edition of Rails Girls Vratsa which
was held yesterday. Let's explore some of the bug proposals submitted by the Bulgarian QA group:</p>
<blockquote>
<ol>
<li>sad() == true is ugly</li>
<li>sad() is not very nice, better make it if(isSad())</li>
<li>use sadStop(), and even better - stopSad()</li>
<li>there is an extra space character in beAwesome( )</li>
<li>the last curly bracket needs to be on a new line</li>
</ol>
<p>Lyudmil Latinov</p>
</blockquote>
<p>My friend Lu describes what I would call style issues. The style he refers to
is mostly Java oriented, especially with naming things. In Ruby we would probably
go with <code>sad?</code> instead of <code>isSad</code>. Style is important and there are many tools
to help us with that this will not cause a functional problem! While I'm at it let me say
the curly brackets are not the problem either. They are not valid in Ruby this is
a pseudo-code and they also fall in the style category.</p>
<p>The next interesting proposal comes from Tsveta Krasteva. She examines the possibility
of <code>sad()</code> returning an object or nil instead of boolean value. Her first question was
will the if statement still work, and the answer is yes. In Ruby everything is an object
and every object can be compared to <code>true</code> and <code>false</code>. See
<a href="http://www.skorks.com/2009/09/true-false-and-nil-objects-in-ruby/">Alan Skorkin's</a> blog
post on the subject.</p>
<p>Then Tsveta says the answer is to use <code>sad().stop()</code> with the warning that it may return
nil. In this context the <code>sad()</code> method returns on object indicating that the person
is feeling sad. If the method returns nil then the person is feeling OK.</p>
<div class="highlight"><span class="filename">example by Tsveta</span><pre><span class="k">class</span> <span class="nc">Csad</span>
  <span class="k">def</span> <span class="nf">stop</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;stop</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">);</span>
  <span class="k">end</span>
<span class="k">end</span>

<span class="k">def</span> <span class="nf">sad</span><span class="p">()</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;sad</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">);</span>
  <span class="no">Csad</span><span class="o">.</span><span class="n">new</span><span class="p">();</span>
<span class="k">end</span>

<span class="k">def</span> <span class="nf">beAwesome</span><span class="p">()</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;beAwesome</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">);</span>
<span class="k">end</span>

<span class="c1"># notice == true was removed</span>
<span class="k">if</span><span class="p">(</span><span class="n">sad</span><span class="p">())</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Yes, I am sad</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">);</span>
  <span class="n">sad</span><span class="o">.</span><span class="n">stop</span><span class="p">();</span>
  <span class="n">beAwesome</span><span class="p">(</span> <span class="p">);</span>
<span class="k">end</span>
</pre></div>


<p>While this is coming closer to a functioning solution something about it is bugging me.
In the if statement the developer has typed more characters than required (<code>== true</code>).
This sounds to me unlikely but is possible with less experienced developers.
The other issue is that we are using an object (of <code>class Csad</code>) to represent an internal
state in the system under test. There is one method to return the state (<code>sad()</code>) and
another one to alter the state (<code>Csad.stop()</code>). The two methods don't operate on
the same object! Not a very strong OOP design. On top of that we have to call the
method twice, first time in the if statement, the second time in the body of the
if statement, which may have unwanted side effects. It is best to assign the return
value to some variable instead.</p>
<p>IMO if we are to use this OOP approach the code should look something like:</p>
<div class="highlight"><pre><span class="k">class</span> <span class="n">Person</span>
  <span class="n">def</span> <span class="n">sad</span>?()
  <span class="nb">end</span>

  <span class="n">def</span> <span class="n">stopBeingSad</span>()
  <span class="nb">end</span>

  <span class="n">def</span> <span class="n">beAwesome</span>()
  <span class="nb">end</span>
<span class="nb">end</span>

<span class="n">p</span> = <span class="n">Person</span>.<span class="nb">new</span>
<span class="k">if</span> <span class="n">p</span>.<span class="n">sad</span>?
    <span class="n">p</span>.<span class="n">stopBeingSad</span>
    <span class="n">p</span>.<span class="n">beAwesome</span>
<span class="nb">end</span>
</pre></div>


<p>Let me return back to assuming we don't use classes here.
The first <em>obvious</em> mistake is the space in <code>sad stop();</code> first spotted by Peter Sabev*.
His proposal, backed by others is to use <code>sad.stop()</code>. However they
didn't use my hint asking what is the return value of <code>sad()</code> ?</p>
<p>If <code>sad()</code> returns boolean then we'll get
<code>undefined method 'stop' for true:TrueClass (NoMethodError)</code>!
Same thing if <code>sad()</code> returns nil, although we skip the if block in this case.</p>
<p>In Ruby we are allowed to skip parentheses when calling a method, like I've shown
above. If we ignore this fact for a second, then <code>sad?.stop()</code> will mean execute the
method named <code>stop()</code> which is a member of the <code>sad?</code> variable, which is of type method!
Again, methods don't have an attribute named <code>stop</code>!</p>
<p>The last two paragraphs are the semantic/functional mistake I see in this code. The only way
for it to work is to use an OOP variant which is further away from what the existing
clues give us.</p>
<p><strong>Note:</strong> The variant <code>sad? stop()</code> is syntactically correct. This means call the function <code>sad?</code>
with parameter the result of calling the method <code>stop()</code>, which depending on the outer scope of this program may or may not
be correct (e.g. <code>stop</code> is defined, <code>sad?</code> accepts optional parameters, <code>sad?</code> maintains
global state).</p>
<p>Thanks for reading and happy testing!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Mon 26 June 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/06/26/whats-the-bug-in-this-pseudo-code/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/06/14/vmwares-favorite-login-form/" rel="bookmark" title="Permalink to VMware's favorite login form">
                <h2 class="post-title">
                    VMware's favorite login form
                </h2>
            </a>
                <p><em>How do you test a login form?</em> is one of my favorite questions when
screening candidates for QA positions and also a good brain exercise
even for experienced testers. I've written about it
<a href="http://atodorov.org/blog/2016/04/12/how-to-hire-software-testers-pt-1/">last year</a>. In this
blog post I'd like to share a different perspective on this same question,
this time courtesy of my friend Rayna Stankova.</p>
<p><img alt="Login form" src="/images/login_form_vmware.png" title="Login form" /></p>
<h2>What bugs do you see above</h2>
<p>The series of images above is from a
<a href="https://www.meetup.com/Women-Who-Code-Sofia/events/239480974/">Women Who Code Sofia</a>
workshop where the participants were given printed copies and asked to find
as much defects as possible. Here they are (counting clock-wise from the top-left corner):</p>
<ol>
<li>Typo in "Registr" link at the bottom;</li>
<li>UI components are not aligned;</li>
<li>Missing "Forgot your password?" link</li>
<li>Backend credentials validation with empty password;
   plain text password field; Too specific information about incorrect credentials;</li>
<li>Too specific information about incorrect credentials with visual hint
   as to what exactly is not correct. In this case it looks like the password
   is OK, maybe it was one of
   <a href="https://www.youtube.com/watch?v=0Jx8Eay5fWQ">the 4 most commonly used passwords</a>,
   but the username is wrong which we can easily figure out;</li>
<li>In this case the error handling appears to be correct, not disclosing what
   exactly is wrong. The placement is somewhat wrong, it looks like an error
   message for one of the fields instead for the entire form. I'd move that to the top
   and even slightly update the wording to be more like <em>Login failed, bad credentials,
   try again</em>.</li>
</ol>
<h2>How do you test this</h2>
<p>Here is a list of possible test scenarios, proposed by Rayna. Notes are mine.</p>
<p><strong>UI Layer</strong></p>
<ul>
<li>Test 1: Verify Email (User ID) field has focus on page load</li>
<li>Test 2: Verify Empty Email (User ID) field and Password field</li>
<li>Test 3: Verify Empty Email (User ID) field</li>
<li>Test 4: Verify Empty Password field</li>
<li>Test 5: Verify Correct sign in</li>
<li>Test 6: Verify Incorrect sign in</li>
<li>Test 7: Verify Password Reset - working link</li>
<li>Test 8: Verify Password Reset - invalid emails</li>
<li>Test 9: Verify Password Reset - valid email</li>
<li>Test 10: Verify Password Reset - using new password</li>
<li>Test 11: Verify Password Reset - using old password</li>
<li>Test 12: Verify whether password text is hidden</li>
<li>Test 13: Verify text field limits - whether the browser accepts more than the allowed database limits</li>
<li>Test 14: Verify that validation message is displayed in case user exceeds the character limit of the username and password fields</li>
<li>Test 15: Verify if there is checkbox with label "remember password" in the login page</li>
<li>Test 16: Verify if its allowed the username to contain non printable characters? If not, this is invalid on the 'create user' section.</li>
<li>Test 17: Verify if the user must be logged in to access any other area of the site.</li>
</ul>
<p>Tests 10 and 11 are particularly relevant for Fedora Account System where
you need a really strong password and (at least in the past) had to change it more often
and couldn't reuse any of your old passwords. As a user I really hate this b/c I can't remember
my own password but it makes for a good test scenario.</p>
<p>13 and 14 are also something I rarely see and could make a nice case for
property based testing.</p>
<p>16 would have been the bread and butter of testing Emoj.li (the first emoji-only
social network).</p>
<p><strong>Keyboard Specific</strong></p>
<ul>
<li>Test 18: Verify Navigate to all fields</li>
<li>Test 19: Verify Enter submits on password focus</li>
<li>Test 20: Verify Space submits on login focus</li>
<li>Test 21: Verify Enter submits</li>
</ul>
<p>These are all so relevant with beautifully styled websites nowadays. The one I hate the most
is when space key doesn't trigger select/unselect for checkboxes which are actually
images!</p>
<p><strong>Security:</strong></p>
<ul>
<li>Test 22: Verify SQL Injections testing - password field</li>
<li>Test 23: Verify SQL Injections testing - username field</li>
<li>Test 24: Verify SQL Injections testing - reset password</li>
<li>Test 25: Verify Password/username not visible from URL login</li>
<li>Test 26: Verify For security point of view, in case of incorrect credentials user is displayed the message like
  "incorrect username or password" instead of exact message pointing at the field that is incorrect.
  As message like "incorrect username" will aid hacker in brute-forcing the fields one by one</li>
<li>Test 27: Verify the timeout of the login session</li>
<li>Test 28: Verify if the password can be copy-pasted or not</li>
<li>Test 29: Verify that once logged in, clicking back button doesn't logout user</li>
</ul>
<p>22, 23 and 24 are a bit generic and I guess can be collapsed into one. Better yet make
them more specific instead.</p>
<p>Test 28 may sound like nonsense but is not. I remember back in the days that
it was possible to copy and paste the password out of Windows dial-up credentials screen.
With heavily styled form fields it is possible to have this problem again so it is
a valid check IMO.</p>
<p><strong>Others:</strong></p>
<ul>
<li>Test 30: Verify that the password is in encrypted form when entered</li>
<li>Test 31: Verify the user must be logged in to call any web services.</li>
<li>Test 32: Verify if the username is allowed to contain non printable characters,
  the code handling login can deal with them and no error is thrown.</li>
</ul>
<p>I think Test 30 means to validate that the backend doesn't store passwords in plain text
but rather stores their hashes.</p>
<p>32 is a duplicate of 16. I also say why only the username? Password field is also
a good candidate for this.</p>
<p>If you check how I
<a href="http://atodorov.org/blog/2016/04/12/how-to-hire-software-testers-pt-1/">would test a login form</a> you will find
some similarities but there are also scenarios which are different. I'm interested to
see what other scenarios we've both missed, especially ones which have manifested themselves
as bugs in actual applications.</p>
<p>Thanks for reading and happy testing!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Wed 14 June 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/06/14/vmwares-favorite-login-form/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/06/12/monitoring-behavior-via-automated-tests/" rel="bookmark" title="Permalink to Monitoring behavior via automated tests">
                <h2 class="post-title">
                    Monitoring behavior via automated tests
                </h2>
            </a>
                <p>In my last several presentations I briefly talked about
using your tests as a monitoring tool. I've not been eating my own
dog food and stuff failed in production!</p>
<h2>What is monitoring via testing</h2>
<p>This is a technique I coined 6 months ago while working with Tradeo's team.
I'm not the first one to figure this out so if you know the proper
name for it please let me know in the comments.
So why not take a subset of your automated tests and run them regularly against
production? Let's say every hour?</p>
<p>In my particular case we
started with integration tests which interact with the product (a web app)
in a way that a living person would do. E.g. login, update their settings,
follow another user, chat with another user, try to deposit money, etc.
The results from these tests are logged into a database and then charted
(using Grafana). This way we can bring lots of data points together and easily
analyze them.</p>
<p>This technique has the added bonus that we can cover the most critical
test paths in a couple of minutes and do so regularly without human intervention.
Perusing the existing monitoring infrastructure of the devops team we can configure
alerts if need be. This makes it sort of early detection/warning system plus
it gives a degree of possibility to spot correlations between data points or
patterns.</p>
<p>As simple as it sounds I've heard about a handfull of companies doing this
sort of continuous testing against production. Maybe you can implement something
similar in your organization and we can talk more about the results?</p>
<h2>Why does it matter</h2>
<p>Anyway, everyone knows
<a href="http://atodorov.org/blog/2017/05/27/learn-python-selenium-automation-in-8-weeks/">how to write Selenium tests</a>
so I'm not going to bother you with the details. Why does this kind of
testing matter?</p>
<p>Do you remember a recent announcement by GitHub about Travis CI leaking some
authentication tokens into their public log files? I did receive an email about
this but didn't pay attention to it because I don't use GitHub tokens for
anything I do in Travis. However as a safety measure GitHub had went ahead and
wiped out my security tokens.</p>
<p>The result from this is that my
<a href="http://mrsenko.com/blog/mr-senko/2016/05/18/triggering-automatic-dependency-testing/">automated upstream testing infrastructure</a>
had stopped working! In particular my requests to the GitHub API stopped
working. And I didn't even know about it!</p>
<p>This means that since May 24th there have been at least 4 new
versions of libraries and frameworks on which some of my software depends
and I failed to test them! One of them was <em>Django 1.11.2</em>.</p>
<p>I have supplied a new GitHub token for my infra but if I had monitoring
I would have known about this problem well in advance. Next I'm off to write
some monitoring tests and also implement better failure detection in
<a href="https://github.com/MrSenko/strazar">Strazar</a> itself!</p>
<p>Thanks for reading and happy testing (in production)!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Mon 12 June 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/06/12/monitoring-behavior-via-automated-tests/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/06/10/semantically-invalid-input/" rel="bookmark" title="Permalink to Semantically Invalid Input">
                <h2 class="post-title">
                    Semantically Invalid Input
                </h2>
            </a>
                <div class="highlight"><span class="filename">Unsolvable Square example</span><pre><span class="o">.</span> <span class="o">.</span> <span class="mi">9</span> <span class="o">|</span> <span class="o">.</span> <span class="mi">2</span> <span class="mi">8</span> <span class="o">|</span> <span class="mi">7</span> <span class="o">.</span> <span class="o">.</span>
<span class="mi">8</span> <span class="o">.</span> <span class="mi">6</span> <span class="o">|</span> <span class="o">.</span> <span class="o">.</span> <span class="mi">4</span> <span class="o">|</span> <span class="o">.</span> <span class="o">.</span> <span class="mi">5</span>
<span class="o">.</span> <span class="o">.</span> <span class="mi">3</span> <span class="o">|</span> <span class="o">.</span> <span class="o">.</span> <span class="o">.</span> <span class="o">|</span> <span class="o">.</span> <span class="o">.</span> <span class="mi">4</span>
<span class="o">------+-------+------</span>
<span class="mi">6</span> <span class="o">.</span> <span class="o">.</span> <span class="o">|</span> <span class="o">.</span> <span class="o">.</span> <span class="o">.</span> <span class="o">|</span> <span class="o">.</span> <span class="o">.</span> <span class="o">.</span>
<span class="err">?</span> <span class="mi">2</span> <span class="o">.</span> <span class="o">|</span> <span class="mi">7</span> <span class="mi">1</span> <span class="mi">3</span> <span class="o">|</span> <span class="mi">4</span> <span class="mi">5</span> <span class="o">.</span>
<span class="o">.</span> <span class="o">.</span> <span class="o">.</span> <span class="o">|</span> <span class="o">.</span> <span class="o">.</span> <span class="o">.</span> <span class="o">|</span> <span class="o">.</span> <span class="o">.</span> <span class="mi">2</span>
<span class="o">------+-------+------</span>
<span class="mi">3</span> <span class="o">.</span> <span class="o">.</span> <span class="o">|</span> <span class="o">.</span> <span class="o">.</span> <span class="o">.</span> <span class="o">|</span> <span class="mi">5</span> <span class="o">.</span> <span class="o">.</span>
<span class="mi">9</span> <span class="o">.</span> <span class="o">.</span> <span class="o">|</span> <span class="mi">4</span> <span class="o">.</span> <span class="o">.</span> <span class="o">|</span> <span class="mi">8</span> <span class="o">.</span> <span class="mi">7</span>
<span class="o">.</span> <span class="o">.</span> <span class="mi">1</span> <span class="o">|</span> <span class="mi">2</span> <span class="mi">5</span> <span class="o">.</span> <span class="o">|</span> <span class="mi">3</span> <span class="o">.</span> <span class="o">.</span>
</pre></div>


<p>In a comment to a
<a href="http://atodorov.org/blog/2016/04/16/how-to-hire-software-testers-pt-2/">previous post</a>
<em>Flavio Poletti</em> proposed a very interesting test case for a function which solves
the Sudoku game - <em>semantically invalid input, i.e. an input that passes intermediate
validation checks (no duplicates in any row/col/9-square) but that cannot possibly
have a solution</em>.</p>
<p>Until then I thought that Sudoku was a completely deterministic game and if input
followed all validation checks then we always have a solution. Apparently I was wrong!
Reading more on the topic I discovered these
<a href="http://sudopedia.enjoysudoku.com/Test_Cases.html">Sudoku test cases from Sudopedia</a>.
Their <a href="http://sudopedia.enjoysudoku.com/Invalid_Test_Cases.html">Invalid Test Cases</a>
section lists several examples of semantically invalid input in Sudoku:</p>
<ul>
<li>Unsolvable Square;</li>
<li>Unsolvable Box;</li>
<li>Unsolvable Column;</li>
<li>Unsolvable Row;</li>
<li>Not Unique with examples having 2, 3, 4, 10 and 125 solutions</li>
</ul>
<p>The example above cannot be solved because the left-most
square of the middle row (r5c1) has no possible candidates.</p>
<p>Following the rule <em>non-repeating numbers from 1 to 9 in each row</em> for row 5 we're
left with numbers: 6, 8 and 9. For (r5c1) 6 is a no-go because it is already present
in the same square. Then 9 is a no-go because it is present in column 1. Which leaves
us with 8, which is also present in column 1! Pretty awesome, isn't it?</p>
<p>Also check the
<a href="http://sudopedia.enjoysudoku.com/Valid_Test_Cases.html">Valid Test Cases</a> section
which includes other interesting examples and definitely not ones which I have considered
previously when <a href="http://atodorov.org/blog/2016/04/16/how-to-hire-software-testers-pt-2/">testing Sudoku</a>.</p>
<p>On a more practical note I have been trying to remember a case from my QA practice
where we had input data that matched all conditions but is semantically invalid. I
can't remember of such a case. If you do have examples about semantically invalid
data in real software please let me know in the comments below!</p>
<p>Thanks for reading and happy testing!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Sat 10 June 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/06/10/semantically-invalid-input/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/05/27/learn-python-selenium-automation-in-8-weeks/" rel="bookmark" title="Permalink to Learn Python & Selenium Automation in 8 weeks">
                <h2 class="post-title">
                    Learn Python &amp; Selenium Automation in 8 weeks
                </h2>
            </a>
                <p>Couple of months ago I conducted a practical,
instructor lead training in Python and Selenium automation for
manual testers. You can find the materials at
<a href="https://github.com/atodorov/qa-automation-python-selenium-101">GitHub</a>.</p>
<p>The training consists of several basic modules and practical homework
assignments. The modules explain</p>
<ol>
<li>The basic structure of a Python program and functions</li>
<li>Commonly used data types</li>
<li>If statements and (for) loops</li>
<li>Classes and objects</li>
<li>The Python unit testing framework and its assertions</li>
<li>High-level introduction to Selenium with Python</li>
<li>High-level introduction to the Page Objects design pattern</li>
<li>Writing automated tests for real world scenarios
   without any help from the instructor.</li>
</ol>
<p>Every module is intended to be taken in the course of 1 week and begins with
links to preparatory materials and lots of reading. Then I help the students
understand the basics and explain with more examples, often writing code as
we go along. At the end there is the homework assignment for which I expect
a solution presented by the end of the week so I can comment and code-review it.</p>
<p>All assignments which require the student to implement functionality, not tests,
are paired with a test suite, which the student should use to validate their
solution.</p>
<h2>What worked well</h2>
<p>Despite everything I've written below I had 2 students (from a group of 8)
which showed very good progress. One of them was the absolute star, taking
active participation in every class and doing almost all homework assignments
on time, pretty much without errors. I think she'd had some previous training
or experience though.
She was in the USA, training was done remotely via Google Hangouts.</p>
<p>The other student was in Sofia, training was done in person. He is not on the
same level as the US student but is the best from the Bulgarian team. IMO he
lacks a little bit of motivation. He "cheated" a bit on some tasks providing
non-standard, easier solutions and made most of his assignments. After the first
Selenium session he started creating small scripts to extract results from
football sites or as helpers to be applied in the daily job.
The interesting
fact for me was that he created his programs as <code>unittest.TestCase</code> classes.
I guess because this was the way he knew how to run them!?!</p>
<p>There were another few students which had had some prior experience with
programming but weren't very active in class so I can't tell how their
careers will progress. If they put some more effort into it I'm sure they
can turn out to have decent programming skills.</p>
<h2>What didn't work well</h2>
<p>Starting from the beginning most students failed to read the preparatory
materials. Some of the students did read a little bit, others didn't read at all.
At the times when they came prepared I had the feeling the sessions progressed
more smoothly. I also had students joining late in the process, which for the
most part didn't participate at all in the training. I'd like to avoid that in
the future if possible.</p>
<p>Sometimes students complained about lack of example code, although
<em>Dive into Python</em> includes tons of examples. I've resorted to sending them
the example.py files which I produced during class.</p>
<p>The practical part of the training was mostly myself programming on a big
TV screen in front of everyone else. Several times someone from the students
took my place. There wasn't much active participation on their part and
unfortunately they didn't want to bring personal laptops to the training
(or maybe weren't allowed)! We did have a company provided laptop though.</p>
<p>When practicing functions and arithmetic operations the students struggled
with basic maths like breaking down a number into its digits or vice versa,
working with Fibonacci sequences and the like. In some cases they cheated
by converting to/from strings and then iterating over them. Also some
hard-coded the first few numbers of the Fibonacci sequence and returned
it directly. Maybe an in-place
explanation of the underlying maths would have been helpful but honestly
I was surprised by this. Somebody please explain or give me an advise here!</p>
<p>I am completely missing examples of the <code>datetime</code> and <code>timedelta</code> classes
which tuned out to be very handy in the practical Selenium tasks and we had
to go over them on the fly.</p>
<p>The OOP assignments went mostly undone, not to mention one of them had
bonus tasks which are easily solved using recursion. I think we could
skip some of the OOP practice (not sure how safe that is) because I really
need classes only for constructing the tests and we don't do anything fancy
there.</p>
<p>Page Object design pattern is also OOP based and I think that went somewhat
well granted that we are only passing values around and performing some actions.
I didn't put constraints nor provided guidance on what the classes should
look like and which methods go where. Maybe I should have made it easier.</p>
<p>Anyway, given that Page Objects is being replaced by Screenplay pattern,
I think we can safely stick to the all-in-one functional based Selenium
tests. Maybe utilize helper functions for repeated tasks (like login).
Indeed this is what I was using last year with Rspec &amp; Capybara!</p>
<h2>What students didn't understand</h2>
<p>Right until the end I had people who had troubles understanding function
signatures, function instances and calling/executing a function. Also
returning a value from a function vs. printing the (same) value on screen
or assigning to the same global variable (e.g. FIB_NUMBERS).</p>
<p>In the same category falls using method parameters vs. using global variables
(which happened to have the same value), using the parameters as arguments to
another function inside the body of the current function, using class attributes
(e.g. <code>self.name</code>) to store and pass values around vs. local variables in methods
vs. method parameters which have the same names.</p>
<p>I think there was some confusion about lists, dictionaries and tuples but
we did practice mostly with list structures so I don't have enough information.</p>
<p>I have the impression that object oriented programming (classes and instances,
we didn't go into inheritance) are generally confusing to beginners with zero
programming experience. The classical way to explain them is by using some
abstractions like animal -&gt; dog -&gt; a particular dog breed -&gt; a particular pet.
OOP was explained to me in a similar way back in school so these kinds of
abstractions are very natural for me. I have no idea if my explanation sucks or students are having hard time
wrapping their heads around the abstraction. I'd love to hear some feedback
from other instructors on this one.</p>
<p>I think there is some misunderstanding between a class (a definition of behavior)
and an instance/object of this class (something which exists into memory). This
may also explain the difficulty remembering or figuring out what <code>self</code> points to
and why do we need to use it inside method bodies.</p>
<p>For <code>unittest.TestCase</code> we didn't do lots of practice which is my fault.
The homework assignments request the students to go back to solutions
of previous modules and implement more tests for them. Next time I should
provide a module (possibly with non-obvious bugs) and request to write
a comprehensive test suite for it.</p>
<p>Because of the missing practice there was some confusion/misunderstanding
about the <code>setUpClass/tearDownClass</code> and the <code>setUp/tearDown</code> methods.
Also add to the mix that the first are <code>@classmethod</code> while the later
are not. "To be safe" students always defined both as class methods!</p>
<p>I have since corrected the training materials but we didn't have
good examples (nor practiced) explaining the difference between
<code>setUpClass</code> (executed once aka before suite) and <code>setUp</code>
(possibly executed multiple times aka before test method).</p>
<p>On the Selenium side I think it is mostly practice which students lack,
not understanding. The entire Selenium framework (any web test framework
for that matter) boils down to</p>
<ul>
<li>Load a page</li>
<li>Find element(s)</li>
<li>Click or hover (that one was tricky) element</li>
<li>Get element's attribute value or text</li>
<li>Wait for the proper page to load (or worst case AJAX calls)</li>
</ul>
<p>IMO finding the correct element on the page is on-par with waiting
(which also relies on locating elements) and took 80% of the time we spent
working with Selenium.</p>
<p>Thanks for reading and don't forget to comment and give me your feedback!</p>
<p><em>Image source: https://www.udemy.com/selenium-webdriver-with-python/</em></p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Sat 27 May 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/05/27/learn-python-selenium-automation-in-8-weeks/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/04/20/quality-assurance-according-2-einstein/" rel="bookmark" title="Permalink to Quality Assurance According 2 Einstein">
                <h2 class="post-title">
                    Quality Assurance According 2 Einstein
                </h2>
            </a>
                <p><img alt="logo" src="/images/qa_einstein.png" title="logo" /></p>
<p><a href="https://www.facebook.com/events/1887550261534408/">Quality Assurance According 2 Einstein</a>
is a talk which introduces several different ideas about how we need to think
and approach software testing. It touches on subjects like mutation testing,
pairwise testing, automatic test execution, smart test non-execution, using tests
as monitoring tools and team/process organization.</p>
<p>Because testing is more thinking than writing I have chosen a different format
for this presentation. It contains only slides with famous quotes from one
of the greatest thinkers of our time - Albert Einstein!</p>
<p>This blog post includes the accompanying links and references only! It is my first
iteration on the topic so expect it to be unclear and incomplete, use your imagination!
I will continue 
working and presenting on the same topic in the next few months
so you can expect updates from time to time. In the mean time I am happy to discuss
with you down in the comments.</p>
<blockquote>
<p>IMAGINATION
IS MORE
IMPORTANT
THAN KNOWLEDGE.</p>
</blockquote>
<ul>
<li><a href="http://atodorov.org/blog/2016/03/25/hello-world-qa-challenge/">Hello World bug challenge</a></li>
<li><a href="http://atodorov.org/blog/2016/04/16/how-to-hire-software-testers-pt-2/#disqus_thread">Testing a Sudoku</a></li>
<li><a href="https://github.com/weldr/welder-web/pull/56">https://github.com/weldr/welder-web/pull/56</a></li>
<li><a href="https://github.com/weldr/welder-web/pull/59">https://github.com/weldr/welder-web/pull/59</a></li>
</ul>
<blockquote>
<p>THE FASTER YOU GO,
THE SHORTER
YOU ARE.</p>
</blockquote>
<ul>
<li><a href="http://bit.ly/GTAC2016Unity3D">Using Statistics to Predict Which Tests to Run</a></li>
<li><a href="http://bit.ly/ISTA2016ExMachina">The framework that knows its bugs</a></li>
<li><a href="http://atodorov.org/blog/2017/04/14/testing-red-hat-enterprise-linux-the-microsoft-way/">Testing Red Hat Enterprise Linux the Microsoft way</a></li>
<li><a href="http://mrsenko.com/blog/mr-senko/2016/05/18/triggering-automatic-dependency-testing/">Automatic dependency testing with Strazar</a></li>
<li><a href="http://atodorov.org/blog/2017/04/15/automatic-cargo-update-pull-requests-for-rust-projects/">Automatic cargo update, test and pull request</a></li>
</ul>
<blockquote>
<p>IF THE FACTS
DON'T FIT
THE THEORY,
CHANGE THE FACTS.</p>
</blockquote>
<ul>
<li><a href="https://www.youtube.com/watch?v=sAfROROGujU&amp;list=PLFjlI7p-h1hxBP3cIjEqePSeoBDHud5Db&amp;index=47">Coverage is Not Strongly Correlated with Test Suite Effectiveness</a></li>
<li><a href="https://www.youtube.com/watch?v=NKEptA3KP08&amp;list=PLFjlI7p-h1hxBP3cIjEqePSeoBDHud5Db&amp;index=1">Code Coverage is a Strong Predictor of Test Suite Effectiveness</a></li>
<li><a href="http://atodorov.org/blog/2016/12/27/mutation-testing-vs-coverage/">Mutation testing vs. coverage, Pt. 1</a></li>
<li><a href="http://atodorov.org/blog/2017/04/05/mutation-testing-vs-coverage-pt-2/">Mutation testing vs. coverage, Pt. 2</a></li>
<li>There are 101 coverage metrics according to <a href="http://www.badsoftware.com/coverage.htm">Cem Kaner</a>.
  Which ones are you measuring and what conclusions are you making out of these metrics?</li>
</ul>
<blockquote>
<p>THE WHOLE OF
SCIENCE
IS NOTHING MORE
THAN A REFINEMENT
OF EVERYDAY
THINKING.</p>
</blockquote>
<ul>
<li><a href="https://github.com/HackBulgaria/QA-and-Automation-101/tree/master/lesson12">How to find 1000 bugs in 30 minutes</a></li>
<li><a href="https://www.youtube.com/watch?v=m5NfgXP76Vw&amp;index=1&amp;list=PLFjlI7p-h1hxBP3cIjEqePSeoBDHud5Db&amp;t=103s">How we found a million style and grammar errors in the English Wikipedia</a></li>
<li><a href="https://www.youtube.com/watch?v=56oNQf5oITw&amp;list=PLFjlI7p-h1hxBP3cIjEqePSeoBDHud5Db&amp;t=1300s&amp;index=47">Simple Testing Can Prevent Most Critical Failures</a></li>
<li><a href="https://www.youtube.com/watch?v=nCGBgI1MNwE&amp;list=PLFjlI7p-h1hxBP3cIjEqePSeoBDHud5Db&amp;index=60">Need it robust, make it fragile</a><ul>
<li>btw its me who asks the first question at the end :)</li>
</ul>
</li>
</ul>
<blockquote>
<p>INSANITY -
DOING THE SAME THING
OVER AND OVER
AND EXPECTING
DIFFERENT RESULTS.</p>
</blockquote>
<ul>
<li><a href="http://atodorov.org/blog/2016/12/28/4-quick-wins-to-manage-the-cost-of-software-testing/">4 Quick Wins to Manage the Cost of Software Testing</a></li>
</ul>
<p>This principle can be applied to any team/process within the organization.
The above link is reference to a nice book which was recommended to me but the
gist of it is that we always need to analyze, ask questions and change is we want
to achieve great results. A practicle example of what is possible if you follow
this principle is this talk
<a href="https://www.youtube.com/watch?v=khSsjjg2eSQ&amp;index=1&amp;list=PLFjlI7p-h1hxBP3cIjEqePSeoBDHud5Db">Accelerate Automation Tests From 3 Hours to 3 Minutes</a>.</p>
<blockquote>
<p>THE ONLY
REASON FOR TIME
IS SO THAT
EVERYTHING DOESN'T
HAPPEN AT ONCE.</p>
</blockquote>
<p>The topic here is "using tests as a monitoring tool".
This is something I started a while back ago, helping a prominent startup with their
production testing but my involvement ended very soon after the framework was
deployed live so I don't have lots of insight.</p>
<p>As the first few days this technique identified some unexpected behaviors,
for example a 3rd party service was updating very often. Once even they were
broken for a few hours - something nobody had information about.</p>
<p>Since then I've heard about 2 more companies using similar techniques to continuously
validate that production software continues to work without having a physical
person to verify it. In the event of failures there are alerts which are
delath with accordingly.</p>
<blockquote>
<p>NO PROBLEM
CAN BE SOLVED FROM
THE SAME LEVEL
OF CONSIOUSNESS
THAT CREATED IT.</p>
</blockquote>
<p>That much must be obvious to us quality engineers. What about the future however?</p>
<ul>
<li><a href="https://www.quora.com/How-do-I-implement-AI-in-test-automation">How do I implement AI in test automation</a></li>
<li><a href="http://sourced.tech/">source{d} - Building the first AI that understands code</a></li>
</ul>
<p>I don't have anything more concrete here. Just looking towards what is coming next!</p>
<blockquote>
<p>DO NOT WORRY ABOUT
YOUR DIFFICULTIES
IN MATHEMATICS.
I CAN ASSURE
YOU MINE ARE STILL
GREATER.</p>
</blockquote>
<p>Thanks for reading and happy testing!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Thu 20 April 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/04/20/quality-assurance-according-2-einstein/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/04/15/automatic-cargo-update-pull-requests-for-rust-projects/" rel="bookmark" title="Permalink to Automatic cargo update & pull requests for Rust projects">
                <h2 class="post-title">
                    Automatic cargo update & pull requests for Rust projects
                </h2>
            </a>
                <p>If you follow my blog you are aware that I use automated tools to do some
boring tasks instead of me. For example they can detect when new versions of
dependencies I'm using are available and then schedule testing against them on the fly.</p>
<p>One of these tools is
<a href="http://mrsenko.com/blog/mr-senko/2016/05/18/triggering-automatic-dependency-testing/">Strazar</a>
which I use heavily for my Django based packages.
Example: <a href="https://travis-ci.org/atodorov/django-s3-cache/builds/218758538">django-s3-cache build job</a>.</p>
<p>Recently I've made a slightly different proof-of-concept for a Rust project.
Because rustc and various dependencies (called crates) are updated very often
we didn't want to expand the test matrix like Strazar does. Instead we wanted to
always build &amp; test against the latest crates versions and if that passes
create a pull request for the update (in <code>Cargo.lock</code>). All of this unattended
of course!</p>
<p>To start create a cron job in Travis CI which will execute once per day and call your
test script. The script looks like this:</p>
<div class="highlight"><span class="filename">cargo-update-and-pr</span><pre><span class="c">#!/bin/bash</span>

<span class="k">if</span> <span class="o">[</span> -z <span class="s2">&quot;</span><span class="nv">$GITHUB_TOKEN</span><span class="s2">&quot;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
    <span class="nb">echo</span> <span class="s2">&quot;GITHUB_TOKEN is not defined&quot;</span>
    <span class="nb">exit </span>1
<span class="k">fi</span>

<span class="nv">BRANCH_NAME</span><span class="o">=</span><span class="s2">&quot;automated_cargo_update&quot;</span>

git checkout -b <span class="nv">$BRANCH_NAME</span>
cargo update <span class="o">&amp;&amp;</span> cargo <span class="nb">test</span>

<span class="nv">DIFF</span><span class="o">=</span><span class="sb">`</span>git diff<span class="sb">`</span>
<span class="c"># NOTE: we don&#39;t really check the result from testing here. Only that</span>
<span class="c"># something has been changed, e.g. Cargo.lock</span>
<span class="k">if</span> <span class="o">[</span> -n <span class="s2">&quot;</span><span class="nv">$DIFF</span><span class="s2">&quot;</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
    <span class="c"># configure git authorship</span>
    git config --global user.email <span class="s2">&quot;atodorov@MrSenko.com&quot;</span>
    git config --global user.name <span class="s2">&quot;Alexander Todorov&quot;</span>

    <span class="c"># add a remote with read/write permissions!</span>
    <span class="c"># use token authentication instead of password</span>
    git remote add authenticated https://atodorov:<span class="nv">$GITHUB_TOKEN</span>@github.com/atodorov/bdcs-api-rs.git

    <span class="c"># commit the changes to Cargo.lock</span>
    git commit -a -m <span class="s2">&quot;Auto-update cargo crates&quot;</span>

    <span class="c"># push the changes so that PR API has something to compare against</span>
    git push authenticated <span class="nv">$BRANCH_NAME</span>

    <span class="c"># finally create the PR</span>
    curl -X POST -H <span class="s2">&quot;Content-Type: application/json&quot;</span> -H <span class="s2">&quot;Authorization: token </span><span class="nv">$GITHUB_TOKEN</span><span class="s2">&quot;</span> <span class="se">\</span>
         --data <span class="s1">&#39;{&quot;title&quot;:&quot;Auto-update cargo crates&quot;,&quot;head&quot;:&quot;automated_cargo_update&quot;,&quot;base&quot;:&quot;master&quot;, &quot;body&quot;:&quot;@atodorov review&quot;}&#39;</span> <span class="se">\</span>
         https://api.github.com/repos/atodorov/bdcs-api-rs/pulls
<span class="k">fi</span>
</pre></div>


<p>A few notes here:</p>
<ul>
<li>You need to define a secret <code>GITHUB_TOKEN</code> variable for authentication;</li>
<li>The script doesn't force push, but in practice that may be useful (e.g. updating the PR);</li>
<li>The script doesn't have any error handling;</li>
<li>If PR is still open GitHub will tell us about it but we ignore the result here;</li>
<li><strong>DON'T</strong> paste this into your <code>Makefile</code> because the <code>GITHUB_TOKEN</code> variable will be
  expanded into the logs and your secrets go away! Always call the script from your
  <code>Makefile</code> to avoid revealing secrets.</li>
<li>I am using topic branches because this is a POC. Switch to <em>master</em> and maybe move
  all URLs as variables at the top of the script!</li>
<li>I run this cron build against a fork of the project because the team doesn't feel
  comfortable having automated commits/pushes. I also create the pull requests against
  my own fork. You will have to adjust the targets if you want your PR to go to the
  original repository.</li>
</ul>
<p>Here is the PR which was created by this script:
<a href="https://github.com/atodorov/bdcs-api-rs/pull/5">https://github.com/atodorov/bdcs-api-rs/pull/5</a></p>
<p>Notice that it includes previous commits b/c they have not been merged to the master branch!</p>
<p>Here's the test job (#77) which generated this PR:
<a href="https://travis-ci.org/atodorov/bdcs-api-rs/builds/219274916">https://travis-ci.org/atodorov/bdcs-api-rs/builds/219274916</a></p>
<p>Here's a test job (#87) which bails out miserably because the PR already exists:
<a href="https://travis-ci.org/atodorov/bdcs-api-rs/builds/220954269">https://travis-ci.org/atodorov/bdcs-api-rs/builds/220954269</a></p>
<p>This post is part of my <em>Quality Assurance According to Einstein</em> series - a detailed description
of useful techniques I will be presenting very soon.</p>
<p>Thanks for reading and happy testing!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Sat 15 April 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/04/15/automatic-cargo-update-pull-requests-for-rust-projects/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/04/14/testing-red-hat-enterprise-linux-the-microsoft-way/" rel="bookmark" title="Permalink to Testing Red Hat Enterprise Linux the Microsoft way">
                <h2 class="post-title">
                    Testing Red Hat Enterprise Linux the Microsoft way
                </h2>
            </a>
                <p><img alt="Microsoft and Red Hat logos" src="/images/microsoft-redhat-puzzle.jpg" title="Microsoft and Red Hat logos" /></p>
<p>Pairwise (a.k.a. all-pairs) testing is an effective test case generation
technique that is based on the observation that most faults are caused by
interactions of at most two factors! Pairwise-generated test suites cover
all combinations of two therefore are much smaller than exhaustive ones yet
still very effective in finding defects. This technique has been pioneered by
Microsoft in testing their products. For an example please see
<a href="https://github.com//microsoft/pict">their GitHub repo</a>!</p>
<p>I heard about pairwise testing by
<a href="https://www.linkedin.com/in/niels-sander-christensen-a2639b1/">Niels Sander Christensen</a>
last year at
<a href="http://qachallengeaccepted.com/2016.html#agenda2016">QA Challenge Accepted 2.0</a> and
I immediately knew where it would fit into my test matrix.</p>
<p>This article describes an experiment made during Red Hat Enterprise Linux 6.9
installation testing campaign. The experiment covers generating a
test plan (referred to Pairwise Test Plan) based on the pairwise test strategy and
some heuristics. The goal was to reduce the number of test cases which needed to be
executed and still maintain good test coverage (in terms of breadth of testing) and
also maintain low risk for the product.</p>
<h1>Product background</h1>
<p>For RHEL 6.9 there are 9 different product variants each comprising of particular
package set and CPU architecture:</p>
<ul>
<li>Server i386</li>
<li>Server x86_64</li>
<li>Server ppc64 (IBM Power)</li>
<li>Server s390x (IBM mainframe)</li>
<li>Workstation i386</li>
<li>Workstation x86_64</li>
<li>Client i386</li>
<li>Client x86_64</li>
<li>ComputeNode x86_64</li>
</ul>
<p>Traditional testing activities are classified as Tier #1, Tier #2 and Tier #3</p>
<ul>
<li>Tier #1 - basic form of installation testing. Executed for all arch/variants on all builds,
  including nightly builds. This group includes the most common installation methods and configurations.
  If Tier #1 fails the product is considered unfit for customers and further testing blocking the release!</li>
<li>Tier #2 and #3 - includes additional installation configurations and/or functionality which is deemed important.
  These are still fairly common scenarios but not the most frequently used ones.
  If some of the Tier#2 and #3 test cases fail they will not block the release.</li>
</ul>
<p>This experiment focuses only on Tier #2 and #3 test cases because they generate the
largest test matrix! This experiment is related only to installation testing of RHEL.
This broadly means <em>"Can the customer install RHEL via the Anaconda installer and boot
into the installed system"</em>.
I do not test functionality of the system after reboot!</p>
<p><strong>I have theorized that from the point of view of installation testing RHEL is mostly a
platform independent product!</strong></p>
<p>Individual product variants rarely exhibit differences in their functional behavior
because they are compiled from the same code base! If a feature is present it should work
the same on all variants. The main differences between variants are:</p>
<ul>
<li>What software has been packaged as part of the variant (e.g. base package set and add-on repos);</li>
<li>Whether or not a particular feature is officially supported, e.g. iBFT on Client variants.
  Support is usually provided via including the respective packages in the variant package set
  and declaring SLA for it.</li>
</ul>
<p>These differences may lead to problems with resolving dependencies and missing packages
but historically haven't shown significant tendency to cause functional failures
e.g. using NFS as installation source working on Server but not on Client.</p>
<p>The main component being tested, Anaconda - the installer, is also mostly platform independent.
In a previous experiment I had collected code coverage data from Anaconda while
performing installation with the same kickstart (or same manual options) on various architectures.
The coverage report supports the claim that Anaconda is platform independent!
See
<a href="http://atodorov.org/blog/2015/10/27/anaconda-coverage.py-coverage-diff/">Anaconda &amp; coverage.py - Pt.3 - coverage-diff, section Kickstart vs. Kickstart</a>!</p>
<h1>Testing approach</h1>
<p>The traditional pairwise approach focuses on features whose functionality is
controlled via parameters. For example: RAID level, encryption cipher, etc.
<strong>I have taken this definition one level up and applied it to the entire product!
Now functionality is also controlled by variant and CPU architecture!</strong>
This allows me to reduce the number of total test cases in the test matrix but still
execute all of them at least once!</p>
<p>The initial implementation used a simple script, built with
<a href="https://github.com/josephwilk/pairwise">the Ruby pairwise gem</a>, that:</p>
<ul>
<li>
<p>Copies verbatim all test cases which are applicable for a single product variant,
    for example s390x Server or ppc64 Server! There's nothing we can do to reduce these
    from combinatorial point of view!</p>
</li>
<li>
<p>Then we have the group of test cases with input parameters. For example:</p>
<div class="highlight"><pre>storage / iBFT / No authentication / Network init script
storage / iBFT / CHAP authentication / Network Manager
storage / iBFT / Reverse CHAP authentication / Network Manager
</pre></div>


<p>In this example the test is <code>storage / iBFT</code> and the parameters are</p>
<ul>
<li>Authentication type<ul>
<li>None</li>
<li>CHAP</li>
<li>Reverse CHAP</li>
</ul>
</li>
<li>Network management type<ul>
<li>SysV init</li>
<li>NetworkManager</li>
</ul>
</li>
</ul>
<p>For test cases in this group I also consider the CPU architecture and OS variant
as part of the input parameters and combine them using pairwise. Usually this results
in around 50% reduction of test efforts compared to testing against all product variants!</p>
</li>
<li>
<p>Last we have the group of test cases which don't depend on any input parameters,
    for example <code>partitioning / swap on LVM</code>. They are grouped together (wrt their applicable variants)
    and each test case is executed only once against a randomly chosen product variant!
    This is my own heuristic based on the fact that the product is platform
    independent!</p>
<p><strong>NOTE:</strong> You may think that for these test cases the product variant is their input parameter.
If we consider this to be the case then we'll not get any reduction because of
how pairwise generation works (the 2 parameters with the largest number of possible values determine
the maximum size of the test matrix). In this case the 9 product variants is the largest set of values!</p>
</li>
</ul>
<p>For this experiment
<a href="https://gist.github.com/atodorov/8f44022e209f749c7121f91281aa641e">pairwise_spec.rb</a>
only produced the list of test scenarios (test cases) to be executed! It doesn't
schedule test execution and it doesn't update the
<a href="http://atodorov.org/blog/2017/04/04/article-about-nitrate-in-methods-tools/">test case management system</a>
with actual results. It just tells you what to do! Obviously this script
will need to integrate with other systems and processes as defined by the organization!</p>
<p>Example results:</p>
<div class="highlight"><pre>RHEL 6.9 Tier #2 and #3 testing
  Test case w/o parameters can&#39;t be reduced via pairwise
    x86_64 Server - partitioning / swap on LVM
    x86_64 Workstation - partitioning / swap on LVM
    x86_64 Client - partitioning / swap on LVM
    x86_64 ComputeNode - partitioning / swap on LVM
    i386 Server - partitioning / swap on LVM
    i386 Workstation - partitioning / swap on LVM
    i386 Client - partitioning / swap on LVM
    ppc64 Server - partitioning / swap on LVM
    s390x Server - partitioning / swap on LVM
  Test case(s) with parameters can be reduced by pairwise
    x86_64 Server - rescue mode / LVM / plain
    x86_64 ComputeNode - rescue mode / RAID / encrypted
    x86_64 Client - rescue mode / RAID / plain
    x86_64 Workstation - rescue mode / LVM / encrypted
    x86_64 Server - rescue mode / RAID / encrypted
    x86_64 Workstation - rescue mode / RAID / plain
    x86_64 Client - rescue mode / LVM / encrypted
    x86_64 ComputeNode - rescue mode / LVM / plain
    i386 Server - rescue mode / LVM / plain
    i386 Client - rescue mode / RAID / encrypted
    i386 Workstation - rescue mode / RAID / plain
    i386 Workstation - rescue mode / LVM / encrypted
    i386 Server - rescue mode / RAID / encrypted
    i386 Workstation - rescue mode / RAID / encrypted
    i386 Client - rescue mode / LVM / plain
    ppc64 Server - rescue mode / LVM / plain
    s390x Server - rescue mode / RAID / encrypted
    s390x Server - rescue mode / RAID / plain
    s390x Server - rescue mode / LVM / encrypted
    ppc64 Server - rescue mode / RAID / encrypted

Finished in 0.00602 seconds (files took 0.10734 seconds to load)
29 examples, 0 failures
</pre></div>


<p>In this example there are 9 (variants) * 2 (partitioning type) * 2 (encryption type) == 32
total combinations! As you can see pairwise reduced them to 20! Also notice that
if you don't take CPU arch and variant into account you are left with
2 (partitioning type) * 2 (encryption type) == 4 combinations for each product variant
and they can't be reduced on their own!</p>
<h1>Acceptance criteria</h1>
<p>I did evaluate all bugs which were found by executing the test cases from the
pairwise test plan and compared them to the list of all bugs found by the team.
This will tell me how good my pairwise test plan was compared to the regular one.
"good" meaning:</p>
<ul>
<li>How many bugs would I find if I don't execute the full test matrix</li>
<li>How many critical bugs would I miss if I don't execute the full test matrix</li>
</ul>
<p>Results:</p>
<ul>
<li>Pairwise found <strong>14 new bugs</strong>;</li>
<li><strong>23 bugs were first found by regular test plan</strong><ul>
<li>some by test cases not included in this experiment;</li>
<li>pairwise totally <strong>missed 4 bugs</strong>!</li>
</ul>
</li>
</ul>
<p>Pairwise test plan <strong>missed 3 critical regressions</strong> due to:</p>
<ul>
<li>Poor planning of pairwise test activity. There was a regression in
  one of the latest builds and that particular test was simply not executed!</li>
<li>Human factor aka me not being careful enough and not following the process diligently.
  I waived a test due to infrastructure issues while there was a bug which stayed undiscovered!
  I should have tried harder to retest this scenario after fixing my infrastructure!</li>
<li>Architecture and networking specific regression which wasn't tested on multiple levels and
  is very narrow corner case.
  Can be mitigated with more testing upstream, more automation and better understanding of the hidden test
  requirements (e.g. IPv4 vs IPv6) for all of which pairwise can help (analysis and more available resources).</li>
</ul>
<p>All of the missed regressions could have been missed by regular test plan as well, however the risk of missing
them in pairwise is higher b/c of the reduced test matrix and the fact that
you may not execute exactly the same test scenario for quite a long time.
OTOH the risk can be mitigated with more automation b/c we now have more free resources.</p>
<p>IMO pairwise test plan did a good job and didn't introduce "dramatic" changes in risk level for the product!</p>
<h1>Summary</h1>
<ul>
<li>65 % reduction in test matrix;</li>
<li>Only 1/3rd of team engineers needed;<ul>
<li>keep arch experts around though;</li>
</ul>
</li>
<li>2/3rd of team engineers could be free for automation and to create even more test cases;</li>
<li>Test run execution completion rate is comparable to regular test plan<ul>
<li>average execution completion for pairwise test plan was 76%!</li>
<li>average execution completion for regular test plan was 85%!</li>
</ul>
</li>
<li>New bugs found:<ul>
<li>30% by Pairwise Test Plan</li>
<li>30% by Tier #1 test cases (good job here)</li>
<li>30% by exploratory testing</li>
</ul>
</li>
<li>Risk of missing regressions or critical bugs exists (I did miss 3) but can be mitigated;</li>
<li>Clearly exposes the need of constant review, analysis and improvement of existing test cases;</li>
<li>Exposes hidden parameters in test scenarios and some hidden relationships;</li>
<li>Patterns and other optimization techniques observed</li>
</ul>
<p>Patterns observed:</p>
<ul>
<li>Many new test case combinations found, which I had to describe into
  <a href="http://atodorov.org/blog/2017/04/04/article-about-nitrate-in-methods-tools/">Nitrate</a>; The longer you use
  pairwise the less new combinations are discovered (aka undocumented scenarios).
  The first 3 initial test runs discovered the most of the missing combinations!</li>
<li>Found quite a few test cases with hidden parameters, for example <code>swap / recommended</code>
  which calculates the recommended size of swap partition based on 4 different
  ranges in which the actual RAM size fits! These ranges became parameters
  to the test case;</li>
<li>Can combine (2, 3, etc) independent test cases together and consider them as parameters
  so we can apply pairwise against the combination. This will create new scenarios, broaden
  the test matrix but not result in significant increase in execution time. I didn't try this
  because it was not the focus of the experiment;</li>
<li>Found some redundant/duplicate test cases - test plans need to be constantly analyzed and
  maintained you know;</li>
<li>Automated scheduling and tools integration is critical. This needs to be working perfectly
  in order to capitalize on the newly freed resources;</li>
<li>Testing on s390x was sub-optimal (mostly my own inexperience with the platform) so for
  specialized environments we still want to keep the experts around;</li>
<li>1 engineer (me) was able to largely keep up with schedule with the rest of the team!<ul>
<li>experiment was conducted during the course of several months</li>
<li>I have tried to adhere to all milestones and deadlines and mostly succeeded</li>
</ul>
</li>
</ul>
<p>I have also discovered ideas for new test execution optimization techniques
which need to be evaluated and measured further:</p>
<ul>
<li>Use a common set-up step for multiple test cases across variants, e.g.<ul>
<li>install a RAID system then;</li>
<li>perform 3 rescue mode tests (same test case, different variants)</li>
</ul>
</li>
<li>Pipeline test cases so that the result of one case is the setup for the next, e.g.<ul>
<li>install a RAID system and test for correctness of the installation;</li>
<li>perform rescue mode test;</li>
<li>damage one of the RAID partitions while still in rescue mode;</li>
<li>test installation with damaged disks - it should not crash!</li>
</ul>
</li>
</ul>
<p>These techniques can be used stand-alone or in combination with
other optimization techniques and tooling available to the team. They are
specific to my particular kind of testing so beware of your surroundings
before you try them out!</p>
<p>Thanks for reading and happy testing!</p>
<p><em>Cover image copyright: cio-today.com</em></p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Fri 14 April 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/04/14/testing-red-hat-enterprise-linux-the-microsoft-way/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/04/05/mutation-testing-vs-coverage-pt-2/" rel="bookmark" title="Permalink to Mutation Testing vs. Coverage, Pt. 2">
                <h2 class="post-title">
                    Mutation Testing vs. Coverage, Pt. 2
                </h2>
            </a>
                <p>In a <a href="http://atodorov.org/blog/2016/12/27/mutation-testing-vs-coverage/">previous post</a> I
have shown an example of real world bugs which we were not able to detect
despite having 100% mutation and test coverage. I am going to show you another
example here.</p>
<p>This example comes from one of my training courses. The task is to write a
class which represents a bank account with methods to deposit, withdraw and
transfer money. The solution looks like this</p>
<div class="highlight"><span class="filename">bank.py</span><pre><span class="k">class</span> <span class="nc">BankAccount</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">balance</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_balance</span> <span class="o">=</span> <span class="n">balance</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_history</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">deposit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">amount</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">amount</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;Deposit amount must be positive!&#39;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_balance</span> <span class="o">+=</span> <span class="n">amount</span>

    <span class="k">def</span> <span class="nf">withdraw</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">amount</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">amount</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">&#39;Withdraw amount must be positive!&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">amount</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_balance</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_balance</span> <span class="o">-=</span> <span class="n">amount</span>
            <span class="k">return</span> <span class="bp">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s">&quot;Withdraw for </span><span class="si">%d</span><span class="s"> failed&quot;</span> <span class="o">%</span> <span class="n">amount</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">False</span>

    <span class="k">def</span> <span class="nf">transfer_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other_account</span><span class="p">,</span> <span class="n">how_much</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">withdraw</span><span class="p">(</span><span class="n">how_much</span><span class="p">)</span>
        <span class="n">other_account</span><span class="o">.</span><span class="n">deposit</span><span class="p">(</span><span class="n">how_much</span><span class="p">)</span>
</pre></div>


<p>Notice that if withdrawal is not possible then the function returns <code>False</code>. The tests
look like this</p>
<div class="highlight"><span class="filename">test.py</span><pre><span class="kn">import</span> <span class="nn">unittest</span>
<span class="kn">from</span> <span class="nn">solution</span> <span class="kn">import</span> <span class="n">BankAccount</span>


<span class="k">class</span> <span class="nc">TestBankAccount</span><span class="p">(</span><span class="n">unittest</span><span class="o">.</span><span class="n">TestCase</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">setUp</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">account</span> <span class="o">=</span> <span class="n">BankAccount</span><span class="p">(</span><span class="s">&quot;Rado&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">test_deposit_positive_amount</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">account</span><span class="o">.</span><span class="n">deposit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">account</span><span class="o">.</span><span class="n">_balance</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">test_deposit_negative_amount</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">assertRaises</span><span class="p">(</span><span class="ne">ValueError</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">account</span><span class="o">.</span><span class="n">deposit</span><span class="p">(</span><span class="o">-</span><span class="mi">100</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">test_deposit_zero_amount</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">assertRaises</span><span class="p">(</span><span class="ne">ValueError</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">account</span><span class="o">.</span><span class="n">deposit</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">test_withdraw_positive_amount</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">account</span><span class="o">.</span><span class="n">deposit</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>

        <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">account</span><span class="o">.</span><span class="n">withdraw</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assertTrue</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">account</span><span class="o">.</span><span class="n">_balance</span><span class="p">,</span> <span class="mi">99</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">test_withdraw_maximum_amount</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">account</span><span class="o">.</span><span class="n">deposit</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>

        <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">account</span><span class="o">.</span><span class="n">withdraw</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assertTrue</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">account</span><span class="o">.</span><span class="n">_balance</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">test_withdraw_from_empty_account</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">account</span><span class="o">.</span><span class="n">withdraw</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">assertIsNotNone</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assertFalse</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
        <span class="k">assert</span> <span class="s">&quot;Withdraw for 50 failed&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">account</span><span class="o">.</span><span class="n">_history</span>

    <span class="k">def</span> <span class="nf">test_withdraw_non_positive_amount</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">assertRaises</span><span class="p">(</span><span class="ne">ValueError</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">account</span><span class="o">.</span><span class="n">withdraw</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">assertRaises</span><span class="p">(</span><span class="ne">ValueError</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">account</span><span class="o">.</span><span class="n">withdraw</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">test_transfer_negative_amount</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">account_1</span> <span class="o">=</span> <span class="n">BankAccount</span><span class="p">(</span><span class="s">&#39;For testing&#39;</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
        <span class="n">account_2</span> <span class="o">=</span> <span class="n">BankAccount</span><span class="p">(</span><span class="s">&#39;In dollars&#39;</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">assertRaises</span><span class="p">(</span><span class="ne">ValueError</span><span class="p">):</span>
            <span class="n">account_1</span><span class="o">.</span><span class="n">transfer_to</span><span class="p">(</span><span class="n">account_2</span><span class="p">,</span> <span class="o">-</span><span class="mi">50</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="n">account_1</span><span class="o">.</span><span class="n">_balance</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="n">account_2</span><span class="o">.</span><span class="n">_balance</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">test_transfer_positive_amount</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">account_1</span> <span class="o">=</span> <span class="n">BankAccount</span><span class="p">(</span><span class="s">&#39;For testing&#39;</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
        <span class="n">account_2</span> <span class="o">=</span> <span class="n">BankAccount</span><span class="p">(</span><span class="s">&#39;In dollars&#39;</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

        <span class="n">account_1</span><span class="o">.</span><span class="n">transfer_to</span><span class="p">(</span><span class="n">account_2</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="n">account_1</span><span class="o">.</span><span class="n">_balance</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="n">account_2</span><span class="o">.</span><span class="n">_balance</span><span class="p">,</span> <span class="mi">60</span><span class="p">)</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">unittest</span><span class="o">.</span><span class="n">main</span><span class="p">()</span>
</pre></div>


<p>Try the following commands to verify that you have 100% coverage</p>
<div class="highlight"><pre>coverage run test.py
coverage report

cosmic-ray run --test-runner nose --baseline 10 example.json bank.py -- test.py`
cosmic-ray report example.json
</pre></div>


<p>Can you tell where the bug is ? How about I try to transfer more money than is
available from one account to the other</p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">test_transfer_more_than_available_balance</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">account_1</span> <span class="o">=</span> <span class="n">BankAccount</span><span class="p">(</span><span class="s">&#39;For testing&#39;</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">account_2</span> <span class="o">=</span> <span class="n">BankAccount</span><span class="p">(</span><span class="s">&#39;In dollars&#39;</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="c"># transfer more than available</span>
    <span class="n">account_1</span><span class="o">.</span><span class="n">transfer_to</span><span class="p">(</span><span class="n">account_2</span><span class="p">,</span> <span class="mi">150</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="n">account_1</span><span class="o">.</span><span class="n">_balance</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="n">account_2</span><span class="o">.</span><span class="n">_balance</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>


<p>If you execute the above test it will fail</p>
<div class="highlight"><pre><span class="n">FAIL</span><span class="o">:</span> <span class="n">test_transfer_more_than_available_balance</span> <span class="o">(</span><span class="n">__main__</span><span class="o">.</span><span class="na">TestBankAccount</span><span class="o">)</span>
<span class="o">----------------------------------------------------------------------</span>
<span class="n">Traceback</span> <span class="o">(</span><span class="n">most</span> <span class="n">recent</span> <span class="n">call</span> <span class="n">last</span><span class="o">):</span>
  <span class="n">File</span> <span class="s2">&quot;./test.py&quot;</span><span class="o">,</span> <span class="n">line</span> <span class="mi">79</span><span class="o">,</span> <span class="k">in</span> <span class="n">test_transfer_more_than_available_balance</span>
    <span class="n">self</span><span class="o">.</span><span class="na">assertEqual</span><span class="o">(</span><span class="n">account_2</span><span class="o">.</span><span class="na">_balance</span><span class="o">,</span> <span class="mi">10</span><span class="o">)</span>
<span class="n">AssertionError</span><span class="o">:</span> <span class="mi">160</span> <span class="o">!=</span> <span class="mi">10</span>

<span class="o">----------------------------------------------------------------------</span>
</pre></div>


<p>The problem is that when <code>self.withdraw(how_much)</code> fails <code>transfer_to()</code> ignores
the result and tries to deposit the money into the other account! A better
implementation would be</p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">transfer_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other_account</span><span class="p">,</span> <span class="n">how_much</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">withdraw</span><span class="p">(</span><span class="n">how_much</span><span class="p">):</span>
        <span class="n">other_account</span><span class="o">.</span><span class="n">deposit</span><span class="p">(</span><span class="n">how_much</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s">&#39;Transfer failed!&#39;</span><span class="p">)</span>
</pre></div>


<p>In my earlier article the bugs were caused by external environment
and tools/metrics like code coverage and mutation score are not affected by it.
In fact the jinja-ab example falls into the category of data coverage testing.</p>
<p>The current example on the other hand is ignoring the return value of the <code>withdraw()</code>
function and that's why it fails when we add the appropriate test.</p>
<p><strong>NOTE:</strong> some mutation test tools support the <em>removing/modifying return value</em>
mutation. Cosmic Ray doesn't support this at the moment (I should add it). Even if it did
that would not help us find the bug because we would kill the mutation using
the <code>test_withdraw...()</code> test methods, which already assert on the return value!</p>
<p>Thanks for reading and happy testing!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Wed 05 April 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/04/05/mutation-testing-vs-coverage-pt-2/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2017/01/03/circo-loco-2017/" rel="bookmark" title="Permalink to Circo loco 2017">
                <h2 class="post-title">
                    Circo loco 2017
                </h2>
            </a>
                <p><img alt="HackConf 2016" src="/images/hack_conf.jpg" title="HackConf 2016" /></p>
<p>Due to popular demand I'm sharing my plans for the upcoming conference season.
Here is a list of events I plan to visit and speak at (hopefully). The list
will be updated throughout the year so please subscribe to the comments section
to receive a notification when that happens! I'm open to meeting new people so
ping me for a beer if you are attending some of these events!</p>
<h2>02 February - Hack Belgium Pre-Event Workshops, Brussels</h2>
<p><strong>Note: added on Jan 12th</strong></p>
<p>Last year I had an amazing time visiting an
<a href="http://atodorov.org/blog/2016/02/02/fosdem-2016-report/">Elixir &amp; Erlang workshop</a> so
I'm about to repeat the experience. I will be visiting a
<a href="http://www.hackathon.com/event/hack-belgium-pre-event-workshops-29372270251">workshop</a>
organized by HackBelgium and keep you posted with the results.</p>
<h2>03 February - Git Merge, Brussels</h2>
<p><strong>Note: added on Jan 12th</strong></p>
<p><a href="http://git-merge.com/">Git Merge</a> is organized by GitHub and will be held in Brussels
this year. I will be visiting only the conference track and hopefully giving a lightning talk
titled <em>Automatic upstream dependency testing with GitHub API</em>! That and the afterparty
of course!</p>
<h2>04-05 February - FOSDEM, Brussels</h2>
<p><a href="https://fosdem.org/2017/">FOSDEM</a> is the largest free and open source gathering
in Europe which I have been visiting since 2009 (IIRC). You can checkout some
of my reports about
<a href="http://atodorov.org/blog/2014/02/03/fosdem-2014-report-day-1-python-stands-lightning-talks/">FOSDEM 2014, Day 1</a>,
<a href="http://atodorov.org/blog/2014/02/03/fosdem-2014-report-day-2-testing-and-automation/">FOSDEM 2014, Day 2</a> and
<a href="http://atodorov.org/blog/2016/02/02/fosdem-2016-report/">FOSDEM 2016</a>.</p>
<p>I will present my
<a href="https://fosdem.org/2017/schedule/event/mutant_testing/">Mutants, tests and zombies</a>
talk at the <em>Testing &amp; Automation devroom</em> on Sunday.</p>
<p><strong>UPDATE:</strong> Video recording is available
<a href="https://www.youtube.com/watch?v=ZyKvwODy9jw&amp;list=PLFjlI7p-h1hxBP3cIjEqePSeoBDHud5Db">here</a></p>
<p>I will be in Brussels between February 1st and 5th to explore the local
start-up scene and get to meet with the Python community so ping me if you are around.</p>
<h2>18 March - QA Challenge Accepted 3.0, Sofia</h2>
<p><a href="http://qachallengeaccepted.com/">QA: Challenge Accepted</a> is a specialized QA conference
in Sofia and most of the sessions are in Bulgarian. I've visited last year and it was great.
I even proposed a <a href="http://atodorov.org/blog/2016/03/25/hello-world-qa-challenge/">challenge of my own</a>.</p>
<p>CFP is still open but I have a strong confidence that my talk
<em>Testing Red Hat Enterprise Linux the MicroSoft way</em> will be approved. It will describe
a large scale experiment with pairwise testing, which btw I learned about at
QA: Challenge Accepted 2.0 :).</p>
<p><strong>UPDATE:</strong> I will be also on the jury for <em>QA of the year</em> award.</p>
<p><strong>UPDATE:</strong> I did a lightning talk about my
<a href="https://www.youtube.com/watch?v=14e1z-pYwMc">test case management system</a> (in Bulgarian).</p>
<h2>07-08 April - Bulgaria Web Summit, Sofia</h2>
<p>I have been a moderator at
<a href="https://bulgariawebsummit.com/">Bulgaria Web Summit</a> for several years and this year
is no exception. This is one of the strongest events held in Sofia and is in English.
Last year over 60% of the attendees were from abroad so you are welcome!</p>
<p>I'm not going to speak at this event but will record as much of it
as possible. <strong>UPDATE:</strong> Checkout the recordings on my
<a href="https://www.youtube.com/watch?v=NW8wjpK-hRg&amp;list=PLFjlI7p-h1hxBP3cIjEqePSeoBDHud5Db&amp;index=11">YouTube</a>
channel!</p>
<h2>10-12 May - Romanian Testing Conference, Cluj-Napoca</h2>
<p><a href="http://romaniatesting.ro/">RTC'17</a> is a new event I found in neighboring Romania.
The topic this year is <em>Thriving and remaining relevant in Quality Assurance</em>. My talk is titled
<em>Quality Assistance in the Brave New World</em> where I'll share some experiences
and visions for the QA profession if that gets accepted.</p>
<p>As it turned out I know a few people living in Cluj so I'll be arriving one day earlier
on May 9th to meet the locals.</p>
<h2>13-14 May - OSCAL, Tirana</h2>
<p><a href="http://oscal.openlabs.cc/">Open Source Conference Albania</a> is the largest OSS event in
the country. I'm on a row here to explore the IT scene on the Balkans. Due to traveling
constraints my availability will be limited to the conference venue only but I've booked
a hotel across the street :).</p>
<p>I will be meeting a few friends in Tirana and hear about the progress of an
<a href="http://atodorov.org/blog/2016/11/21/women-in-open-source/">psychological experiment</a> we devised
with Jona Azizaj and Suela Palushi.</p>
<p>Talking wise I'm hoping to get the chance of introducing mutation testing and
even host a workshop on the topic.</p>
<p><strong>UPDATE:</strong> Here is the video recording from
<a href="https://www.youtube.com/watch?v=mG1eAhMWZJ8">OSCAL</a>. The quality is very poor though.</p>
<h2>20-21 May - DEVit, Thessaloniki</h2>
<p>This is the 3rd edition of <a href="http://devitconf.org/">DEVit</a>,
the 360 web development conference of Northern Greece. I've been a regular
visitor since the beginning and this year I've proposed a session on mutation
testing. Because the Thessaloniki community seems more interested in Ruby and Rails
my goal is to share more examples from my Ruby work and compare how that is different
from the Python world. There is once again an opportunity for a workshop.</p>
<p>So far I've been the only Bulgarian to visit DEVit and also locally known as
"The guy who Kosta &amp; Kosta met in Sofia"! Checkout my impressions from
<a href="http://atodorov.org/blog/2015/05/22/devit-conf-2015-impressions/">DEVit'15</a> and
<a href="http://atodorov.org/blog/2016/05/25/devit-conf-2016/">DEVit'16</a>
if you are still wondering whether to attend or not! I strongly recommend it!</p>
<p><strong>UPDATE:</strong> I am still the only Bulgarian visiting DEVit!</p>
<h2>22 May - Dev.bg, Sofia</h2>
<p>I've hosted a session titled <em>Quality Assurance According to Einstein</em> for the
local QA community in Sofia. <a href="https://www.youtube.com/watch?v=CnxNCdX3iG0">Video</a>
(in Bulgarian) and <a href="http://atodorov.org/blog/2017/04/20/quality-assurance-according-2-einstein/">links</a> are available!</p>
<p><strong>UPDATE:</strong> added post-mortem.</p>
<h2>01-02 June - Shift Developer Conference, Split</h2>
<p><a href="http://shift.codeanywhere.com/">Shift</a> appears to be a very big event in Croatia.
My attendance is still unconfirmed due to lots of traveling before that and
the general trouble of efficiently traveling on the Balkans. However I have a
CFP submitted and waiting for approval. This time it is my
<em>Mutation Testing in Patterns</em>, which is a journal of different code patterns
found during mutation testing. I have not yet presented it to the public
but will blog about it sometime soon so stay tuned.</p>
<p><strong>UPDATE:</strong> this one is a no-go!</p>
<h2>03-04 June - TuxCon, Plovdiv</h2>
<p><a href="http://tuxcon.mobi/">TuxCon</a> is held in Plovdiv around the beginning of July.
I'm usually presenting some lightning talks there and use the opportunity to
meet with friends and peers outside Sofia and catch up with news from the
local community. The conference is in Bulgarian with the exception of the
occasional foreign speaker. If you understand Bulgarian I recommend the
story of
<a href="http://atodorov.org/blog/2016/07/12/testing-the-8-bit-computer-puldin/">Puldin - a Bulgarian computer from the 80s</a>.</p>
<p><strong>UPDATE:</strong> I will be opening the conference with <em>QA According to Einstein</em></p>
<h2>17-18 June - How Camp, Varna</h2>
<p><a href="http://how.camp/">How Camp</a> is the little brother of Bulgaria Web Summit and is always
held outside of Sofia. This year it will be in Varna, Bulgaria. I will be there of course
and depending on the crowd may talk about some software testing patterns.</p>
<p><strong>UPDATE:</strong> it looks like this is also a no-go but stay tuned for the upcoming
<em>Macedonia Web Summit</em> and <em>Albania Web Summit</em> where yours trully will probably
be a moderator!</p>
<h2>25-29 September - SuSE CON, Prague</h2>
<p>Yeah, this is the <a href="http://www.susecon.com">conference organized by SuSE</a>. I'm definitely
not afraid to visit the competition. I even hope I could teach them something. More
details are still TBA because this event is very close to/overlapping with the next
two.</p>
<h2>28-29 September - SEETEST, Sofia</h2>
<p><a href="http://seetest.org">South East European Software Testing Conference</a> is, AFAIK,
an international event which is hosted in a major city on the Balkans. Last year it was
held in Bucharest with previous years held in Sofia.</p>
<p>In my view this is the most formal event, especially related to software testing,
I'm about to visit. Nevertheless I like to hear about new ideas and some research
in the field of QA so this is a good opportunity.</p>
<p><strong>UPDATE:</strong> I have submitted a new talk titled
<em>If the facts don't fit the theory, change the facts!</em></p>
<h2>30 September-1 October - HackConf, Sofia</h2>
<p><a href="https://hackconf.bg">HackConf</a> is one of the largest conferences in Bulgaria, gathering
over 1000 people each year. I am strongly affiliated with the people who organize it
and even had the opportunity to host the opening session last year. The picture above
is from this event.</p>
<p>The audience is still very young and inexperienced but the presenters are above average.
Organizers' goal is HackConf to become the strongest technical conference in the country
and also serve as a sort of inspiration for young IT professionals.</p>
<p><strong>UPDATE:</strong> I have submitted both a talk proposal and a workshop proposal.
For the workshop I intend to teach children
<a href="http://atodorov.org/blog/2017/05/27/learn-python-selenium-automation-in-8-weeks/">Python and Selenium Automation in 8 hours</a>.
I've also been helping the organizers with bringing some very cool speakers
from abroad!</p>
<h2>October - IT Weekend, Bulgaria</h2>
<p><a href="http://it-weekend.com/">IT Weekend</a> is organized by Petar Sabev, the same person who's
behind QA: Challenge Accepted. It is a non-formal gathering of engineers with the intent
to share some news and then discuss and share problems and experiences. Checkout my
review of
<a href="http://atodorov.org/blog/2016/10/08/what-i-learned-from-it-weekend/">IT Weekend #1</a> and
<a href="http://atodorov.org/blog/2016/11/02/it-weekend-highlights/">IT Weekend #3</a>.</p>
<p>The topic revolve around QA, leadership and management but the format is open and the
intention is to broaden the topics covered. The event is held outside Sofia at a SPA
hotel and makes for a very nice retreat. I don't have a topic but I'm definitely going
if time allows. I will probably make something up if we have QA slots available :).</p>
<h2>October - Software Freedom Kosova, Prishtina</h2>
<p><a href="http://sfk.flossk.org">Software Freedom Kosova</a> is one of the oldest conferences
about free and open source software in the region.
This is part of my goal to explore the IT communities on the Balkans.
Kosovo sounds a bit strange to visit but I did recognize a few names on the speaker
list of previous years.</p>
<p>The CFP is not open yet but I'm planning to make a presentation. Also if weather
allows I'm planning a road trip on my motorbike :).</p>
<p><strong>UPDATE:</strong> I've met with some of the FLOSSK members in Tirana at OSCAL and they
seem to be more busy with running the hacker space in Prishtina so the conference
is nearly a no-go.</p>
<h2>14-15 November - Google Test Automation Conference, London</h2>
<p><a href="https://developers.google.com/google-test-automation-conference/">GTAC 2017</a>
will be held in London. Both speakers and attendees are pre-approved
and my goal is to be the first Red Hatter and second Bulgarian to speak at GTAC.</p>
<p>The previous two years saw talks about
<a href="http://atodorov.org/blog/2016/12/27/mutation-testing-vs-coverage/">mutation testing vs. coverage</a>
and their respective use
to determine the quality of a test suite with both parties arguing against
each other. Since I'm working in both of these fields and have at least two
practical example I'm trying to gather more information and present my findings
to the world.</p>
<p><strong>UPDATE:</strong> I've also submitted my <em>Testing Red Hat Enterprise Linux the Microsoft way</em></p>
<h2>16-17 November - ISTA Con, Sofia</h2>
<p><a href="https://istacon.org/">Innovations in Software Technologies and Automation</a> started as a QA conference
several years ago but it has broaden the range of acceptable topics to include
development, devops and agile. I was at the first two editions and then didn't attend
for a while until <a href="http://atodorov.org/blog/2016/11/30/highlights-from-ista-and-gtac-2016/">last year</a> when I
really liked it. The event is entirely in English with lots of foreign speakers.</p>
<p>Recently I've been working on something I call "Regression Test Monitoring" and my
intention is to present this at ISTA 2017 so stay tuned.</p>
<p><strong>UPDATE:</strong> I didn't manage to collect enough information on the
<em>Regression Test Monitoring</em> topic but have made two other proposals.</p>
<p>Thanks for reading and see you around!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Tue 03 January 2017
            </p>
<p>There are <a href="http://atodorov.org/blog/2017/01/03/circo-loco-2017/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2016/12/29/qa-automation-101-retrospective/" rel="bookmark" title="Permalink to QA & Automation 101 Retrospective">
                <h2 class="post-title">
                    QA & Automation 101 Retrospective
                </h2>
            </a>
                <p><img alt="QA 101 graduation" src="/images/qa101_final.jpg" title="QA 101 graduation" /></p>
<p>At the beginning of this year I've hosted the first QA related course at
<a href="http://hackbulgaria.com/">HackBulgaria</a>. This is a long overdue post about
how the course went, what worked well and what didn't. Brace yourself because
it is going to be a long one.</p>
<p>The idea behind a QA course has been lurking in both RadoRado's (from HackBulgaria)
and my heads for quite a while. We've been discussing it at least a year before we
actually started. One day Rado told me he'd found a sponsor and we have the go ahead
for the course and that's how it all started!</p>
<p>The first issue was that we weren't prepared to start at a moments notice. I literally
had two weeks to prepare
<a href="https://github.com/HackBulgaria/QA-and-Automation-101">the curriculum</a>
and <a href="http://atodorov.org/blog/2016/04/12/how-to-hire-software-testers-pt-1/">initial interview questions</a>.
Next we opened the application form and left it open until the last possible moment.
I've been reviewing candidate answers hours before the course started, which was
another mistake we made!</p>
<p>On the positive side is that I hosted a
<a href="https://www.youtube.com/watch?v=Tgu0VTxytYw&amp;index=37&amp;list=PLFjlI7p-h1hxBP3cIjEqePSeoBDHud5Db">Q&amp;A session on YouTube</a>
answering general questions about the profession and the course itself. This
live stream helped popularize the course.</p>
<p>At the start we had 30 people and around 13 of them managed to "graduate"
till the final lesson. The biggest portion of students dropped out after the
first 5 lessons of Java crash course! Each lesson was around 4 hours with 20-30
minutes break in the middle.</p>
<p>With respect to the criteria find a first job or find a new/better job I consider the
training successful. To my knowledge all students have found better jobs, many of them
as software testers!</p>
<p>On the practical side of things students managed to find and report 11 interesting
bugs against Fedora. Mind you that these were all found in the wild:
<a href="https://fedorahosted.org/fedora-infrastructure/ticket/5323">fedora-infrastructure #5323</a>,
<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1339701">RHBZ#1339701</a>,
<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1339709">RHBZ#1339709</a>,
<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1339713">RHBZ#1339713</a>,
<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1339719">RHBZ#1339719</a>,
<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1339731">RHBZ#1339731</a>,
<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1339739">RHBZ#1339739</a>,
<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1339742">RHBZ#1339742</a>,
<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1339746">RHBZ#1339746</a>,
<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1340541">RHBZ#1340541</a>,
<a href="https://bugzilla.redhat.com/show_bug.cgi?id=1340891">RHBZ#1340891</a>.</p>
<p>Then students also made a few pull requests on GitHub (3 which I know off):
<a href="https://github.com/apache/commons-math/pull/38">commons-math #38</a>,
<a href="https://github.com/apache/commons-csv/pull/12">commons-csv #12</a>,
<a href="https://github.com/apache/commons-email/pull/1">commons-email #1</a>.</p>
<h2>Lesson format</h2>
<p>For reference most lessons were a mix of short presentation about theory and best practices
followed by discussions and where appropriate practical sessions with technology or
projects. The exercises were designed for individual work, work in pairs or small groups (4-5)
on purpose.</p>
<p>By request from the sponsors I've tried to keep a detailed record of each student's
performance and personality traits as much as I was able to observe them. I really enjoyed
keeping such a journal but didn't share this info with my students which I consider
a negative issue. I think knowing where your strong and weak areas are would help you
become a better expert in your field!</p>
<h2>Feedback from students</h2>
<ul>
<li>There was little time (to work on the practical examples I guess);</li>
<li>Not having particular practical tasks was a problem. For example we didn't have tasks of the
sort do "X" then "Y";</li>
<li>They needed more time and more attention to be given to them;</li>
<li>Installing different pieces of software and tools took a lot of time and frustration.
It was also quite problematic sometimes depending on whether they used Linux, Windows or Mac OS X;</li>
<li>Working with Eclipse IDE was horrible. Nobody new what to do and the interface
wasn't newbie friendly. Also it took quite a lot of time to install dependencies and/or
import projects to run in Eclipse;</li>
<li>There were a few problems with Selenium and its different versions being used.</li>
</ul>
<p>I have to point out that while these are valid concerns and major issues students were
at least partially guilty for the last 3 of them. It was my impression that most of them
didn't prepare at home, didn't read the next lesson and didn't install prerequisite
tools and software!</p>
<h2>5x Java Crash Course</h2>
<p>We've started with a Java crash course as requested by our sponsors which was extended to
5 instead of the original 3 sessions. RadoRado was teaching Java fundamentals while
I was assisting him with comments.</p>
<p>On the good side is that Rado explains very well and in much details. He also writes code
to demonstrate what he teaches and while doing so uses only the knowledge he's presented
so far. For example if there's a repeating logic/functionality he would just write it
twice instead of refactoring that into a separate function with parameters (assuming
the students have not learned about functions yet). I think this made it more easier
to understand the concepts being taught.</p>
<p>Another positive thing we did was me going behind Rado's computer and modifying some
of the code while he was explaining something on screen. If you take the above example
and have two methods with print out salutations, e.g. "Good morning, Alex" I would go
and modify one of them to include "Mr." while the other will not. This introduced a
change in behavior which ultimately results in a bug! This was a nice practical way
to demonstrate how some classes of bugs get introduced in reality. We did only a few of
these behind the computer changes and I definitely liked them! They were all ad-hoc,
not planned for.</p>
<p>On the negative side Java seems hard to learn and after these 5 lessons half of the
students dropped out. Maybe part of the reason is they didn't expect to start a QA
course with lessons about programming. But that also means they didn't pay enough
attention to the curriculum, which was announced in advance!</p>
<h2>Lesson 01 - QA Fundamentals</h2>
<p>I had made a point to assign time constraints to each exercise in the lessons.
While that mostly worked in the first few lessons, where there is more theory,
we didn't keep the schedule and were overtime.</p>
<p>Explaining testing theory (based on ISTQB fundamentals) took longer than I expected.
it was also evident that we needed more written examples of what different test
analysis techniques are (e.g. boundary value analysis). Here Petar Sabev helped
me deliver few very nice examples.</p>
<p>One of the exercises was "when to stop testing" with an example of a Sudoku solving
function and different environments in which this code operates, e.g. browser, mobile,
etc. Students appeared to have a hard time understanding what a
"runtime environment" is and define relevant tests based on that! I believe most
of the students, due to lack of knowledge and experience, were also having a hard time grasping the
concept of non-functional testing.</p>
<p>A positive thing was that students started explaining to one another and giving
examples for bugs they've seen outside the course.</p>
<h2>Lesson 02 - Software Development Lifecycle</h2>
<p>This lesson was designed as role playing game to demonstrate the most common
software development methodologies - waterfall and agile and discuss the QA role
in both of them. The format by itself is very hard to conduct successfully and
this was my first time ever doing this. I've also never taken part of such games
until then, only heard about them.</p>
<p>During the waterfall exercise it was harder for the students to follow the game
constraints and not exchange information with one another because they were sitting
on the same table.</p>
<p>On the positive side all groups came with unique ideas about software features and how
they want to develop them. Timewise we managed to do very well. On the negative side
is that I was the client for all groups and didn't manage to pay enough attention
to everyone, which btw is what clients usually do in real life.</p>
<h2>Lesson 03 - Bug Tracking</h2>
<p>This lesson was a practical exercise in writing bug reports and figuring out what
information needs to be present in a good bug report. Btw this is something I
always ask junior members at job interviews.</p>
<p>First we started with working in pairs to define what a good bug report is
without actually knowing what that means. Students found it hard to brainstorm
together and most of them worked alone during this exercise.</p>
<p>Next students had to write bug reports for some example bugs, which I've explained briefly
on purpose and perform peer reviews of their bugs. Reviews took a long time to complete
but overall students had a good idea of what information to include in a bug report.</p>
<p>Then, after learning from their mistakes and hearing what others had done, they've
learned about some good practices and were tasked to rewrite their bug reports using
the new knowledge. I really like the approach of letting students make some mistakes
and then showing them the easier/better way of doing things. This is also on-par with
<a href="http://atodorov.org/blog/2016/09/25/what-ivan-learned-from-organizing-internships/">Ivan Nemytchenko's methodology</a> of
letting his interns learn by their mistakes.</p>
<p>All bug reports can be found in students repositories, which are forked from the
curriculum. Check out <a href="https://github.com/HackBulgaria/QA-and-Automation-101/network">https://github.com/HackBulgaria/QA-and-Automation-101/network</a>.</p>
<p>I should have really asked everyone to file bugs under the curriculum repository so
it is easier for me to track them. On the other hand I wanted each student to start building
their own public profile to show potential employers.</p>
<h2>Lesson 04 - Test Case Management</h2>
<p>This lesson started with an exercise asking students to create accounts for
Red Hat's OpenShift cloud platform in the form of a test scenario. The scenario intentionally
left out some details. The idea being that missing information introduces inconsistencies
during testing and to demonstrate how the same steps were performed slightly differently.</p>
<p>We had some troubles explaining exactly "how did you test" because most inexperienced people
would not remember all details about where they clicked, did they use the mouse or the
keyboard, was the tab order correct, etc. Regardless students managed to receive different
results and discover that some email providers were not supported.</p>
<p>The homework assignment was to create test plans and test cases in Nitrate at
<a href="https://nitrate-hackbg.rhcloud.com/">https://nitrate-hackbg.rhcloud.com/</a>. Unfortunately the system appears to be down ATM
and I don't have time to investigate why. This piece of infrastructure was put together
in 2 hours and I'm surprised it lasted without issues during the entire course.</p>
<h2>2x Introduction to Linux</h2>
<p>This was a crash course in Linux fundamentals and exercise with most common commands
and text editors in the console. Most of the students were not prepared with virtual
machines. We've also used a cloud provider to give students remote shell but the
provider API was failing and we had to deploy docker containers manually. Overall
infrastructure was a big problem but we somehow managed. Another problem was with
ssh clients from Windows who generated keys in a format that our cloud provider
couldn't understand.</p>
<p>Wrt commands and exercises students did well and managed to execute most of them
on their own. That's very good for people who've never seen a terminal in their lives
(more or less).</p>
<h2>Lesson 05 - Testing Fedora 24(25) Changes</h2>
<p>Once again nobody was prepared with a virtual machine with Fedora and students
were installing software as we go. Because of that we didn't manage to conduct
the lesson properly and had to repeat it on the next session.</p>
<p>Rawhide being the bleeding edge of Fedora means it is full of bugs. Well I couldn't
keep up with everyone and explain workarounds or how to install/upgrade Fedora.
That was a major setback. It also became evident that you can't move quickly
if you have no idea what to do and no instructions about it either.</p>
<h2>Lesson 05 - Again</h2>
<p>Once prepared with the latest and greatest from Rawhide the task was to analyze the
proposed feature changes (on the Fedora wiki) and create test plans and design
test cases for said changes. Then execute the tests in search for bugs. This is where
some of the bugs above came from. The rest were found during upgrades.</p>
<p>This lesson was team work (4-5 students) but the results were mixed. IMO Fedora changes
are quite hard to grasp, especially if you lack domain knowledge and broader knowledge
about the structure and operation of a Linux distribution. I don't think most teams
were able to clearly understand their chosen features and successfully create good
plans/scenarios for them. On the other hand in real life software you don't necessarily
understand the domain better and know what to do. I've been in situations where
whole features have been defined by a single sentence and requested to be tested by QA.</p>
<p>One of the teams didn't manage to install Fedora (IIRC they didn't have laptops
capable of running a VM) and were not able to conduct the exercise.</p>
<p>Being able to find real life bugs, some of them serious, and getting traction in Bugzilla
is the most positive effect of this lesson. I personally wanted to have more output
(e.g. more bugs, more cases defined, etc) but taking into account the blocking factors
and setbacks I think this is a good initial result.</p>
<h2>Lesson 06 - Unit Testing and Continuous Integration</h2>
<p>Here we had a few examples of bad stubs and mocks which were not received very well.
The topic is hard in itself and wasn't very well explained with practical examples.</p>
<p>Another negative thing is that students took a lot of time to fiddle around with
Eclipse, they were mistyping commands in the terminal and generally not paying enough
attention to instructions. This caused the exercises to go slowly.</p>
<p>We've had an exercise which asks the student to write a new test for a non-existing
method. Then implement the method and make sure all the tests passed. You guessed it
this is Test Driven Development.
IIRC one of the students was having a hard time with that exercise so I popped up
my editor on the large screen and started typing what she told me, then re-running the
tests and asking her to show me the errors I've made and tell me how to correct them.
The exercise was received very well and was fun to do.</p>
<p>Due to lack of time we had to go over TravisCI very quickly. The other bad thing about
TravisCI is that it requires git/GitHub and the students were generally inexperienced
with that. Both GitHub for Windows and Mac OS suck a big time IMO. What you need is the
console. However none of the students had any practical experience with git and knew
how to commit code and push branches to GitHub. git fundamentals however is a separate
one or two lessons by itself which we didn't do.</p>
<h2>Lesson 07 - Writing JUnit tests for Apache Commons</h2>
<p>Excluding the problems with Eclipse and the GitHub desktop client and
missing instructions for Windows the hardest part of this lesson was actually
selecting a component to work on, understanding what the code does and actually
writing meaningful tests. On top of that most students were not very proficient
programmers and Java was completely new to them.</p>
<p>Despite having 3 pull requests on GitHub I consider this lesson to be a failure.</p>
<h2>Lesson 08 - Integration Testing with Selenium</h2>
<p>This lesson starts with an example of what a flaky test it. At the moment I don't
think this lesson is the best place for that example. To make things even more difficult
the example is in Python (because that way was the easiest for me to write it) instead
of Java. Students had problems installing Python on Windows just to make this example work.
They also were lacking the knowledge how to execute a script in the terminal.</p>
<p>One of the students proposed a better flaky example utilizing dates and times and
executing it during various hours of the day. I have yet to code this and
prepare environment in which it would be executed. Btw recently I've seen similar behavior
caused by inconsistent timezone usage in Ruby which resulted in unexpected time offset
a little after midnight :).</p>
<p>Once again I have to point out that students came generally unprepared for the lesson
and haven't installed prerequisite software and programming languages. This is becoming
a trend and needs to be split out into a preparation session, possibly with a check
list.</p>
<p>On the Selenium side, starting with Selenium IDE, it was a bit unclear how to use it
and what needs to be done. This is another negative trend, where students were missing clear
instructions what they are expected to do. At the end we did resort to live demo
using Selenium IDE so they can at least get some idea about it.</p>
<h2>Lesson 09 and 10 - Writing Selenium tests for Mozilla Add-ons website</h2>
<p>IMO these two lessons are the biggest disaster of the entire course. Python &amp; virtualenv
on Windows was a total no go but on Linux things weren't much easier because students
had no idea what a virtualenv is.</p>
<p>Practice wise they haven't managed to read all the bugs on the Mozilla bug tracker and
had a very hard time selecting bugs to write tests for. Not to mention that many of the
reported bugs were administrative tasks to create or remove add-on categories. There
weren't many functional related bugs to write tests for.</p>
<p>The product under test was also hard to understand and most students were seeing it for the first
time, let alone getting to know the devel and testing environments that Mozilla provides.
Mozilla's test suite being in Python is just another issue to make contribution harder
because we've never actually studied Python.</p>
<p>Between the two lessons there were students who've missed the Selenium introduction
lesson and were having even harder time to figure things out. I didn't have the time
to explain and go back to the previous lesson for them.
Maybe an attendance policy is needed for dependent lessons.</p>
<p>Before the course started I've talked to some guys at Mozilla's IRC channel and
they agreed to help but at the end we didn't even engage with them. At this point
I'm skeptical that mentoring over IRC would have worked anyway.</p>
<h2>Lesson 11 - Introduction to Performance Testing</h2>
<p>This was a more theoretical lesson with less practical examples and exercises.
I have provided some blog posts of mine related to the topic of performance
testing but in general they are related to software that I've used which isn't
generally known to a less experienced audience (Celery, Twisted). These blog posts
IMO were hard to put into perspective and didn't serve a good purpose as examples.</p>
<p>The practical part of the lesson was a discussion with the goal of creating a
performance testing strategy for GitHub's infrastructure. It was me who was
doing most of the talking because students have no experience working on such
a large infrastructure like GitHub and didn't know what components might be
there, how they might be organized (load balancers, fail overs, etc)
and what needs to be tested.</p>
<p>There was also a more practical example to create a performance test in Java for
one of the classes found in <code>commons-codec/src/main/java/org/apache/commons/codec/digest</code>.
Again the main difficulty here was working fluently with Eclipse, getting the projects to
build/run and knowing how the software under test was supposed to work and be executed.</p>
<h2>Lesson 12 - How to find 1000 bugs in 30 minutes</h2>
<p>This was a more relaxed lesson with examples of simple types of bugs found on a
large scale. Most examples came from my blog and experiments I've made against
Fedora.</p>
<p>While amusing and fun I don't think all of the students understood me and kept
their attention. Part of that is because Fedora tends to focus on low level
stuff and my examples were not necessarily easy to understand.</p>
<h2>Feedback from the sponsor</h2>
<p>Experian Bulgaria was the exclusive sponsor for this course. At the end of the summer
Rado and I met with them to discuss the results of the training. Here's what they say</p>
<ul>
<li>Overall technical knowledge they consider to be weak. What they need are people
with basic knowledge in the field of object oriented programming, databases, operating
systems and networking. These are skills candidates need in order to work for Experian.
It must be noted that most of them were not taught at this course!</li>
<li>English language proficiency for all students is low on average;</li>
<li>User level experience with Linux was fine but students were missing deeper
knowledge about the operating system. Once again something we didn't teach;</li>
<li>The programming languages favored at Experian are Java and Ruby and students
had poor knowledge of them. They also had weak understanding of OOP principles;</li>
<li>According to Experian students were more ingrained with the development mindset
and were trying their luck in the QA profession instead of genuinely being interested
in the field. While this is generally the case I have to point out that as sponsor
Experian did a poor job at promoting their company and the QA profession
as a whole. What isn't known to the general public is their big in-house
QA community which could have served as a source of inspiration for the students!</li>
<li>Low motivation and lack of fundamental knowledge from university are other
traits interviewers at Experian have identified. They argued for a stronger
acceptance process and a requirement of minimum 2 years of university
education in computer science.</li>
</ul>
<p>On the topic of testing knowledge candidates did mostly OK, however
we don't have enough information about this. Also the hiring process at
Experian is more focused on the broader knowledge areas listed above so
substantial improvement in the testing knowledge of candidates doesn't given
them much head start.</p>
<p>While to my knowledge they didn't hire anyone few people
received an offer but declined due to various personal reasons. I view this
as poor performance on our side but Experian thinks otherwise and are willing
to sponsor another round of training.</p>
<h2>Summary</h2>
<p>Here is a list of all the things that could be improved</p>
<ul>
<li>Take time to develop the curriculum and have it pass QA review by other
experienced testers (in particular such that also teach students);</li>
<li>Make the application process harder to include people with broader IT knowledge;</li>
<li>Allow time to review all applicants;</li>
<li>Find a co-trainer and additional mentors for some of the exercises;</li>
<li>Minimize the number of students accepted in the course so we can handle everyone
with more care;</li>
<li>Give homework assignments and examine them before each lesson;</li>
<li>Collect and provide performance review at the end of the course;</li>
<li>Minimize the technology stack and tools used;</li>
<li>Host a technology preparation session at the beginning paired
with a check list of all the things that need to be done. Very likely
make the chosen technology stack a hard requirement to avoid setbacks
later in the schedule;</li>
<li>We need more fundamental lessons like practical git tutorial,
practical Linux tutorial, databases, networking, etc.</li>
<li>Same goes for Java or any other programming language. Each of these technologies
easily makes a course by itself. Possibly include technology skill assessment
in the application form and reject candidates who don't meet the minimum level.
This is to ensure the group is able to move at the same pace.</li>
<li>We need to improve the infrastructure used during the course,
especially exercise bug tracking and test case management systems;</li>
<li>Students need clear instructions for every exercise - what is required from them,
what the expected result is and what they need to be doing, etc;</li>
<li>Provide more examples for just about everything. Also make the examples
easier to understand/simple with the harder examples left for further
reading;</li>
<li>Provide easier projects to work on. This means applications whose domain
is easier to understand and closer to what experiences the students might have.
Also projects need clear instructions how to join and how to contribute with tests.</li>
<li>The same application/software needs to be used for manual bug finding,
unit test writing and integration test with Selenium. This will minimize
both context and technology switches and allow to view various testing
activities in the context of a single software under test;</li>
<li>While using open source projects for the above listed purposes sounds great
the reality is that they are hard to join and will not move according to our
schedule. Maybe less focus on open source per-se and more focus on a particular
application under test. Instructors can then proxy all the tests forward
to the upstream community;</li>
<li>Focus more on practice and less on exotic topics like performance testing
and large scale bug finding;</li>
<li>Seek more active participation from sponsors!</li>
</ul>
<p>If you have suggestions please comment below, especially if you can tell me
how to implement them in practice.</p>
<p>Thanks for reading and happy testing!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Thu 29 December 2016
            </p>
<p>There are <a href="http://atodorov.org/blog/2016/12/29/qa-automation-101-retrospective/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2016/12/28/4-quick-wins-to-manage-the-cost-of-software-testing/" rel="bookmark" title="Permalink to 4 Quick Wins to Manage the Cost of Software Testing">
                <h2 class="post-title">
                    4 Quick Wins to Manage the Cost of Software Testing
                </h2>
            </a>
                <div class="highlight"><pre>Every activity in software development has a cost and a value. Getting cost to
trend down while increasing value, is the ultimate goal.
</pre></div>


<p>This is the introduction of an e-book called
<a href="http://pi.qasymphony.com/four-quick-wins-thank-you">4 Quick Wins to Manage the Cost of Software Testing</a>.
It was sent to me by Ivan Fingarov couple of months ago. Just now I've managed to
read it and here's a quick summary. I urge everyone to download the original copy
and give it a read.</p>
<p>The paper focuses on several practices which organizations can apply immediately
in order to become more efficient and transparent in their software testing. While
larger organizations (e.g. enterprises) have most of these practices already in place
smaller companies (up to 50-100 engineering staff) may not be familiar with them
and will reap the most benefits of implementing said practices. Even though I work
for a large enterprise I find this guide useful when considered at the individual
team level!</p>
<p>The first chapter focuses on <em>Tactics</em> to minimize cost: Process, Tools, Bug System Mining
and Eliminating Handoffs.</p>
<p>In <em>Process</em> the goal is to minimize the burden of documenting the test process
(aka testing artifacts), allow for better transparency and visibility outside the QA group
and streamline the decision making process of what to test and when to stop testing,
how much has been tested, what the risk is, ect. The authors propose testing core functionality
paired with emerging risk areas based on new features development. They propose making
a list of these and sorting that list by perceived risk/priority and testing as much
as possible. Indeed this is very similar to the method I've used at Red Hat when designing
testing for new features and new major releases of Red Hat Enterprise Linux. A similar
method I've seen in place at several start-ups as well, although in the small organization
the primary driver for this method is lack of sufficient test resources.</p>
<p><em>Tools</em> proposes the use of test case management systems to ease the documentation burden.
I've used <a href="http://testlink.org/">TestLink</a> and <a href="https://github.com/Nitrate/Nitrate">Nitrate</a>.
From them Nitrate has more features but is currently unmaintained with me being the
largest contributor on GitHub. From the paid variants I've used
<a href="https://www.polarion.com/products/qa/test-automation">Polarion</a> which I generally dislike.
Polarion is most suitable for large organizations because it gives lots of opportunities
for tracking and reporting. For small organizations it is an overkill.</p>
<p><em>Bug System Mining</em> is a technique which involves regularly scanning the bug tracker
and searching for patterns. This is useful for finding bug types which appear
frequently and generally point to a flaw in the software development process. The fix for these
flaws usually is a change in policy/workflow which eliminates the source of the errors.
I'm a fan of this technique when joining an existing project and need to assess
what the current state is. I've done this when consulting for a few start-ups, including
<a href="http://meet.jit.si">Jitsi Meet</a> (acquired by Atlassian), however I'm not doing bug mining
on a regular basis which I consider a drawback and I really should start doing!</p>
<p>For example at one project I found lots of bugs reported against translations, e.g.
missing translations, text overflowing the visible screen area or not playing well with
existing design, chosen language/style not fitting well with the product domain, etc.</p>
<p>The root cause of the problem was how the software in question has been localized.
The translators were given a file of English strings, which they would translate and
return back in an spread sheet. Developers would copy&amp;paste the translated strings
into localization files and integrate with the software. Then QA would usually
inspect all the pages and report the above issues. The solution was to remove devel
and QA from the translation process, implement a translation management system together
with live preview (web based) so that translators can keep track of what is left to
translate and can visually inspect their work immediately after a string was translated.
Thus translators are given more context for their work but also given the responsibility
to produce good quality translations.</p>
<p>Another example I've seen are many bugs which seem like a follow up/nice to have features
of partially implemented functionality. The root cause of this problem turned out to be
that devel was jumping straight to implementation without taking the time to brainstorm
and consult with QE and product owners, not taking into account corner cases and minor
issues which would have easily been raised by skillful testers. This process lead to
several iterations until the said functionality was considered initially implemented.</p>
<p><em>Eliminating Handoffs</em> proposes the use of cross-functional teams to reduce idle time
and reduce the back-and-forth communication which happens when a bug is found, reported,
evaluated and considered for a fix, fixed by devel and finally deployed for testing.
This method argues that including testers early in the process and pairing them with
the devel team will produce faster bug fixes and reduce communication burden.</p>
<p>While I generally agree with that statement it's worth noting that cross-functional
teams perform really well when all team members have relatively equal skill level
on the horizontal scale and strong experience on the vertical scale (think T-shaped specialist).
Cross-functional teams
don't work well when you have developers who aren't well versed in the testing domain
and/or testers who are not well versed in programming or the broader OS/computer science
fundamentals domain. In my opinion you need well experienced engineers for a good cross-functional
team.</p>
<p>In the chapter <em>Collaboration</em> the paper focuses on pairing, building the right thing
and faster feedback loops for developers. This overlaps with earlier proposals for
cross-functional teams and QA bringing value by asking the "what if" questions.
The chapter specifically talks about the Three Amigos meeting between PM, devel and QA
where they discuss a feature proposal from all angles and finally come to a conclusion
what the feature should look like. I'm a strong supporter of this technique and have
been working with it under one form or another during my entire career. This also
touches on the notion that testers need to move into the <em>Quality Assistance</em> business
and be proactive during the software development process, which is something I'm
hoping to talk about at the
<a href="http://www.romaniatesting.ro/">Romanian Testing Conference</a> next year!</p>
<p>Finally the book talks about <em>Skills Development</em> and makes the distinction between
Centers of Excellence (CoE) and Communities of Practice (CoP). Both the book and I
are supporters of the CoP approach. This is a bottoms-up approach which is open for
everyone to join in and harnesses the team creative abilities. It also takes into
account that different teams use different methods and tools and that
"one size doesn't fit all"!</p>
<div class="highlight"><pre>Skilled teams find important bugs faster, discover innovative solutions to hard
testing problems and know how to communicate their value. Sometimes, a few super
testers can replace an army of average testers.
</pre></div>


<p>While I consider myself to be a "super tester" with thousands of bugs reported there
is a very important note to make here. Communities of Practice are successful when
their members are self-focused on skill development! In my view and to some
extent the communities I've worked with everyone should strive to constantly improve
their skills but also exercise peer pressure on their co-workers to not fall behind.
This has been confirmed by other folks in the QA industry and I've heard it many times
when talking to friends from other companies.</p>
<p>Thanks for reading and happy testing!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Wed 28 December 2016
            </p>
<p>There are <a href="http://atodorov.org/blog/2016/12/28/4-quick-wins-to-manage-the-cost-of-software-testing/#disqus_thread">comments</a>.</p>        </div>
        <div class="post-preview">
            <a href="http://atodorov.org/blog/2016/12/27/mutation-testing-vs-coverage/" rel="bookmark" title="Permalink to Mutation Testing vs. Coverage">
                <h2 class="post-title">
                    Mutation Testing vs. Coverage
                </h2>
            </a>
                <p>At GTAC 2015 Laura Inozemtseva made a lightning talk titled
<a href="https://www.youtube.com/watch?v=sAfROROGujU&amp;list=PLFjlI7p-h1hxBP3cIjEqePSeoBDHud5Db&amp;index=47">Coverage is Not Strongly Correlated with Test Suite Effectiveness</a>
which is the single event that got me hooked up with mutation testing.
This year, at GTAC 2016, Rahul Gopinath made a counter argument
with his lightning talk
<a href="https://www.youtube.com/watch?v=NKEptA3KP08&amp;list=PLFjlI7p-h1hxBP3cIjEqePSeoBDHud5Db&amp;index=1">Code Coverage is a Strong Predictor of Test Suite Effectiveness</a>.
So which one is better ? I urge you to watch both talks and take notes before
reading about my practical experiment and other opinions on the topic!</p>
<p><strong>DISCLAIMER:</strong> I'm a heavy contributor to Cosmic-Ray, the mutation testing tool for Python
so my view is biased!</p>
<p>Both Laura and Rahul (you will too) agree that a test suite effectiveness depends
on the strength of its oracles. In other words the assertions you make in your tests.
This is what makes a test suite good and determines its ability to detect bugs when
present. I've decided to use <a href="https://github.com/MrSenko/pelican-ab">pelican-ab</a> as
a practical example. pelican-ab is a plugin for Pelican, the static site generator
for Python. It allows you to generate A/B experiments by writing out the content
into different directories and adjusting URL paths based on the experiment name.</p>
<h2>Can 100% code coverage detect bugs</h2>
<p>Absolutely <strong>NOT</strong>! In version 0.2.1,
<a href="https://github.com/MrSenko/pelican-ab/commit/ef1e2117ad8ffa5b9fa8470d32d54a36ccb140bb">commit ef1e211</a>,
pelican-ab had the following bug:</p>
<div class="highlight"><pre><span class="n">Given</span><span class="o">:</span> <span class="n">Pelican</span><span class="err">&#39;</span><span class="n">s</span> <span class="n">DELETE_OUTPUT_DIRECTORY</span> <span class="k">is</span> <span class="kd">set</span> <span class="n">to</span> <span class="n">True</span> <span class="o">(</span><span class="n">which</span> <span class="n">it</span> <span class="k">is</span> <span class="n">by</span> <span class="k">default</span><span class="o">)</span>
<span class="n">When</span><span class="o">:</span> <span class="n">we</span> <span class="n">generate</span> <span class="n">several</span> <span class="n">experiments</span> <span class="n">using</span> <span class="n">the</span> <span class="n">commands</span><span class="o">:</span>
    <span class="n">AB_EXPERIMENT</span><span class="o">=</span><span class="s2">&quot;control&quot;</span> <span class="n">make</span> <span class="n">regenerate</span>
    <span class="n">AB_EXPERIMENT</span><span class="o">=</span><span class="s2">&quot;123&quot;</span> <span class="n">make</span> <span class="n">regenerate</span>
    <span class="n">AB_EXPERIMENT</span><span class="o">=</span><span class="s2">&quot;xy&quot;</span> <span class="n">make</span> <span class="n">regenerate</span>
    <span class="n">make</span> <span class="n">publish</span>
<span class="n">Actual</span> <span class="n">result</span><span class="o">:</span> <span class="n">only</span> <span class="n">the</span> <span class="s2">&quot;xy&quot;</span> <span class="n">experiment</span> <span class="o">(</span><span class="n">the</span> <span class="n">last</span> <span class="n">one</span><span class="o">)</span> <span class="n">would</span> <span class="n">be</span> <span class="n">published</span> <span class="n">online</span><span class="o">.</span>
<span class="n">And</span><span class="o">:</span> <span class="n">all</span> <span class="n">of</span> <span class="n">the</span> <span class="n">other</span> <span class="n">contents</span> <span class="n">will</span> <span class="n">be</span> <span class="n">deleted</span><span class="o">.</span>

<span class="n">Expected</span> <span class="n">result</span><span class="o">:</span> <span class="n">content</span> <span class="n">from</span> <span class="n">all</span> <span class="n">experiments</span> <span class="n">will</span> <span class="n">be</span> <span class="n">available</span> <span class="n">under</span> <span class="n">the</span> <span class="n">output</span> <span class="n">directory</span><span class="o">.</span>
</pre></div>


<p>This is because before each invocation Pelican deletes the output directory and
re-creates the entire content structure. The bug was not caught regardless of
having 100% line + branch coverage. See
<a href="https://travis-ci.org/MrSenko/pelican-ab/builds/129514715">Build #10</a> for more
info.</p>
<h2>Can 100% mutation coverage detect bugs</h2>
<p>So I've branched off since commit ef1e211 into the
<a href="https://github.com/MrSenko/pelican-ab/commits/mutation_testing_vs_coverage_experiment">mutation_testing_vs_coverage_experiment branch</a>
(requires Pelican==3.6.3).</p>
<p>After initial execution of Cosmic Ray I have 2 mutants left:</p>
<div class="highlight"><pre>$ cosmic-ray run --baseline=10 --test-runner=unittest example.json pelican_ab -- tests/
$ cosmic-ray report example.json 
job ID 29:Outcome.SURVIVED:pelican_ab
command: cosmic-ray worker pelican_ab mutate_comparison_operator 3 unittest -- tests/
<span class="gd">--- mutation diff ---</span>
<span class="gd">--- a/home/senko/pelican-ab/pelican_ab/__init__.py</span>
<span class="gi">+++ b/home/senko/pelican-ab/pelican_ab/__init__.py</span>
<span class="gu">@@ -14,7 +14,7 @@</span>
     def __init__(self, output_path, settings=None):
         super(self.__class__, self).__init__(output_path, settings)
         experiment = os.environ.get(jinja_ab._ENV, jinja_ab._ENV_DEFAULT)
<span class="gd">-        if (experiment != jinja_ab._ENV_DEFAULT):</span>
<span class="gi">+        if (experiment &gt; jinja_ab._ENV_DEFAULT):</span>
             self.output_path = os.path.join(self.output_path, experiment)
             Content.url = property((lambda s: ((experiment + &#39;/&#39;) + _orig_content_url.fget(s))))
             URLWrapper.url = property((lambda s: ((experiment + &#39;/&#39;) + _orig_urlwrapper_url.fget(s))))

job ID 33:Outcome.SURVIVED:pelican_ab
command: cosmic-ray worker pelican_ab mutate_comparison_operator 7 unittest -- tests/
<span class="gd">--- mutation diff ---</span>
<span class="gd">--- a/home/senko/pelican-ab/pelican_ab/__init__.py</span>
<span class="gi">+++ b/home/senko/pelican-ab/pelican_ab/__init__.py</span>
<span class="gu">@@ -14,7 +14,7 @@</span>
     def __init__(self, output_path, settings=None):
         super(self.__class__, self).__init__(output_path, settings)
         experiment = os.environ.get(jinja_ab._ENV, jinja_ab._ENV_DEFAULT)
<span class="gd">-        if (experiment != jinja_ab._ENV_DEFAULT):</span>
<span class="gi">+        if (experiment not in jinja_ab._ENV_DEFAULT):</span>
             self.output_path = os.path.join(self.output_path, experiment)
             Content.url = property((lambda s: ((experiment + &#39;/&#39;) + _orig_content_url.fget(s))))
             URLWrapper.url = property((lambda s: ((experiment + &#39;/&#39;) + _orig_urlwrapper_url.fget(s))))

total jobs: 33
complete: 33 (100.00%)
survival rate: 6.06%
</pre></div>


<p>The last one, job 33 is equivalent mutation. The first one, job 29 is killed by the test
added in
<a href="https://github.com/MrSenko/pelican-ab/commit/b8bff85eeca6c18fbf62cac55fd1a0d64295c43c">commit b8bff85</a>.
For all practical purposes we now have 100% code coverage and 100% mutation coverage.
The bug described above still exists thought.</p>
<h2>How can we detect the bug</h2>
<p>The bug isn't detected by any test because
we don't have tests designed to perform and validate the exact same steps that a
physical person will execute when using pelican-ab. Such test is added in
<a href="https://github.com/MrSenko/pelican-ab/commit/ca85bd042d783f2f6551ae17f16b29aa3750711b">commit ca85bd0</a>
and you can see that it causes
<a href="https://travis-ci.org/MrSenko/pelican-ab/builds/186510356">Build #22</a> to fail.</p>
<p>Experiment with setting <code>DELETE_OUTPUT_DIRECTORY=False</code> in <code>tests/pelicanconf.py</code> and
the test will PASS!</p>
<h2>Is pelican-ab bug free</h2>
<p>Not of course. Even after 100% code and mutation coverage and after manually constructing
a test which mimics user behavior there is at least one more bug present. There
is a pylint <code>bad-super-call</code> error, fixed in
<a href="https://github.com/MrSenko/pelican-ab/commit/193e3db9e7d021e11b54f54ac8b8718651c633c8">commit 193e3db</a>.
For more information about the error see
<a href="http://mrsenko.com/blog/atodorov/2016/09/14/beware-of-recursion-loop-when-using-super/">this blog post</a>.</p>
<h2>Other bugs found</h2>
<p>During my humble experience with mutation testing so far I've added
<a href="https://github.com/rhinstaller/pykickstart/pulls/atodorov">quite a few new tests</a>
and discovered two bugs which went unnoticed for years. The first one is
constructor parameter not passed to parent constructor,
see
<a href="https://github.com/rhinstaller/pykickstart/pull/96/files#diff-f4294048719aeac4da7a86eee0fbdfd3">PR#96, pykickstart/commands/authconfig.py</a></p>
<div class="highlight"><pre>     def __init__(self, writePriority=0, *args, **kwargs):
<span class="gd">-        KickstartCommand.__init__(self, *args, **kwargs)</span>
<span class="gi">+        KickstartCommand.__init__(self, writePriority, *args, **kwargs)</span>
         self.authconfig = kwargs.get(&quot;authconfig&quot;, &quot;&quot;)
</pre></div>


<p>The second bug is parameter being passed to parent class constructor,
but the parent class doesn't care about this parameter. For example
<a href="https://github.com/rhinstaller/pykickstart/pull/96/files#diff-7d2833450b6e1c9a94eb90e7f171ff52">PR#96, pykickstart/commands/driverdisk.py</a></p>
<div class="highlight"><pre><span class="gd">-    def __init__(self, writePriority=0, *args, **kwargs):</span>
<span class="gd">-        BaseData.__init__(self, writePriority, *args, **kwargs)</span>
<span class="gi">+    def __init__(self, *args, **kwargs):</span>
<span class="gi">+        BaseData.__init__(self, *args, **kwargs)</span>
</pre></div>


<p>Also note that pykickstart has nearly 100% test coverage as a whole
and the affected files were 100% covered as well.</p>
<p>The bugs above don't seem like a big deal and when considered out of context
are relatively minor. However pykickstart's biggest client is anaconda, the Fedora
and Red Hat Enterprise Linux installation program. Anaconda uses pykickstart to
parse and generate text files (called kickstart files) which contain information
for driving the installation in a fully automated manner. This is used by everyone
who installs Linux on a large scale and is pretty important functionality!</p>
<p><code>writePriority</code> controls the order of which individual commands are written to file
at the end of the installation. In rare cases the order of commands may depend
on each other. Now imagine the bugs above produce a disordered kickstart file,
which a system administrator thinks should work but it doesn't. It may be the case
this administrator is trying to provision hundreds of Linux systems to bootstrap
a new data center or maybe performing disaster recovery. You get the scale of
the problem now, don't you?</p>
<p>To be honest I've seen bugs of this nature but not in the last several years.</p>
<p>This is all to say a minor change like this may have
an unexpectedly big impact somewhere down the line.</p>
<h2>Conclusion</h2>
<p>With respect to the above findings and my bias I'll say the following:</p>
<ul>
<li>Neither 100% coverage, nor 100% mutation coverage are a silver bullet against bugs;</li>
<li>100% mutation coverage appears to be better than 100% code coverage in practice;</li>
<li>Mutation testing clearly shows out pieces of code which need refactoring
  which in turn minimizes the number of possible mutations;</li>
<li>Mutation testing causes you to write more asserts and construct more detailed tests
  which is always a good thing when testing software;</li>
<li>You can't replace humans designing test cases just yet but can give them tools
  to allow them to write more and better tests;</li>
<li>You should not rely on a single tool (or two of them) because tools are only
  able to find bugs they were designed for!</li>
</ul>
<h2>Bonus: What others think</h2>
<p>As a bonus to this article let me share a transcript from the
<a href="https://mutation-testing.slack.com">mutation-testing.slack.com</a> community:</p>
<div class="highlight"><pre>atodorov 2:28 PM
Hello everyone, I&#39;d like to kick-off a discussion / interested in what you think about
Rahul Gopinath&#39;s talk at GTAC this year. What he argues is that test coverage is still
the best metric for how good a test suite is and that mutation coverage doesn&#39;t add much
additional value. His talk is basically the opposite of what @lminozem presented last year
at GTAC. Obviously the community here and especially tools authors will have an opinion on
these two presentations.

tjchambers 12:37 AM
@atodorov I have had the &quot;pleasure&quot; of working on a couple projects lately that illustrate
why LOC test coverage is a misnomer. I am a **strong** proponent of mutation testing so will
declare my bias.

The projects I have worked on have had a mix of test coverage - one about 50% and
another &gt; 90%.

In both cases however there was a significant difference IMO relative to mutation coverage
(which I have more faith in as representative of true tested code).

Critical factors I see when I look at the difference:

- Line length: in both projects the line lengths FAR exceeded visible line lengths that are
&quot;acceptable&quot;. Many LONGER lines had inline conditionals at the end, or had ternary operators
and therefore were in fact only 50% or not at all covered, but were &quot;traversed&quot;

- Code Conviction (my term): Most of the code in these projects (Rails applications) had
significant Hash references all of which were declared in &quot;traditional&quot; format hhh[:symbol].
So it was nearly impossible for the code in execution to confirm the expectation of the
existence of a hash entry as would be the case with stronger code such as &quot;hhh.fetch(:symbol)&quot;

- Instance variables abound: As with most of Rails code the number of instance variables
in a controller are extreme. This pattern of reference leaked into all other code as well,
making it nearly impossible with the complex code flow to ascertain proper reference
patterns that ensured the use of the instance variables, so there were numerous cases
of instance variable typos that went unnoticed for years. (edited)

- .save and .update: yes again a Rails issue, but use of these &quot;weak&quot; operations showed again
that although they were traversed, in many cases those method references could be removed
during mutation and the tests would still pass - a clear indication that save or update was
silently failing.

I could go on and on, but the mere traversal of a line of code in Ruby is far from an indication
of anything more than it may be &quot;typed in correctly&quot;.

@atodorov Hope that helps.

LOC test coverage is a place to begin - NOT a place to end.

atodorov 1:01 AM
@tjchambers: thanks for your answer. It&#39;s too late for me here to read it carefully but
I&#39;ll do it tomorrow and ping you back

dkubb 1:13 AM
As a practice mutation testing is less widely used. The tooling is still maturing. Depending on your
language and environment you might have widely different experiences with mutation testing

I have not watched the video, but it is conceivable that someone could try out mutation testing tools
for their language and conclude it doesnt add very much

mbj 1:14 AM
Yeah, I recall talking with @lminozem here and we identified that the tools she used likely
show high rates of false positives / false coverage (as the tools likely do not protect against
certain types of integration errors)

dkubb 1:15 AM
IME, having done TDD for about 15+ years or so, and mutation testing for about 6 years, I think
when it is done well it can be far superior to using line coverage as a measurement of test quality

mbj 1:16 AM
Any talk pro/against mutation testing must, as the tool basis is not very homogeneous, show a non consistent result.

dkubb 1:16 AM
Like @tjchambers says though, if you have really poor line coverage youre not going to
get as much of a benefit from mutation testing, since its going to be telling you what
you already know  that your project is poorly tested and lots of code is never exercised

mbj 1:19 AM
Thats a good and likely the core point. I consider that mutation testing only makes sense
when aiming for 100% (and this is to my experience not impractical).

tjchambers 1:20 AM
I don&#39;t discount the fact that tool quality in any endeavor can bring pro/con judgements
based on particular outcomes

dkubb 1:20 AM
What is really interesting for people is to get to 100% line coverage, and then try mutation
testing. You think youve done a good job, but I guarantee mutation testing will find dozens
if not hundreds of untested cases .. even in something with 100% line coverage

To properly evaluate mutation testing, I think this process is required, because you cant
truly understand how little line coverage gives you in comparison

tjchambers 1:22 AM
But I don&#39;t need a tool to tell me that a 250 character line of conditional code that by
itself would be an oversized method AND counts more because there are fewer lines in the
overall percentage contributes to a very foggy sense of coverage.

dkubb 1:22 AM
It would not be unusual for something with 100% line coverage to undergo mutation testing
and actually find out that the tests only kill 60-70% of possible mutations

tjchambers 1:22 AM
@dkubb or less

dkubb 1:23 AM
usually much less :stuck_out_tongue:

it can be really humbling

mbj 1:23 AM
In this discussion you miss that many test suites (unless you have noop detection):
Will show false coverage.

tjchambers 1:23 AM
When I started with mutant on my own project which I developed I had 95% LOC coverage

mbj 1:23 AM
Test suites need to be fixed to comply to mutation testing invariants.

tjchambers 1:23 AM
I had 34% mutation coverage

And that was ignoring the 5% that wasn&#39;t covered at all

mbj 1:24 AM
Also if the tool you compare MT with line coverage on: Is not very strong,
the improvement may not be visible.

dkubb 1:24 AM
another nice benefit is that you will become much better at enumerating all
the things you need to do when writing tests

tjchambers 1:24 AM
@dkubb or better yet - when writing code.

The way I look at it - the fewer the alive mutations the better the test,
the fewer the mutations the better the code.

dkubb 1:29 AM
yeah, you can infer a kind of cyclomatic complexity by looking at how many mutations there are

tjchambers 1:31 AM
Even without tests (not recommended) you can judge a lot from the mutations themselves.

I still am an advocate for mutations/LOC metric
</pre></div>


<p>As you can see members in the community are strong supporters of mutation testing, all of them
having much more experience than I do.</p>
<p>Please help me collect more practical examples! My goal is to
collect enough information and present the findings at
<a href="https://developers.google.com/google-test-automation-conference/">GTAC 2017</a>
which will be held in London.</p>
<p><strong>UPDATE:</strong> I have written
<a href="http://atodorov.org/blog/2017/04/05/mutation-testing-vs-coverage-pt-2/">Mutation testing vs. coverage, Pt.2</a>
with another example.</p>
<p>Thanks for reading and happy testing!</p>
            <p class="post-meta">Posted by
                    <a href="http://atodorov.org/author/alexander-todorov.html">Alexander Todorov</a>
                 on Tue 27 December 2016
            </p>
<p>There are <a href="http://atodorov.org/blog/2016/12/27/mutation-testing-vs-coverage/#disqus_thread">comments</a>.</p>        </div>

    <hr>
    <!-- Pager -->
    <ul class="pager">
        <li class="next">
                <a href="http://atodorov.org/blog/categories/qa/index2.html">Older Posts &rarr;</a>
        </li>
    </ul>
    Page 1 / 6
    <hr>
            </div>
        </div>
    </div>

    <hr>

    <!-- Footer -->
    <footer>
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <ul class="list-inline text-center">
                            <li>
                                <a href="https://twitter.com/atodorov_">
                                    <span class="fa-stack fa-lg">
                                        <i class="fa fa-circle fa-stack-2x"></i>
                                        <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li>
                                <a href="https://github.com/atodorov">
                                    <span class="fa-stack fa-lg">
                                        <i class="fa fa-circle fa-stack-2x"></i>
                                        <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li>
                                <a href="https://bg.linkedin.com/in/alextodorov">
                                    <span class="fa-stack fa-lg">
                                        <i class="fa fa-circle fa-stack-2x"></i>
                                        <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li>
                                <a href="http://feeds.feedburner.com/atodorov">
                                    <span class="fa-stack fa-lg">
                                        <i class="fa fa-circle fa-stack-2x"></i>
                                        <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li>
                                <a href="https://www.youtube.com/playlist?list=PLFjlI7p-h1hxBP3cIjEqePSeoBDHud5Db">
                                    <span class="fa-stack fa-lg">
                                        <i class="fa fa-circle fa-stack-2x"></i>
                                        <i class="fa fa-youtube fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li>
                                <a href="http://amzn.to/1ivu2q4">
                                    <span class="fa-stack fa-lg">
                                        <i class="fa fa-circle fa-stack-2x"></i>
                                        <i class="fa fa-amazon fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li>
                                <a href="http://mrsenko.com/?utm_source=atodorov.org&utm_medium=blog&utm_campaign=social_icon">
                                    <span class="fa-stack fa-lg">
                                        <i class="fa fa-circle fa-stack-2x"></i>
                                        <i class="fa fa-user-secret fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                    </ul>
<section>
    <p>
        I am a Senior QA contractor at Red Hat responsible for finding over
        <a href="/blog/2014/02/19/7-years-1400-bugs-red-hat-qa/">1600 bugs</a>,
        a general purpose open source developer, Red Hat Certified professional,
        cloud hacker and an entrepreneur! My latest start-up is
        <a href="http://mrsenko.com/?utm_source=atodorov.org&utm_medium=blog&utm_campaign=footer">Mr. Senko</a>!
    </p>
    <p>
        I am living in the <a href="http://planet.sofiavalley.com">Sofia Valley</a>
        which is emerging as a busy place for start-up founders and tech enthusiasts
        in Eastern Europe! You can find more about me <a href="/blog/2013/01/25/about-me/">here</a>.
    </p>
    <p>
        <small>
            <em>
                Some of the links contained within this site have my referral id (e.g.,
                <a target="_blank" href="http://www.amazon.com/ref=as_li_ss_tl?_encoding=UTF8&camp=1789&creative=390957&linkCode=ur2&tag=atodorovorg-20&linkId=L6Q34XAXQS5RDMOY">Amazon</a><img src="https://ir-na.amazon-adsystem.com/e/ir?t=atodorovorg-20&l=ur2&o=1" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />),
                which provides me with a small commission for each sale. Thank you for your support.
            </em>
        </small>
    </p>
</section>

<form action="http://google.com/search" method="get" style="width:300px;margin:0 auto;">
    <fieldset role="search">
        <input type="hidden" name="sitesearch" value="http://atodorov.org" />
        <input class="search" type="text" name="q" placeholder="Search" style="width:100%"/>
    </fieldset>
</form>

<p class="copyright text-muted">
    <a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/deed.en_US">CC-BY-SA</a> &amp;
    <a rel="license" href="http://opensource.org/licenses/MIT">MIT</a>
    2011-2016 &diams; Alexander Todorov &diams;
    <a href="http://planet.sofiavalley.com">SofiaValley Blog</a>
</p>

<script type='text/javascript'>
window.__lo_site_id = 55936;
    (function() {
        var wa = document.createElement('script'); wa.type = 'text/javascript'; wa.async = true;
        wa.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://cdn') + '.luckyorange.com/w.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(wa, s);
      })();
</script>
                </div>
            </div>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="http://atodorov.org/theme/js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="http://atodorov.org/theme/js/bootstrap.min.js"></script>


    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-37979549-1']);
    _gaq.push(['_trackPageview']);
    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
    </script>
<script type="text/javascript">
    var disqus_shortname = 'atodorov';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
</body>

</html>