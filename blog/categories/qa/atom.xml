<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: QA | atodorov.org - you can logoff, but you can never leave]]></title>
  <link href="http://atodorov.org/blog/categories/qa/atom.xml" rel="self"/>
  <link href="http://atodorov.org/"/>
  <updated>2014-09-30T11:20:34+03:00</updated>
  <id>http://atodorov.org/</id>
  <author>
    <name><![CDATA[Alexander Todorov]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[SNAKE is no Longer Needed to Run Installation Tests in Beaker]]></title>
    <link href="http://atodorov.org/blog/2014/07/18/snake-is-no-longer-needed-to-run-installation-tests-in-beaker/"/>
    <updated>2014-07-18T23:05:00+03:00</updated>
    <id>http://atodorov.org/blog/2014/07/18/snake-is-no-longer-needed-to-run-installation-tests-in-beaker</id>
    <content type="html"><![CDATA[<p>This is a quick status update for one of the pieces of
<a href="/blog/2013/11/19/open-source-quality-assurance-infrastructure-for-fedora-qa/">Fedora QA infrastructure</a>
and mostly a self-note.</p>

<p>Previously to control the kickstart configuration used during installation in Beaker one
had to either modify the job XML in Beaker or use SNAKE (<code>bkr workflow-snake</code>) to render
a kickstart configuration from a Python template.</p>

<p>SNAKE presented challenges when deploying and using
<a href="https://beaker.fedoraproject.org">beaker.fedoraproject.org</a> and is
virtually unmaintained.</p>

<p>I present the new <code>bkr workflow-installer-test</code> which uses Jinja2 templates to
generate a kickstart configuration when provisioning the system. This is already
available in beaker-client-0.17.1.</p>

<p>The templates make use of all Jinja2 features (as far as I can tell) so you can create
very complex ones. You can even include snippets from one template into another if required.
The standard context that is passed to the template is:</p>

<ul>
<li><strong>DISTRO</strong> - if specified, the distro name</li>
<li><strong>FAMILY</strong> - as returned by Beaker server, e.g. <em>RedHatEnterpriseLinux6</em></li>
<li><strong>OS_MAJOR</strong> and <strong>OS_MINOR</strong> - also taken from Beaker server. e.g. OS_MAJOR=6 and OS_MINOR=5 for RHEL 6.5</li>
<li><strong>VARIANT</strong> - if specified</li>
<li><strong>ARCH</strong> - CPU architecture like x86_64</li>
<li>any parameters passed to the test job with <code>--taskparam</code>. They are processed last and can override previous values.</li>
</ul>


<p>Installation related tests at <a href="https://bitbucket.org/fedoraqa/fedora-beaker-tests">fedora-beaker-tests</a>
have been updated with a <code>ks.cfg.tmpl</code> templates to use with this new workflow.</p>

<p>This workflow also has the ability to return boot arguments for the installer if needed.
If any, they should be defined in a {% block kernel_options %}{% endblock %}
block inside the template. A simpler variant is to define a comment line that stars with
<em>## kernel_options:</em></p>

<p>There are still a few issues which need to be fixed before beaker.fedoraproject.org
can be used by the general public though. I will be writing another post about that
so stay tuned.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How Do You Test Thai Scalable Fonts]]></title>
    <link href="http://atodorov.org/blog/2014/03/17/how-do-you-test-thai-scalable-fonts/"/>
    <updated>2014-03-17T22:51:00+02:00</updated>
    <id>http://atodorov.org/blog/2014/03/17/how-do-you-test-thai-scalable-fonts</id>
    <content type="html"><![CDATA[<p>Recently I wrote about <a href="/blog/2014/03/04/how-do-you-test-fonts/">testing fonts</a>.
I finally managed to get an answer from the authors of <em>thai-scalable-fonts</em>.</p>

<p><blockquote><p></p></p><p><blockquote><ul><br/><li>What is your approach for testing Fonts-TLWG?</li><br/></ul><br/></blockquote></p><p><p>It's not automated test. What it does is generate PDF with sample<br/>texts at several sizes (the waterfall), pangrams, and glyph table.<br/>It needs human eyes to investigate.</p></p><p><blockquote><ul><br/><li>What kind of problems is your test suite designed for ?</li><br/></ul><br/></blockquote></p><p><ul><br/><li>Shaping</li><br/><li>Glyph coverage</li><br/><li>Metrics</li><br/></ul></p><p><br/><p>We also make use of fontforge features to make spotting errors<br/>easier, such as<br/>- Show extremas<br/>- Show almost vertical/horizontal lines/curves</p><footer><strong>Theppitak Karoonboonyanan</strong> <cite>Fonts-TLWG</cite></footer></blockquote></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How do You Test Fonts]]></title>
    <link href="http://atodorov.org/blog/2014/03/04/how-do-you-test-fonts/"/>
    <updated>2014-03-04T21:30:00+02:00</updated>
    <id>http://atodorov.org/blog/2014/03/04/how-do-you-test-fonts</id>
    <content type="html"><![CDATA[<p><a href="/blog/2014/03/03/last-week-in-fedora-qa/">Previously</a> I mentioned about testing
fonts but didn't have any idea how this is done. Authors
Khaled Hosny of <a href="http://www.amirifont.org/">Amiri Font</a> and Steve White of
<a href="http://www.gnu.org/software/freefont/">GNU FreeFont</a> provided valuable insight
and material for further reading. I've asked them:</p>

<ul>
<li>What is your approach for testing ?</li>
<li>What kind of problems is your test suite designed for ?</li>
</ul>


<p>Here's what they say:</p>

<p><blockquote><p>Currently my test suite consists of text strings (or lists of code<br/>points) and expected output glyph sequences and then use HarfBuzz<br/>(through its hb-shape command line tool) to check that the fonts always<br/>output the expected sequence of glyphs, sometimes with the expected<br/>positioning as well. Amiri is a complex font that have many glyph<br/>substitution and positioning rules, so the test suite is designed to<br/>make sure those rules are always executed correctly to catch regressions<br/>in the font (or in HarfBuzz, which sometimes happens since the things I<br/>do in my fonts are not always that common).</p></p><p><p>I think Lohit project do similar testing for their fonts, and HarfBuzz<br/>itself has a similar test suite with a bunch of nice scripts (though<br/>they are not installed when building HarfBuzz, yet[1]).</p></p><p><p>Recently I added more kinds of tests, namely checking that OTS[2]<br/>sanitizes the fonts successfully as this is important for using them on<br/>the web, and a test for a common mistakes I made in my feature files<br/>that result in unexpected blank glyphs in the fonts.</p><footer><strong>Khaled Hosny</strong> <cite>Amiri Font</cite></footer></blockquote></p>

<ol>
<li><a href="https://github.com/behdad/harfbuzz/pull/12">https://github.com/behdad/harfbuzz/pull/12</a></li>
<li><a href="https://github.com/khaledhosny/ots">https://github.com/khaledhosny/ots</a></li>
</ol>


<p><blockquote><p>The answer is complicated.  I'll do what I can to answer.</p></p><p><p>First, the FontForge application has a "verification" function which<br/>can be run from a script, and which identifies numerous technical<br/>problems.</p></p><p><p>FontForge also has a "Find Problems" function that I run by hand.</p></p><p><p>The monospaced face has special restrictions, first that all glyphs of<br/>non-zero width must be of the same width, and second, that all glyphs<br/>lie within the vertical bounds of the font.</p></p><p><p>Beside this, I have several other scripts that check for a few things<br/>that FontForge doesn't (duplicate names, that glyph slots agree with<br/>Unicode code within Unicode character blocks).</p></p><p><p>Several tests scripts have yet to be uploaded to the version control<br/>system -- because I'm unsure of them.</p></p><p><p>There is a more complicated check of TrueType tables, which attempts<br/>to find cases of tables that have been "shadowed" by the<br/>script/language specification of another table.  This is helpful, but<br/>works imperfectly.</p></p><p><p>ALL THAT SAID,</p></p><p><p>In the end, every script used in the font has to be visually checked.<br/>This process takes me weeks, and there's nothing systematic about it,<br/>except that I look at printout of documents in each language to see if<br/>things have gone awry.</p></p><p><p>For a few documents in a few languages, I have images of how text<br/><em>should</em> look, and can compare that visually (especially important for<br/>complex scripts.)</p></p><p><p>A few years back, somebody wrote a clever script that generated images<br/>of text and compared them pixel-by-pixel.  This was a great idea, and<br/>I wish I could use it more effectively, but the problem was that it<br/>was much too sensitive.  A small change to the font (e.g. PostScript<br/>parameters) would cause a small but global change in the rendering.<br/>Also the rendering could vary from one version of the rendering<br/>software to another.  So I don't use this anymore.</p></p><p><p>That's all I can think of right now.</p></p><p><p>In fact, testing has been a big problem in getting releases out.  In<br/>the past, each release has taken at least two weeks to test, and then<br/>another week to fix and bundle...if I was lucky.  And for the past<br/>couple of years, I just haven't been able to justify the time<br/>expenditure.  (Besides this, there are still a few serious problems<br/>with the fonts--once again, a matter of time.)</p></p><p><p>Have a look at the bugs pages, to get an idea of work being done.</p><footer><strong>Steve White</strong> <cite>GNU FreeFont</cite></footer></blockquote></p>

<p><a href="http://savannah.gnu.org/bugs/?group=freefont">http://savannah.gnu.org/bugs/?group=freefont</a></p>

<p>I'm not sure if ImageMagic or PIL can help solve the rendering and compare
problem Steve is talking about. They can definitely be used for
<a href="/blog/2013/05/17/linux-and-python-tools-to-compare-images/">image comparison</a>
so maybe coupled with some rendering library it's worth a quick try.</p>

<p>If you happen to know more about fonts, please join me in
<a href="/blog/2014/02/28/action-improving-test-coverage-in-fedora/">improving overall test coverage in Fedora</a>
by designing test suites for fonts packages.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Last Week in Fedora QA]]></title>
    <link href="http://atodorov.org/blog/2014/03/03/last-week-in-fedora-qa/"/>
    <updated>2014-03-03T10:23:00+02:00</updated>
    <id>http://atodorov.org/blog/2014/03/03/last-week-in-fedora-qa</id>
    <content type="html"><![CDATA[<p>Here are some highlights from the past week discussions in Fedora which I found
interesting or participated in.</p>

<h2>Call to Action: Improving Overall Test Coverage in Fedora</h2>

<p>I can not stress enough how important it is to further
<a href="/blog/2014/02/28/action-improving-test-coverage-in-fedora/">improve test coverage in Fedora</a>!
You can help too. Here's how:</p>

<ul>
<li>Join upstream and create a test suite for a package you find interesting;</li>
<li>Provide patches - <a href="https://lists.fedoraproject.org/pipermail/devel/2014-February/196035.html">first patch</a>
came in less than 30 minutes of initial announcement :);</li>
<li>Review packages in the wiki and help identify false negatives;</li>
<li>Forward to people who may be interested to work on these items;</li>
<li>Share and promote in your local open source and developer communities;</li>
</ul>


<h2>Auto BuildRequires</h2>

<p><a href="http://people.redhat.com/~rjones/auto-buildrequires/">Auto-BuildRequires</a>
is a simple set of scripts which compliments <code>rpmbuild</code> by
automatically suggesting BuildRequires lines for the just built package.</p>

<p>It would be interesting to have this integrated into Koji and/or
continuous integration environment and compare the output between every two
consecutive builds (iow older and newer package versions). It sounds like a
good way to identify newly added or removed dependencies and update the package
specs accordingly.</p>

<h2>How To Test Fonts Packages</h2>

<p>This is exactly what
<a href="https://lists.fedoraproject.org/pipermail/test/2014-February/120570.html">Christopher Meng asked</a>
and frankly I have no idea.</p>

<p>I've come across a few fonts packages (<em>amiri-fonts</em>, <em>gnu-free-fonts</em> and <em>thai-scalable-fonts</em>)
which seem to have some sort of test suites but I don't know how they work or
what type of problems they test for. On top of that all three have a different
way of doing things (e.g. not using a standardized test framework or a variation of such).</p>

<p>I'll keep you posted on this once I manage to get more info from upstream developers.</p>

<h2>Is URL Field in RPM Useless</h2>

<p>So is it? Opinions here differ from totally useless to "don't remove it, I need it".
However I run a small test and from 2574 RPMs on the source DVD there is around
40% of "something different than HTTP 200 OK". This means <strong>40% potentially broken URLs</strong>!</p>

<p>The majority are responses in the 3XX range and only less than 10% are
actual errors (4XX, 5XX, missing URLs or connection errors).</p>

<p>It will be interesting to see if this can be removed from <code>rpm</code> altogether.
I don't think it will happen soon but if we don't use it why have it there?</p>

<p>My script for the test is
<a href="https://github.com/atodorov/fedora-scripts/blob/master/test-rpm-url-field.sh">here</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Call to Action: Improving Overall Test Coverage in Fedora]]></title>
    <link href="http://atodorov.org/blog/2014/02/28/action-improving-test-coverage-in-fedora/"/>
    <updated>2014-02-28T14:46:00+02:00</updated>
    <id>http://atodorov.org/blog/2014/02/28/action-improving-test-coverage-in-fedora</id>
    <content type="html"><![CDATA[<p>Around Christmas 2013
<a href="/blog/2013/12/24/upstream-test-suite-status-of-fedora-20/">I said</a>
<blockquote><p>... it looks like on average 30% of the packages execute their test suites at<br/>build time in the %check section and less than 35% have test suites at all!<br/>There’s definitely room for improvement and I plan to focus on this during 2014!</p></blockquote></p>

<p>I've recently started working on this goal by first identifying potential offending
packages and discussing the idea on Fedora's
<a href="https://lists.fedoraproject.org/pipermail/devel/2014-February/thread.html">devel</a>,
<a href="https://lists.fedoraproject.org/pipermail/packaging/2014-February/thread.html">packaging</a>
and <a href="https://lists.fedoraproject.org/pipermail/test/2014-February/thread.html">test</a>
mailing lists.</p>

<p>May I present you nearly <strong>2000 packages</strong> which need your love:</p>

<ul>
<li><a href="https://fedoraproject.org/wiki/QA/Testing_in_check">wiki/QA/Testing_in_check</a></li>
<li><a href="https://fedoraproject.org/wiki/QA/Missing_upstream_test_suites">wiki/QA/Missing_upstream_test_suites</a></li>
</ul>


<p>The intent for these pages is to serve as a source of working material for Fedora
volunteers.</p>

<h2>How Can I Help</h2>

<ul>
<li>Join upstream and create a test suite for a package you find interesting;</li>
<li>Provide patches - <a href="https://lists.fedoraproject.org/pipermail/devel/2014-February/196035.html">first patch</a>
came in less than 30 minutes of initial announcement :);</li>
<li>Review packages in the wiki and help identify false negatives;</li>
<li>Forward to people who may be interested to work on these items;</li>
<li>Share and promote in your local open source and developer communities;</li>
</ul>


<h2>Important</h2>

<p>If you would like to gain some open source practice and QA experience I will
happily provide mentorship and general help so you can start working on Fedora.
Just ping me!</p>
]]></content>
  </entry>
  
</feed>
