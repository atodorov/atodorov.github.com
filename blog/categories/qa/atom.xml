<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: QA | atodorov.org - you can logoff, but you can never leave]]></title>
  <link href="http://atodorov.org/blog/categories/qa/atom.xml" rel="self"/>
  <link href="http://atodorov.org/"/>
  <updated>2014-11-17T15:18:43+02:00</updated>
  <id>http://atodorov.org/</id>
  <author>
    <name><![CDATA[Alexander Todorov]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Speeding Up Celery Backends, Part 3]]></title>
    <link href="http://atodorov.org/blog/2014/11/11/speeding-up-celery-backends-part-3/"/>
    <updated>2014-11-11T15:59:00+02:00</updated>
    <id>http://atodorov.org/blog/2014/11/11/speeding-up-celery-backends-part-3</id>
    <content type="html"><![CDATA[<p>In the second part of this article we've seen
<a href="/blog/2014/11/07/speeding-up-celery-backends-part-2/">how slow Celery actually is</a>.
Now let's explore what happens inside and see if we can't speed things up.</p>

<p>I've used <a href="http://pycallgraph.slowchop.com/en/latest/">pycallgraph</a> to create
call graph visualizations of my application. It has the nice feature to also show
execution time and use different colors for fast and slow operations.</p>

<p>Full command line is:</p>

<pre><code>pycallgraph -v --stdlib --include ... graphviz -o calls.png -- ./manage.py celery_load_test
</code></pre>

<p>where the <code>--include</code> is used to limit the graph to a particular Python module(s).</p>

<h2>General findings</h2>

<p><img src="/images/celery/general.png" title="call graph" alt="call graph" /></p>

<ul>
<li>The first four calls is where most of the time is spent as seen on the picture.</li>
<li>As it seems most of the slow down comes from Celery itself, not the underlying messaging
transport Kombu (not shown on picture)</li>
<li><code>celery.app.amqp.TaskProducer.publish_task</code> takes half of the execution time of
<code>celery.app.base.Celery.send_task</code></li>
<li><code>celery.app.task.Task.delay</code> directly executes <code>.apply_async</code> and can be skipped if one
rewrites the code.</li>
</ul>


<h2>More findings</h2>

<p>In <code>celery.app.base.Celery.send_task</code> there is this block of code:</p>

<pre><code>349         with self.producer_or_acquire(producer) as P:
350             self.backend.on_task_call(P, task_id)
351             task_id = P.publish_task(
352                 name, args, kwargs, countdown=countdown, eta=eta,
353                 task_id=task_id, expires=expires,
354                 callbacks=maybe_list(link), errbacks=maybe_list(link_error),
355                 reply_to=reply_to or self.oid, **options
356             )
</code></pre>

<p><code>producer</code> is always None because delay() doesn't pass it as argument.
I've tried passing it explicitly to apply_async() as so:</p>

<pre><code>from djapp.celery import *

# app = debug_task._get_app() # if not defined in djapp.celery
producer = app.amqp.producer_pool.acquire(block=True)
debug_task.apply_async(producer=producer)
</code></pre>

<p>However this doesn't speedup anything. If we replace the above code block like this:</p>

<pre><code>349         with producer as P:
</code></pre>

<p>it blows up on the second iteration because producer and its channel is already None !?!</p>

<p>If you are unfamiliar with the with statement in Python please read
<a href="http://effbot.org/zone/python-with-statement.htm">this article</a>. In short the with statement is
a compact way of writing try/finally. The underlying <code>kombu.messaging.Producer</code> class does a
<code>self.release()</code> on exit of the with statement.</p>

<p>I also tried killing the with statement and using producer directly but with limited success. While
it was not released(was non None) on subsequent iterations the memory usage grew much more and there
wasn't any performance boost.</p>

<h2>Conclusion</h2>

<p>The with statement is used throughout both Celery and Kombu and I'm not at all sure if
there's a mechanism for keep-alive connections. My time constraints are limited and I'll probably
not spend anymore time on this problem soon.</p>

<p>Since my use case involves task producer and consumers on localhost I'll try to workaround the
current limitations by using Kombu directly
(see <a href="https://gist.github.com/atodorov/2bc1fcd34531ad260ed7">this gist</a>) with a transport that
uses either a UNIX domain socket or a name pipe (FIFO) file.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Speeding up Celery Backends, Part 2]]></title>
    <link href="http://atodorov.org/blog/2014/11/07/speeding-up-celery-backends-part-2/"/>
    <updated>2014-11-07T15:48:00+02:00</updated>
    <id>http://atodorov.org/blog/2014/11/07/speeding-up-celery-backends-part-2</id>
    <content type="html"><![CDATA[<p>In the <a href="/blog/2014/11/05/speeding-up-celery-backends/">first part</a> of this
post I looked at a few celery backends and discovered they didn't meet my needs.
Why is the Celery stack slow? How slow is it actually?</p>

<h2>How slow is Celery in practice</h2>

<ul>
<li>Queue: 500`000 msg/sec</li>
<li>Kombu:  14`000 msg/sec</li>
<li>Celery:  2`000 msg/sec</li>
</ul>


<h2>Detailed test description</h2>

<p>There are three main components of the Celery stack:</p>

<ul>
<li>Celery itself</li>
<li>Kombu which handles the transport layer</li>
<li>Python Queue()'s underlying everything</li>
</ul>


<p>Using the <a href="https://gist.github.com/atodorov/2bc1fcd34531ad260ed7">Queue and Kombu tests</a>
run for 1 000 000 messages I got the following results:</p>

<ul>
<li>Raw Python Queue: Msgs per sec: 500`000</li>
<li>Raw Kombu without Celery where <code>kombu/utils/__init__.py:uuid()</code> is set to return 0

<ul>
<li>with json serializer: Msgs per sec: 5`988</li>
<li>with pickle serializer: Msgs per sec: 12`820</li>
<li>with the custom mem_serializer from <a href="/blog/2014/11/05/speeding-up-celery-backends/">part 1</a>:
Msgs per sec: 14`492</li>
</ul>
</li>
</ul>


<p><strong>Note:</strong> when the test is executed with 100K messages mem_serializer yielded
25`000 msg/sec then the performance is saturated. I've observed similar behavior
with raw Python Queue()'s. I saw some cache buffers being managed internally to avoid OOM
exceptions. This is probably the main reason performance becomes saturated over a longer
execution.</p>

<ul>
<li>Using <a href="https://gist.github.com/atodorov/0156cc41491a5e1ff953">celery_load_test.py</a> modified to
loop 1 000 000 times I got 1908.0 tasks created per sec.</li>
</ul>


<p>Another interesting this worth outlining - in the kombu test there are these lines:
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>with producers[connection].acquire(block=True) as producer:&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>for j in range(1000000):
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p></span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>If we swap them the performance drops down to 3875 msg/sec which is comparable with the
Celery results. Indeed inside Celery there's the same <code>with producer.acquire(block=True)</code>
construct which is executed every time a new task is published. Next I will be looking
into this to figure out exactly where the slowliness comes from.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Speeding up Celery Backends, Part 1]]></title>
    <link href="http://atodorov.org/blog/2014/11/05/speeding-up-celery-backends/"/>
    <updated>2014-11-05T15:20:00+02:00</updated>
    <id>http://atodorov.org/blog/2014/11/05/speeding-up-celery-backends</id>
    <content type="html"><![CDATA[<p>I'm working on an application which fires a lot of Celery tasks - the more
the better! Unfortunately Celery backends seem to be rather slow :(.
Using the <a href="https://gist.github.com/atodorov/0156cc41491a5e1ff953">celery_load_test.py</a>
command for Django I was able to capture some metrics:</p>

<ul>
<li>Amazon SQS backend: 2 or 3 tasks/sec</li>
<li>Filesystem backend: 2000 - 2500 tasks/sec</li>
<li>Memory backend: around 3000 tasks/sec</li>
</ul>


<p>Not bad but I need in the order of 10000 tasks created per sec!
The other noticeable thing is that memory backend isn't much faster compared to
the filesystem one! NB: all of these backends actually come from the kombu package.</p>

<h2>Why is Celery slow ?</h2>

<p>Using <code>celery_load_test.py</code> together with
<a href="/blog/2014/11/05/performance-profiling-in-python-with-cprofile/">cProfile</a> I
was able to pin-point some problematic areas:</p>

<ul>
<li><code>kombu/transports/virtual/__init__.py</code>: class Channel.basic_publish() - does
self.encode_body() into base64 encoded string. Fixed with custom transport backend
I called fastmemory which redefines the body_encoding property:</li>
</ul>


<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="nd">@cached_property</span>
</span><span class='line'><span class="k">def</span> <span class="nf">body_encoding</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class='line'>    <span class="k">return</span> <span class="bp">None</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<ul>
<li>Celery uses json or pickle (or other) serializers to serialize the data.
While json yields between 2000-3000 tasks/sec, pickle does around 3500 tasks/sec.
Replacing with a custom serializer which just returns
the objects (since we read/write from/to memory) yields about 4000 tasks/sec tops:
<div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">from</span> <span class="nn">kombu.serialization</span> <span class="kn">import</span> <span class="n">register</span><span class="o">&lt;/</span><span class="n">li</span><span class="o">&gt;</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">ul</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="k">def</span> <span class="nf">loads</span><span class="p">(</span><span class="n">s</span><span class="p">):</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="k">return</span> <span class="n">s</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="k">def</span> <span class="nf">dumps</span><span class="p">(</span><span class="n">s</span><span class="p">):</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="k">return</span> <span class="n">s</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">register</span><span class="p">(</span><span class="s">&#39;mem_serializer&#39;</span><span class="p">,</span> <span class="n">dumps</span><span class="p">,</span> <span class="n">loads</span><span class="p">,</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span>    <span class="n">content_type</span><span class="o">=</span><span class="s">&#39;application/x-memory&#39;</span><span class="p">,</span>
</span><span class='line'>    <span class="n">content_encoding</span><span class="o">=</span><span class="s">&#39;binary&#39;</span><span class="p">)</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<ul>
<li><code>kombu/utils/__init__.py</code>: def uuid() - generates random unique identifiers
which is a slow operation. Replacing it with <code>return "00000000"</code> boosts performance
to 7000 tasks/sec.</li>
</ul>


<p>It's clear that a constant UUID is not of any practical use but serves well to illustrate
how much does this function affect performance.</p>

<p><strong>Note:</strong>
Subsequent executions of <code>celery_load_test</code> seem to report degraded performance even with
the most optimized transport backend. I'm not sure why is this. One possibility is the random
UUID usage in other parts of the Celery/Kombu stack which drains entropy on the system and
generating more random numbers becomes slower. If you know better please tell me!</p>

<p>I will be looking for a better understanding
of these IDs in Celery and hope to be able to produce a faster uuid() function. Then I'll be
exploring the transport stack even more in order to reach the goal of 10000 tasks/sec.
If you have any suggestions or pointers please share them in the comments.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Performance Profiling in Python with cProfile]]></title>
    <link href="http://atodorov.org/blog/2014/11/05/performance-profiling-in-python-with-cprofile/"/>
    <updated>2014-11-05T14:40:00+02:00</updated>
    <id>http://atodorov.org/blog/2014/11/05/performance-profiling-in-python-with-cprofile</id>
    <content type="html"><![CDATA[<p>This is a quick reference on profiling Python applications with
<a href="https://docs.python.org/2/library/profile.html#module-cProfile">cProfile</a>:</p>

<pre><code>$ python -m cProfile -s time application.py
</code></pre>

<p>The output is sorted by execution time <code>-s time</code></p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code> 9072842 function calls (8882140 primitive calls) in 9.830 CPU seconds
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p>   Ordered by: internal time&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;p>   ncalls  tottime  percall  cumtime  percall filename:lineno(function)&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>61868    0.575    0.000    0.861    0.000 abstract.py:28(__init__)
</span><span class='line'>41250    0.527    0.000    0.660    0.000 uuid.py:101(__init__)
</span><span class='line'>61863    0.405    0.000    1.054    0.000 abstract.py:40(as_dict)
</span><span class='line'>41243    0.343    0.000    1.131    0.000 __init__.py:143(uuid4)
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p>   577388    0.338    0.000    0.649    0.000 abstract.py:46(&lt;genexpr>)&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>20622    0.289    0.000    8.824    0.000 base.py:331(send_task)
</span><span class='line'>61907    0.232    0.000    0.477    0.000 datastructures.py:467(__getitem__)
</span><span class='line'>20622    0.225    0.000    9.298    0.000 task.py:455(apply_async)
</span><span class='line'>61863    0.218    0.000    2.502    0.000 abstract.py:52(__copy__)
</span><span class='line'>20621    0.208    0.000    4.766    0.000 amqp.py:208(publish_task)
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p>   462640    0.193    0.000    0.247    0.000 {isinstance}
</span><span class='line'>   515525    0.162    0.000    0.193    0.000 abstract.py:41(f)&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>41246    0.153    0.000    0.633    0.000 entity.py:143(__init__)
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p></span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>In the example above (actual application) first line is kombu's
<code>abstract.py: class Object(object).__init__()</code>
and the second one is Python's
<code>uuid.py: class UUID().__init__()</code>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SNAKE is no Longer Needed to Run Installation Tests in Beaker]]></title>
    <link href="http://atodorov.org/blog/2014/07/18/snake-is-no-longer-needed-to-run-installation-tests-in-beaker/"/>
    <updated>2014-07-18T23:05:00+03:00</updated>
    <id>http://atodorov.org/blog/2014/07/18/snake-is-no-longer-needed-to-run-installation-tests-in-beaker</id>
    <content type="html"><![CDATA[<p>This is a quick status update for one of the pieces of
<a href="/blog/2013/11/19/open-source-quality-assurance-infrastructure-for-fedora-qa/">Fedora QA infrastructure</a>
and mostly a self-note.</p>

<p>Previously to control the kickstart configuration used during installation in Beaker one
had to either modify the job XML in Beaker or use SNAKE (<code>bkr workflow-snake</code>) to render
a kickstart configuration from a Python template.</p>

<p>SNAKE presented challenges when deploying and using
<a href="https://beaker.fedoraproject.org">beaker.fedoraproject.org</a> and is
virtually unmaintained.</p>

<p>I present the new <code>bkr workflow-installer-test</code> which uses Jinja2 templates to
generate a kickstart configuration when provisioning the system. This is already
available in beaker-client-0.17.1.</p>

<p>The templates make use of all Jinja2 features (as far as I can tell) so you can create
very complex ones. You can even include snippets from one template into another if required.
The standard context that is passed to the template is:</p>

<ul>
<li><strong>DISTRO</strong> - if specified, the distro name</li>
<li><strong>FAMILY</strong> - as returned by Beaker server, e.g. <em>RedHatEnterpriseLinux6</em></li>
<li><strong>OS_MAJOR</strong> and <strong>OS_MINOR</strong> - also taken from Beaker server. e.g. OS_MAJOR=6 and OS_MINOR=5 for RHEL 6.5</li>
<li><strong>VARIANT</strong> - if specified</li>
<li><strong>ARCH</strong> - CPU architecture like x86_64</li>
<li>any parameters passed to the test job with <code>--taskparam</code>. They are processed last and can override previous values.</li>
</ul>


<p>Installation related tests at <a href="https://bitbucket.org/fedoraqa/fedora-beaker-tests">fedora-beaker-tests</a>
have been updated with a <code>ks.cfg.tmpl</code> templates to use with this new workflow.</p>

<p>This workflow also has the ability to return boot arguments for the installer if needed.
If any, they should be defined in a {% block kernel_options %}{% endblock %}
block inside the template. A simpler variant is to define a comment line that stars with
<em>## kernel_options:</em></p>

<p>There are still a few issues which need to be fixed before beaker.fedoraproject.org
can be used by the general public though. I will be writing another post about that
so stay tuned.</p>
]]></content>
  </entry>
  
</feed>
