<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: QA | atodorov.org - you can logoff, but you can never leave]]></title>
  <link href="http://atodorov.org/blog/categories/qa/atom.xml" rel="self"/>
  <link href="http://atodorov.org/"/>
  <updated>2015-01-05T15:55:27+02:00</updated>
  <id>http://atodorov.org/</id>
  <author>
    <name><![CDATA[Alexander Todorov]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Endless Loop Bug in Candy Crush Saga Level 80]]></title>
    <link href="http://atodorov.org/blog/2015/01/05/endless-loop-bug-candy-crush-saga-level-80/"/>
    <updated>2015-01-05T15:44:00+02:00</updated>
    <id>http://atodorov.org/blog/2015/01/05/endless-loop-bug-candy-crush-saga-level-80</id>
    <content type="html"><![CDATA[<p>Happy new year everyone. During the holidays I've discovered several interesting
bugs which will be revealed in this blog. Starting today with a bug in the popular
game <em>Candy Crush Saga</em>.</p>

<p>In level 80 one teleport is still open but the chocolates are blocking the rest.
The game has ended but candies keep flowing through the teleport and the level doesn't exit.
My guess is that the game logic is missing a check whether or not it will go into an endless loop.</p>

<iframe width="560" height="315" src="http://atodorov.org//www.youtube.com/embed/haBepFwyaxY" frameborder="0" allowfullscreen></iframe>


<p>This bug seems to be generic for the entire game. It pops up also on
level 137 in the Owl part of the game (recorded by somebody else):</p>

<iframe width="420" height="315" src="http://atodorov.org//www.youtube.com/embed/6q1_LIdamqw" frameborder="0" allowfullscreen></iframe>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[BlackBerry Z10 is Killing My WiFi Router]]></title>
    <link href="http://atodorov.org/blog/2014/12/22/blackberry-z10-is-killing-my-wifi-router/"/>
    <updated>2014-12-22T15:46:00+02:00</updated>
    <id>http://atodorov.org/blog/2014/12/22/blackberry-z10-is-killing-my-wifi-router</id>
    <content type="html"><![CDATA[<p>Few days ago I've resurrected my BlackBerry Z10 only to find out that it kills
my WiFi router shortly after connecting to the network.
It looks like many people are having the same problem with BlackBerry but most forum
threads don't offer a meaningful solution so I did some tests.</p>

<p>Everything works fine when WiFi mode is set to either 11bgn mixed or 11n only and
WiFi security is disabled.</p>

<p>When using WPA2/Personal security mode and AES encryption the problem occurs
regardless of which WiFi mode is used. There is another type of encryption called TKIP
but the device itself warns that this is not supported by the 802.11n specification
(all my devices use it anyway).</p>

<p>So to recap:
<strong>BlackBerry Z10 causes my TP-Link router to die if using WPA2/Personal security with
AES Encryption. Switching to open network with MAC address filtering works fine!</strong></p>

<p>I haven't had the time to upgrade the firmware of this router and see if the problem persists.
Most likely I'll just go ahead and flash it with OpenWRT.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Speed Comparison of Web Proxies Written in Python Twisted and Go]]></title>
    <link href="http://atodorov.org/blog/2014/11/19/speed-comparison-of-web-proxies-written-in-python-twisted-and-go/"/>
    <updated>2014-11-19T16:57:00+02:00</updated>
    <id>http://atodorov.org/blog/2014/11/19/speed-comparison-of-web-proxies-written-in-python-twisted-and-go</id>
    <content type="html"><![CDATA[<p>After I figured out that
<a href="/blog/2014/11/11/speeding-up-celery-backends-part-3/">Celery is rather slow</a>
I moved on to test another part of my environment - a web proxy server.
The test here compares two proxy
<a href="https://gist.github.com/atodorov/666035d270d97d982cd5">implementations</a>
- one with Python Twisted,
the other in Go. The backend is a simple web server written in Go, which is
probably the fastest thing when it comes to serving HTML.</p>

<p>The test content is a snapshot of the front page of this blog taken few days ago.
The system is a standard Lenovo X220 laptop, with Intel Core i7 CPU, with 4 cores.
The measurement instrument is the popular wrk tool with a
<a href="/blog/2014/11/18/proxy-support-for-wrk-http-benchmarking-tool/">custom Lua script to redirect the requests through the proxy</a>.</p>

<p>All tests were repeated several times, only the best results are shown here.
I've taken time between the tests in order for all open TCP ports to close.
I've also observed the number of open ports (e.g. sockets) using <code>netstat</code>.</p>

<h2>Baseline</h2>

<p>Using wrk against the web server in Go yields around 30000 requests per second
with an average of 2000 TCP ports in use:</p>

<pre><code>$ ./wrk -c1000 -t20 -d30s http://127.0.0.1:8000/atodorov.html
Running 30s test @ http://127.0.0.1:8000/atodorov.html
  20 threads and 1000 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   304.43ms  518.27ms   1.47s    82.69%
    Req/Sec     1.72k     2.45k   17.63k    88.70%
  1016810 requests in 29.97s, 34.73GB read
  Non-2xx or 3xx responses: 685544
Requests/sec:  33928.41
Transfer/sec:      1.16GB
</code></pre>

<h2>Python Twisted</h2>

<p>The <a href="https://gist.github.com/atodorov/666035d270d97d982cd5">Twisted implementation</a>
performs at little over 1000 reqs/sec with an average TCP port use between 20000 and 30000:</p>

<pre><code>./wrk -c1000 -t20 -d30s http://127.0.0.1:8080 -s scripts/proxy.lua -- http://127.0.0.1:8000/atodorov.html
Running 30s test @ http://127.0.0.1:8080
  20 threads and 1000 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   335.53ms  117.26ms 707.57ms   64.77%
    Req/Sec   104.14     72.19   335.00     55.94%
  40449 requests in 30.02s, 3.67GB read
  Socket errors: connect 0, read 0, write 0, timeout 8542
  Non-2xx or 3xx responses: 5382
Requests/sec:   1347.55
Transfer/sec:    125.12MB
</code></pre>

<h2>Go proxy</h2>

<p>First I've run several 30 seconds tests and performance was around 8000 req/sec
with around 20000 ports used (most of them remain in TIME_WAIT state for a while).
Then I've modified <code>proxy.go</code> to make use of all available CPUs on the system and let
the test run for 5 minutes.</p>

<pre><code>$ ./wrk -c1000 -t20 -d300s http://127.0.0.1:9090 -s scripts/proxy.lua -- http://127.0.0.1:8000/atodorov.html
Running 5m test @ http://127.0.0.1:9090
  20 threads and 1000 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   137.22ms  437.95ms   4.45s    97.55%
    Req/Sec   669.54    198.52     1.71k    76.40%
  3423108 requests in 5.00m, 58.27GB read
  Socket errors: connect 0, read 26, write 181, timeout 24268
  Non-2xx or 3xx responses: 2870522
Requests/sec:  11404.19
Transfer/sec:    198.78MB
</code></pre>

<p>Performance peaked at 10000 req/sec. TCP port usage initially rose to around 30000
but rapidly dropped and stayed around 3000. Both <code>webserver.go</code> and <code>proxy.go</code> were
printing the following messages on the console:</p>

<pre><code>2014/11/18 21:53:06 http: Accept error: accept tcp [::]:9090: too many open files; retrying in 1s
</code></pre>

<h2>Conclusion</h2>

<p>There's no doubt that Go is blazingly fast compared to Python and I'm most likely to use it
further in my experiments. Still I didn't expect a 3x difference in performance from webserver vs. proxy.</p>

<p>Another thing that worries me is the huge number of open TCP ports which then drops and stays
consistent over time and the error messages from both webserver and proxy (maybe per process sockets limit).</p>

<p>At the moment I'm not aware of the internal workings of neither wrk, nor
Go itself, nor the goproxy library to make conclusion if this is a bad thing or expected.
I'm eager to hear what others think in the comments. Thanks!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Proxy Support for wrk HTTP Benchmarking Tool]]></title>
    <link href="http://atodorov.org/blog/2014/11/18/proxy-support-for-wrk-http-benchmarking-tool/"/>
    <updated>2014-11-18T10:04:00+02:00</updated>
    <id>http://atodorov.org/blog/2014/11/18/proxy-support-for-wrk-http-benchmarking-tool</id>
    <content type="html"><![CDATA[<p>Few times recently I've seen people using an HTTP benchmarking tool called
<a href="https://github.com/wg/wrk">wrk</a> and decided to give it a try. It is a very cool
instrument but didn't fit my use case perfectly. What I needed is to be able to
redirect the connection through a web proxy and measure how much the proxy
slows down things compared to hitting the web server directly with wrk.
In other words - how fast is the proxy server.</p>

<h2>How does a proxy work</h2>

<p>I've examined the source code of two proxies (one in Python and another one in Go)
and what happens is this:</p>

<ul>
<li>The proxy server starts listening to a TCP port</li>
<li>A client (e.g. the browser) sends the request using an absolute URL (GET http://example.com/about.html)</li>
<li>Instead of connecting directly to the web server behind example.com the client connects to the proxy</li>
<li>The proxy server does connect to example.com directly, reads the response and delivers it back to
the client.</li>
</ul>


<h2>Proxy in wrk</h2>

<p>Luckily wrk supports the execution of Lua scripts so we can make a
<a href="https://github.com/wg/wrk/pull/107">simple script</a> like this:</p>

<pre><code>init = function(args)
    target_url = args[1] -- proxy needs absolute URL
end

request = function()
    return wrk.format("GET", target_url)
end
</code></pre>

<p>Then update your command line to something like this:</p>

<pre><code>./wrk [options] http://proxy:port -s proxy.lua -- http://example.com/about.html
</code></pre>

<p>This causes wrk to connect to our proxy server but instead issue GET requests for another URL.
Depending on how your proxy works you may need to add the <code>Host: example.com</code> header as well.
Now let's do some testing.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Speeding Up Celery Backends, Part 3]]></title>
    <link href="http://atodorov.org/blog/2014/11/11/speeding-up-celery-backends-part-3/"/>
    <updated>2014-11-11T15:59:00+02:00</updated>
    <id>http://atodorov.org/blog/2014/11/11/speeding-up-celery-backends-part-3</id>
    <content type="html"><![CDATA[<p>In the second part of this article we've seen
<a href="/blog/2014/11/07/speeding-up-celery-backends-part-2/">how slow Celery actually is</a>.
Now let's explore what happens inside and see if we can't speed things up.</p>

<p>I've used <a href="http://pycallgraph.slowchop.com/en/latest/">pycallgraph</a> to create
call graph visualizations of my application. It has the nice feature to also show
execution time and use different colors for fast and slow operations.</p>

<p>Full command line is:</p>

<pre><code>pycallgraph -v --stdlib --include ... graphviz -o calls.png -- ./manage.py celery_load_test
</code></pre>

<p>where the <code>--include</code> is used to limit the graph to a particular Python module(s).</p>

<h2>General findings</h2>

<p><img src="/images/celery/general.png" title="call graph" alt="call graph" /></p>

<ul>
<li>The first four calls is where most of the time is spent as seen on the picture.</li>
<li>As it seems most of the slow down comes from Celery itself, not the underlying messaging
transport Kombu (not shown on picture)</li>
<li><code>celery.app.amqp.TaskProducer.publish_task</code> takes half of the execution time of
<code>celery.app.base.Celery.send_task</code></li>
<li><code>celery.app.task.Task.delay</code> directly executes <code>.apply_async</code> and can be skipped if one
rewrites the code.</li>
</ul>


<h2>More findings</h2>

<p>In <code>celery.app.base.Celery.send_task</code> there is this block of code:</p>

<pre><code>349         with self.producer_or_acquire(producer) as P:
350             self.backend.on_task_call(P, task_id)
351             task_id = P.publish_task(
352                 name, args, kwargs, countdown=countdown, eta=eta,
353                 task_id=task_id, expires=expires,
354                 callbacks=maybe_list(link), errbacks=maybe_list(link_error),
355                 reply_to=reply_to or self.oid, **options
356             )
</code></pre>

<p><code>producer</code> is always None because delay() doesn't pass it as argument.
I've tried passing it explicitly to apply_async() as so:</p>

<pre><code>from djapp.celery import *

# app = debug_task._get_app() # if not defined in djapp.celery
producer = app.amqp.producer_pool.acquire(block=True)
debug_task.apply_async(producer=producer)
</code></pre>

<p>However this doesn't speedup anything. If we replace the above code block like this:</p>

<pre><code>349         with producer as P:
</code></pre>

<p>it blows up on the second iteration because producer and its channel is already None !?!</p>

<p>If you are unfamiliar with the with statement in Python please read
<a href="http://effbot.org/zone/python-with-statement.htm">this article</a>. In short the with statement is
a compact way of writing try/finally. The underlying <code>kombu.messaging.Producer</code> class does a
<code>self.release()</code> on exit of the with statement.</p>

<p>I also tried killing the with statement and using producer directly but with limited success. While
it was not released(was non None) on subsequent iterations the memory usage grew much more and there
wasn't any performance boost.</p>

<h2>Conclusion</h2>

<p>The with statement is used throughout both Celery and Kombu and I'm not at all sure if
there's a mechanism for keep-alive connections. My time constraints are limited and I'll probably
not spend anymore time on this problem soon.</p>

<p>Since my use case involves task producer and consumers on localhost I'll try to workaround the
current limitations by using Kombu directly
(see <a href="https://gist.github.com/atodorov/2bc1fcd34531ad260ed7">this gist</a>) with a transport that
uses either a UNIX domain socket or a name pipe (FIFO) file.</p>
]]></content>
  </entry>
  
</feed>
