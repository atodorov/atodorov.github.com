<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>atodorov.org</title><link href="http://atodorov.org/" rel="alternate"></link><link href="http://atodorov.org/blog/categories/qa/atom.xml" rel="self"></link><id>http://atodorov.org/</id><updated>2016-11-30T17:48:00+02:00</updated><entry><title>Highlights from ISTA and GTAC 2016</title><link href="http://atodorov.org/blog/2016/11/30/highlights-from-ista-and-gtac-2016/" rel="alternate"></link><updated>2016-11-30T17:48:00+02:00</updated><author><name>Alexander Todorov</name></author><id>tag:atodorov.org,2016-11-30:blog/2016/11/30/highlights-from-ista-and-gtac-2016/</id><summary type="html">&lt;p&gt;&lt;img alt="ISTA 2016" src="/images/ista2016.jpg" title="ISTA 2016" /&gt;&lt;/p&gt;
&lt;p&gt;Another two weeks have passed and I'm blogging about another 2 conferences.
This year both
&lt;a href="https://istacon.org/"&gt;Innovations in Software Technologies and Automation&lt;/a&gt; and
&lt;a href="https://developers.google.com/google-test-automation-conference/2016/"&gt;Google Test Automation Conference&lt;/a&gt;
happened on the same day. I was attending ISTA in Sofia during the day and
watching the live stream of GTAC during the evenings. Here are some of the things
that reflected on me:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How can I build my software in order to make this a perfect day for the user ?&lt;/li&gt;
&lt;li&gt;People are not the problem which causes bad software to exist. When designing
software focus on what people need not on what technology is forcing them to do;&lt;/li&gt;
&lt;li&gt;You need to have blind trust in the people you work with because all the times
projects look like they are not going to work until the very end!&lt;/li&gt;
&lt;li&gt;It's good to have diverse set of characters in the company and not homogenize people;&lt;/li&gt;
&lt;li&gt;Team performance grows over time. Effective teams minimize time waste
during bad periods. They (have and) extract value from conflicts!&lt;/li&gt;
&lt;li&gt;One-on-one meetings are usually like status reports which is bad. Both parties
should bring their own issues to the table;&lt;/li&gt;
&lt;li&gt;To grow an effective team members need to do things together. For example
pair programming, writing test scenarios, etc;&lt;/li&gt;
&lt;li&gt;When teams don't take actions after retrospective meetings it is usually a 
sign of missing foundational trust;&lt;/li&gt;
&lt;li&gt;QA engineers need to be present at every step of the software development
life-cycle! This is something I teach my students and have been confirmed by
many friends in the industry;&lt;/li&gt;
&lt;li&gt;Agile is all about data, even when it comes to testing and quality. We need to
decompose and measure iteratively;&lt;/li&gt;
&lt;li&gt;Agile is also about really skillful people. One way to boost your skills is to
adopt the T-shaped specialist model;&lt;/li&gt;
&lt;li&gt;In agile iterative work and continuous delivery is king so QA engineers need to focus
even more on visualization (I will also add documentation), refactoring and code reviews;&lt;/li&gt;
&lt;li&gt;Agile teams need to give opportunities to team members for taking risk and taking
ownership of their actions in the gray zone (e.g. actions which isn't clear who should
be doing).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the brave new world of micro services end to end testing is no more! We test each level
in isolation but keep stable contracts/APIs between levels. That way we can reduce
the test burden and still guarantee an acceptable level of risk. This change in
software architecture (monolithic vs. micro) leads to change in technologies (one framework/language
vs. what's best for the task) which in turn leads to changes in testing approach and
testing tools. This ultimately leads to changing people on the team because they now
need different skills than when they were hired! This circles back to the T-shaped
specialist model and the fact that QA should be integrated in every step of the way!
Thanks to Denitsa Evtimova and Lyudmila Labova for this wisdom and the quote pictured
above.&lt;/p&gt;
&lt;p&gt;Aneta Petkova talked about monitoring of test results which is a topic very close to my work.
Imagine you have your automated test suite but still get failures occasionally. Are these
bugs or something else broke ? If they are bugs and you are waiting for them to be fixed
do you execute the tests against the still broken build or wait ? If you do, what additional info
do you get from these executions vs. how much time do you spend figuring out
"oh, that's still not fixed or geez, here's another hidden bug in the code" ?&lt;/p&gt;
&lt;p&gt;Her team has modified their test execution framework (what I'd usually call a test runner
or even test lab) to have knowledge about issues in JIRA and skip some tests
when no meaningful information can be extracted from them. I have to point out that this
approach may require a lot of maintenance in environments where you have failures due to
infrastructure issues. This idea connects very nicely
with the general idea behind this year's GTAC - don't run tests if you don't need to aka
smart test execution!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=2yN53k9jz3U#t=2h54m0s"&gt;Boris Prikhodskiy&lt;/a&gt;
shared a very simple rule. Don't execute tests&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;which have 100 % pass rate;&lt;/li&gt;
&lt;li&gt;during the last month;&lt;/li&gt;
&lt;li&gt;and have been executed at least 100 times before that!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is what Unity does for their numerous topic branches and reduces test time
with 60-70 percent. All of the test suite is still executed against their trunk branch
and PR merge queue branches!&lt;/p&gt;
&lt;p&gt;At GTAC there were several presentations about speeding up test execution time.
&lt;a href="https://www.youtube.com/watch?v=hbocBqOpuAo#t=3h18m25s"&gt;Emanuil Slavov&lt;/a&gt; was very practical
but the most important thing he said was that a fast test suite is the result of many
conscious actions which introduced small improvements over time. His team had assigned
themselves the task to iteratively improve their test suite performance and at every step
of the way they analyzed the existing bottlenecks and experimented with possible solutions.&lt;/p&gt;
&lt;p&gt;The steps in particular are (on a single machine):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Execute tests in dedicated environment;&lt;/li&gt;
&lt;li&gt;Start with empty database, not used by anything else; This also leads to
adjustments in your test suite architecture and DB setup procedures;&lt;/li&gt;
&lt;li&gt;Simulate and stub external dependencies like 3rd party services;&lt;/li&gt;
&lt;li&gt;Move to containers but beware of slow disk I/O;&lt;/li&gt;
&lt;li&gt;Run database in memory not on disk because it is a temporary DB anyway;&lt;/li&gt;
&lt;li&gt;Don't clean test data, just trash the entire DB once you're done;&lt;/li&gt;
&lt;li&gt;Execute tests in parallel which should be the last thing to do!&lt;/li&gt;
&lt;li&gt;Equalize workload between parallel threads for optimal performance;&lt;/li&gt;
&lt;li&gt;Upgrade the hardware (RAM, CPU) aka vertical scaling;&lt;/li&gt;
&lt;li&gt;Add horizontal scaling (probably with a messaging layer);&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=2yN53k9jz3U#t=6h38m7s"&gt;John Micco and Atif Memon&lt;/a&gt;
talked about flaky tests at Google:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;84% of the transitions from PASS to FAIL are flakes;&lt;/li&gt;
&lt;li&gt;Almost 16% of their 3.5 million tests have some level of flakiness;&lt;/li&gt;
&lt;li&gt;Flaky failures frequently block and delay releases;&lt;/li&gt;
&lt;li&gt;Google spends between 2% and 16% of their CI compute resources
re-running flaky tests;&lt;/li&gt;
&lt;li&gt;Flakiness insertion speed is comparable to flakiness removal speed!&lt;/li&gt;
&lt;li&gt;The optimal setting is 2 persons modifying the same source file at the same time.
This leads to minimal chance of breaking stuff;&lt;/li&gt;
&lt;li&gt;Fix or delete flaky tests because you don't get meaningful value out of them.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So Google want to stop a test execution before it is executed if historical
data shows that the test has attributes of flakiness. The research they talk
about utilizes tons of data collected from Google's CI environment which was
the most interesting fact for me. Indeed if we use data to decide which features
to build for our customers then why not use data to govern the process of testing?
In addition to the video you should read John's post
&lt;a href="https://testing.googleblog.com/2016/05/flaky-tests-at-google-and-how-we.html"&gt;Flaky Tests at Google and How We Mitigate Them&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;At the end I'd like to finish with 
Rahul Gopinath's
&lt;a href="https://www.youtube.com/watch?v=hbocBqOpuAo#t=4h57m30s"&gt;Code Coverage is a Strong Predictor of Test suite Effectiveness in the Real World&lt;/a&gt;.
He basically said that code coverage metrics as we know them today are still
the best practical indicator of how good a test suite is. He argues that mutation testing is slow
and only provides additional 4% to a well designed test suite. This is absolutely the opposite of
what Laura Inozemtseva presented last year in her
&lt;a href="https://www.youtube.com/watch?v=sAfROROGujU&amp;amp;list=PLSIUOFhnxEiCWGsN9t5A-XOhRbmz54IS1&amp;amp;index=25"&gt;Coverage is Not Strongly Correlated with Test Suite Effectiveness&lt;/a&gt;
lightning talk. Rahul also made a point about sample size in the two research papers
and I had the impression he's saying Laura didn't do a proper academic research.&lt;/p&gt;
&lt;p&gt;I'm a heavy contributor to Cosmic Ray, the mutation testing tool for Python and also use
mutation testing in my daily job so this is a very interesting topic indeed. I've asked
fellow tool authors to have a look at both presentations and share their opinions.
I also have an idea about a practical experiment to see if full branch coverage and
full mutation coverage will be able to find a known bug in a piece of software
I wrote. I will be writing about this experiment next week so stay tuned.&lt;/p&gt;
&lt;p&gt;Thanks for reading and happy testing!&lt;/p&gt;</summary><category term="events"></category><category term="QA"></category><category term="fedora.planet"></category></entry><entry><title>IT Weekend Highlights</title><link href="http://atodorov.org/blog/2016/11/02/it-weekend-highlights/" rel="alternate"></link><updated>2016-11-02T09:48:00+02:00</updated><author><name>Alexander Todorov</name></author><id>tag:atodorov.org,2016-11-02:blog/2016/11/02/it-weekend-highlights/</id><summary type="html">&lt;p&gt;&lt;img alt="IT Weekend" src="/images/itweekend_cake.jpg" title="IT Weekend" /&gt;&lt;/p&gt;
&lt;p&gt;Last weekend I attended the third &lt;a href="http://it-weekend.com"&gt;IT Weekend&lt;/a&gt;.
It's like a training camp for athletes but for QA engineers. While during the
&lt;a href="http://atodorov.org/blog/2016/10/08/what-i-learned-from-it-weekend/"&gt;first week&lt;/a&gt; the crowd was more in
the junior to mid level range now the crowd was more into the senior level range
which made for better talks and discussions. The most interesting sessions
for me were &lt;em&gt;On-boarding of New Team Members&lt;/em&gt; by Nikola Naidenov and
&lt;em&gt;Agile Leadership&lt;/em&gt; by Bogoi Bogdanov. Here are some of the highlights
that I wrote down:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;On-boarding of new team members is very important. You need to have a plan for
their first 6 months at the company. This plan needs to have clearly defined tasks
and expectations;&lt;/li&gt;
&lt;li&gt;When a person becomes productive for their team/company it means they have been
on-boarded successfully;&lt;/li&gt;
&lt;li&gt;A company provided trainer is a good thing but they tend to focus on broader
knowledge, they don't cover team specific domain knowledge;&lt;/li&gt;
&lt;li&gt;Some companies provide both technical and business trainers for their teams;&lt;/li&gt;
&lt;li&gt;It is very important to get timely feedback when you are the one providing training.
However feedback isn't always easy to get and we don't always receive sincere feedback;&lt;/li&gt;
&lt;li&gt;If the team is swamped with work tasks you need to provide 10-20% of the time
for learning and experimenting with new technologies. IMO this is best done by
filing tickets in your bug/task tracking system and prioritizing them together with
the rest of the tasks;&lt;/li&gt;
&lt;li&gt;It is also important to have an individual training plan for each team member
and review this on a regular basis;&lt;/li&gt;
&lt;li&gt;We should strive to use unified terminology and jargon as to not confuse
people. IMO it is usually the new hires who are likely to get confused because they
are not familiar with the history of the terms used;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In an agile environment we calculate productivity using the formula&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;Productivity = Effort * Competence * Environment * Motivation^2
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;There are 3 important factors that drive motivation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;to have a &lt;strong&gt;Purpose&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;to feel &lt;strong&gt;Autonomy&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;to be able to achieve &lt;strong&gt;Mastery&lt;/strong&gt; in your skills&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is also important to note that autonomy is not the opposite of alignment
as depicted by the image below.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Alignment and autonomy" src="/images/alignment_autonomy.jpg" title="alignment and autonomy" /&gt;&lt;/p&gt;
&lt;p&gt;In agile environment control is a function of trust. To be able to trust people we
need to give them autonomy, transparency and short feedback loop!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Manage for the normal treat exceptions as exceptional;&lt;/li&gt;
&lt;li&gt;Failure recovery is more important than failure avoidance;&lt;/li&gt;
&lt;li&gt;Fail fast means learning fast and improving fast.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thanks for reading and happy testing!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Photo credit: Rayna Stankova&lt;/em&gt;&lt;/p&gt;</summary><category term="events"></category><category term="QA"></category></entry><entry><title>Animate &amp; Automate</title><link href="http://atodorov.org/blog/2016/10/24/animate-automate/" rel="alternate"></link><updated>2016-10-24T00:01:00+03:00</updated><author><name>Alexander Todorov</name></author><id>tag:atodorov.org,2016-10-24:blog/2016/10/24/animate-automate/</id><summary type="html">&lt;p&gt;Recently I've attended a presentation by MentorMate where they talked about
testing CSS animations (
&lt;a href="https://www.youtube.com/watch?v=MIxzzfFBR8o&amp;amp;list=PLFjlI7p-h1hxBP3cIjEqePSeoBDHud5Db&amp;amp;index=6"&gt;video in Bulgarian&lt;/a&gt;
). The software under test was an ad tech SDK which
provides CSS based animations to mobile apps and games. The content
is displayed inside a webview and they had to make sure animations were
working correctly on different OS and devices.&lt;/p&gt;
&lt;p&gt;Analyzing the content (aka getting to know the domain) they figured out
in reality there were about 20 basic movements and transformations. So the
problem was reduced to "How do we test these 20 basic movements under
various OSes and devices" or "How do we verify that basic CSS transformations
are supported under different versions and flavors of the OS"?&lt;/p&gt;
&lt;p&gt;Their test bed included hand crafted web pages with each basic movement
and then several ones with more complex animations (aka integration testing).
The idea was to load
these pages under different devices and inspect whether or not the animations
were visualized properly.&lt;/p&gt;
&lt;p&gt;A test script (aka their testing framework) was constantly recording the
coordinates of the elements under test to verify that they were really animated.
The idea was to use a sample rate of 20ms and expect at lest 20 different changes
to the element under test. Coordinates along with color and gradient were recorded
and then returned back and analyzed to report a PASS or FAIL result.&lt;/p&gt;
&lt;p&gt;This simplistic framework has limitations of course. It is not currently checking
the boundaries of where the elements are rendered on the screen. Thus if everything
else works as expected this will be a false positive result. On their slides
this can be seen at &lt;a href="https://youtu.be/MIxzzfFBR8o?t=23m10s"&gt;23:10&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As a side note the entire effort took about 2 days, including research and preparing
the test content.&lt;/p&gt;
&lt;p&gt;I really like the back to basics approach here and the simplistic framework that
MentorMate came up with. Sure it misses some problems but for that particular case
it is good enough, easy and fast to implement.&lt;/p&gt;</summary><category term="QA"></category></entry><entry><title>What I Learned from IT Weekend</title><link href="http://atodorov.org/blog/2016/10/08/what-i-learned-from-it-weekend/" rel="alternate"></link><updated>2016-10-08T13:48:00+03:00</updated><author><name>Alexander Todorov</name></author><id>tag:atodorov.org,2016-10-08:blog/2016/10/08/what-i-learned-from-it-weekend/</id><summary type="html">&lt;p&gt;&lt;img alt="IT Weekend" src="/images/itweekend.png" title="IT Weekend" /&gt;&lt;/p&gt;
&lt;p&gt;Last week I attended the first &lt;a href="http://it-weekend.com"&gt;IT Weekend&lt;/a&gt; in Bulgaria.
It's like a training camp for athletes but for QA engineers. There were 20 people
attending and the format was very friendly and relaxed. The group had members with
various levels of experience and technical skills, also different areas they work in.
All presentations are on
&lt;a href="https://www.youtube.com/watch?v=iRA3tRwclRU&amp;amp;list=PLFjlI7p-h1hxBP3cIjEqePSeoBDHud5Db&amp;amp;index=1"&gt;YouTube&lt;/a&gt;.
Here's a brief of what happened and what I learned.&lt;/p&gt;
&lt;p&gt;I had the honor to present in the first slot and gave a quick introduction
to mutation testing. This was my first time giving this talk and I'm not entirely
happy with how I've presented it. Also mutation testing is touching a lot on unit tests,
programming and source code which in some organizations goes to the devel department.
I think mutation testing is harder to understand from people not familiar with it
than I initially thought. I'm taking note to improve the way I present this topic
to the public.&lt;/p&gt;
&lt;p&gt;Yavor Donev gave a good overview of Appium and how to use it on Android.
The most important question for me was "&lt;em&gt;Is it possible to utilize the same test suite
on Android and iOS, given that the environments are different&lt;/em&gt;". With this I mean
regardless of how much we try to make the same (native) app on both platforms
it will end up differently because the platforms are essentially different. For
example there is a different number of physical buttons available.&lt;/p&gt;
&lt;p&gt;If we assume that both iOS and Android apps follow the same design and use
similar workflow then it should be possible to create a test suite which is
platform aware and account for the minor differences. We're also adding
another layer of complexity by introducing the requirement that both apps stay
in sync with each other and account for the quirks of the foreign platform.
Depending on your apps and goals this may not be an easy task!&lt;/p&gt;
&lt;p&gt;The one thing I didn't like about Appium is that upstream doesn't care much about
version compatibility and they tend to break and change stuff arbitrary between
releases. That said if it works, don't update it or otherwise be extremely
careful.&lt;/p&gt;
&lt;p&gt;I also had a nice chat with Yavor on the topic of career change, learning to
program and working with people who have very little coding experience. His
approach is to develop a higher level test framework on top of Appium which
his team mates can use more easily.&lt;/p&gt;
&lt;p&gt;Aneta Petkova's &lt;em&gt;Selenium Grid in Unix Environment&lt;/em&gt; is a bit out of my domain.
However I took one important lesson: regardless of how great your tools are
there are minor details which can make or break your day. In her case these
are the physical location of the tests (e.g. which Selenium node runs them) and
access to shared resources. Turns out WebDriver doesn't give you
this information directly and you need to go through hoops to get it. Her
solution was to place the test code on the Grid Hub and provide a shared file system.&lt;/p&gt;
&lt;p&gt;The bigger lesson is: whenever you have to design an automated test environment
(aka test lab) make sure to evaluate your needs beforehand.&lt;/p&gt;
&lt;p&gt;The last talk was a guest appearance by Denitsa Evtimova. She is a QA architect
with 16 years of experience and presented the QA strategy at Paysafe group.
They have a large monolithic system (legacy code) and have adopted a pyramid
style approach to testing. Whenever possible tests are brought down to the lowest
level (e.g. unit tests) and not repeated on the higher levels. At the top
stand manual testing. Teams are small: 3-4 developers and 2-3 QAs. It is the team
responsibility to make sure tests are implemented at the lowest possible level.
The process is not strictly enforced and the company relies more on self
governance in this aspect. Also everyone on the team can contribute
additional tests whenever they see something missing.
Test (writing) tasks are all logged in JIRA. They are also small so that
everything can be completed in the same day.&lt;/p&gt;
&lt;p&gt;The second day was more informal. We did a quick exploratory testing
exercise and shared opinions on different test tools. Then the group had
a discussion about soft skills and how QA engineers can change the perception
of developers about the QA profession (especially in teams where there are many
manual testers). The key points are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Criticize the software, not the person, e.g. don't blame the person directly;&lt;/li&gt;
&lt;li&gt;Communicate with concrete facts and data, not emotions and perceptions;&lt;/li&gt;
&lt;li&gt;Jokes of the type "how many QA engineers are needed to screw a light bulb"
are a problem because they lead to underestimation of the job role;&lt;/li&gt;
&lt;li&gt;Sometimes it is not quite clear (to others) how the QA role contributes to
the development of the product and the organization;&lt;/li&gt;
&lt;li&gt;For a QA it is important to be able to give a non-biased opinion and
observations on what is happening with the product/process;&lt;/li&gt;
&lt;li&gt;A QA person needs to be very calm. They have to be able to listen to
everybody (especially developers) and accept their point of view but at
the same time also communicate their own point of view.&lt;/li&gt;
&lt;li&gt;It is important
to sit together with developers and observe the problem, brainstorm and
propose possible solutions. This also creates a feedback loop where the
developer feels empowered because he's part of the process identifying
the problem and proposing the best solution;&lt;/li&gt;
&lt;li&gt;In agile teams it is a good idea to rotate people between developer and
QA positions. This will help them better understand the job of others,
acquire new skills and also bring fresh thinking to the team;&lt;/li&gt;
&lt;li&gt;Quality Assurance is an ungrateful job and only people with very calm and methodical
thinking (to follow through and write all possible scenarios) are able to excel
in this field. On the other hand developer usually think about the happy path
scenarios and strive to make their code work as best as they can;&lt;/li&gt;
&lt;li&gt;By rotating job roles within the team developers
will quickly find out that testing is not their field and gain respect
towards their QA peers;&lt;/li&gt;
&lt;li&gt;US managers have the habit of telling "good job" to everyone, even for
small and routine tasks. In Bulgaria (and maybe elsewhere) we're not used
to this. Instead we're used of being scolded when we do something wrong.
If everything is good then we don't receive any recognition;&lt;/li&gt;
&lt;li&gt;Using the American "good job" is actually a good thing. Team mates will
start performing better over time because they will feel their work is
valued and not meaningless, they will feel recognized which will boost
morale and productivity.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thanks for reading and happy testing!&lt;/p&gt;</summary><category term="events"></category><category term="QA"></category></entry><entry><title>Python 2 vs. Python 3 List Sort Causes Bugs</title><link href="http://atodorov.org/blog/2016/08/05/python-2-vs-python-3-list-sort-causes-bugs/" rel="alternate"></link><updated>2016-08-05T10:30:00+03:00</updated><author><name>Alexander Todorov</name></author><id>tag:atodorov.org,2016-08-05:blog/2016/08/05/python-2-vs-python-3-list-sort-causes-bugs/</id><summary type="html">&lt;p&gt;Can sorting a list of values crash your software? Apparently it can and is
another example of my
&lt;a href="http://atodorov.org/blog/2016/03/25/hello-world-qa-challenge/"&gt;Hello World Bugs&lt;/a&gt;.
Python 3 has simplified the
&lt;a href="https://docs.python.org/3/whatsnew/3.0.html#ordering-comparisons"&gt;rules for ordering comparisons&lt;/a&gt;
which changes the behavior of sorting lists when their contents are dictionaries.
For example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Python&lt;/span&gt; &lt;span class="mf"&gt;2.7&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Oct&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt; &lt;span class="mi"&gt;2015&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;17&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;47&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;GCC&lt;/span&gt; &lt;span class="mf"&gt;4.8&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="mi"&gt;20140911&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Red&lt;/span&gt; &lt;span class="n"&gt;Hat&lt;/span&gt; &lt;span class="mf"&gt;4.8&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="n"&gt;linux2&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; 
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;[{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;b&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;}]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="p"&gt;[{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;b&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;c&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;}]&lt;/span&gt;
&lt;span class="bp"&gt;True&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Python&lt;/span&gt; &lt;span class="mf"&gt;3.5&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Apr&lt;/span&gt; &lt;span class="mi"&gt;27&lt;/span&gt; &lt;span class="mi"&gt;2016&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mo"&gt;04&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;21&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;56&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;GCC&lt;/span&gt; &lt;span class="mf"&gt;4.8&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="mi"&gt;20140911&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Red&lt;/span&gt; &lt;span class="n"&gt;Hat&lt;/span&gt; &lt;span class="mf"&gt;4.8&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="n"&gt;linux&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;[{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;b&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;}]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="p"&gt;[{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;b&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;c&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;}]&lt;/span&gt;
&lt;span class="n"&gt;Traceback&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;most&lt;/span&gt; &lt;span class="n"&gt;recent&lt;/span&gt; &lt;span class="n"&gt;call&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
  &lt;span class="n"&gt;File&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&amp;lt;stdin&amp;gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;module&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="ne"&gt;TypeError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;unorderable&lt;/span&gt; &lt;span class="n"&gt;types&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The problem is that the second elements in both lists have different keys
and Python doesn't know how to compare them. In earlier Python versions
this has been special cased as
&lt;a href="http://stackoverflow.com/questions/3484293/is-there-a-description-of-how-cmp-works-for-dict-objects-in-python-2/3484456#3484456"&gt;described here&lt;/a&gt;
by Ned Batchelder (the author of Python's coverage tool) but in Python 3
dictionaries have no natural sort order.&lt;/p&gt;
&lt;p&gt;In the case of &lt;em&gt;django-chartit&lt;/em&gt; (of which I'm now the official maintainer) this
bug triggers when you want to plot data from multiple sources (models) on the same
chart. In this case the fields coming from each data series are different and the
above error is triggered.&lt;/p&gt;
&lt;p&gt;I have worked around this in
&lt;a href="https://github.com/chartit/django-chartit/commit/9d9033ecd5a8592a12872293cdf6d710cebf894f"&gt;commit 9d9033e&lt;/a&gt;
by simply disabling an iterator sort but this is sub-optimal and I'm not quite certain
what the side effect might be. I suspect you may end up with a chart where the order
of values on the X axis isn't the same for the different models, e.g. one graph plotting
the data in ascending order the other one in descending.&lt;/p&gt;
&lt;p&gt;The trouble also comes from the fact that we're sorting an iterator (a list of fields) by
telling Python to use a list of dicts to determine the sort order. In this arrangement
there is no way to tell Python how we want to compare our dicts. The only solution I
can think about is creating a custom class and implementing a custom &lt;code&gt;__cmp__()&lt;/code&gt; method
for this data structure!&lt;/p&gt;</summary><category term="QA"></category><category term="Python"></category></entry><entry><title>PhantomJS 2.1.1 in Ubuntu different from upstream</title><link href="http://atodorov.org/blog/2016/07/23/phantomjs-211-in-ubuntu-different-from-upstream/" rel="alternate"></link><updated>2016-07-23T11:30:00+03:00</updated><author><name>Alexander Todorov</name></author><id>tag:atodorov.org,2016-07-23:blog/2016/07/23/phantomjs-211-in-ubuntu-different-from-upstream/</id><summary type="html">&lt;p&gt;For some time now I've been hitting
&lt;a href="https://github.com/ariya/phantomjs/issues/12506"&gt;PhantomJS #12506&lt;/a&gt; with the
latest 2.1.1 version. The problem is supposedly fixed in 2.1.0 but this is not
always the case. If you use a .deb package from the latest Ubuntu then the problem
still exists, see
&lt;a href="https://bugs.launchpad.net/ubuntu/+source/phantomjs/+bug/1605628"&gt;Ubuntu #1605628&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It turns out the root cause of this, and probably other problems, is the way
PhantomJS packages are built. Ubuntu builds the package against their stock
Qt5WebKit libraries which leads to&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;ldd usr/lib/phantomjs/phantomjs &lt;span class="p"&gt;|&lt;/span&gt; grep -i qt
    libQt5WebKitWidgets.so.5 &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; /lib64/libQt5WebKitWidgets.so.5 &lt;span class="o"&gt;(&lt;/span&gt;0x00007f5173ebf000&lt;span class="o"&gt;)&lt;/span&gt;
    libQt5PrintSupport.so.5 &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; /lib64/libQt5PrintSupport.so.5 &lt;span class="o"&gt;(&lt;/span&gt;0x00007f5173e4d000&lt;span class="o"&gt;)&lt;/span&gt;
    libQt5Widgets.so.5 &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; /lib64/libQt5Widgets.so.5 &lt;span class="o"&gt;(&lt;/span&gt;0x00007f51737b6000&lt;span class="o"&gt;)&lt;/span&gt;
    libQt5WebKit.so.5 &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; /lib64/libQt5WebKit.so.5 &lt;span class="o"&gt;(&lt;/span&gt;0x00007f5171342000&lt;span class="o"&gt;)&lt;/span&gt;
    libQt5Gui.so.5 &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; /lib64/libQt5Gui.so.5 &lt;span class="o"&gt;(&lt;/span&gt;0x00007f5170df8000&lt;span class="o"&gt;)&lt;/span&gt;
    libQt5Network.so.5 &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; /lib64/libQt5Network.so.5 &lt;span class="o"&gt;(&lt;/span&gt;0x00007f5170c9a000&lt;span class="o"&gt;)&lt;/span&gt;
    libQt5Core.so.5 &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; /lib64/libQt5Core.so.5 &lt;span class="o"&gt;(&lt;/span&gt;0x00007f517080d000&lt;span class="o"&gt;)&lt;/span&gt;
    libQt5Sensors.so.5 &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; /lib64/libQt5Sensors.so.5 &lt;span class="o"&gt;(&lt;/span&gt;0x00007f516b218000&lt;span class="o"&gt;)&lt;/span&gt;
    libQt5Positioning.so.5 &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; /lib64/libQt5Positioning.so.5 &lt;span class="o"&gt;(&lt;/span&gt;0x00007f516b1d7000&lt;span class="o"&gt;)&lt;/span&gt;
    libQt5OpenGL.so.5 &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; /lib64/libQt5OpenGL.so.5 &lt;span class="o"&gt;(&lt;/span&gt;0x00007f516b17c000&lt;span class="o"&gt;)&lt;/span&gt;
    libQt5Sql.so.5 &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; /lib64/libQt5Sql.so.5 &lt;span class="o"&gt;(&lt;/span&gt;0x00007f516b136000&lt;span class="o"&gt;)&lt;/span&gt;
    libQt5Quick.so.5 &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; /lib64/libQt5Quick.so.5 &lt;span class="o"&gt;(&lt;/span&gt;0x00007f5169dad000&lt;span class="o"&gt;)&lt;/span&gt;
    libQt5Qml.so.5 &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; /lib64/libQt5Qml.so.5 &lt;span class="o"&gt;(&lt;/span&gt;0x00007f5169999000&lt;span class="o"&gt;)&lt;/span&gt;
    libQt5WebChannel.so.5 &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; /lib64/libQt5WebChannel.so.5 &lt;span class="o"&gt;(&lt;/span&gt;0x00007f5169978000&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;While building from the upstream sources gives&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;ldd /tmp/bin/phantomjs &lt;span class="p"&gt;|&lt;/span&gt; grep -i qt
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If you take a closer look at PhantomJS's sources you will notice there are
3 git submodules in their repository - &lt;code&gt;3rdparty&lt;/code&gt;, &lt;code&gt;qtbase&lt;/code&gt; and &lt;code&gt;qtwebkit&lt;/code&gt;.
Then in their &lt;code&gt;build.py&lt;/code&gt; you can clearly see that this local fork of &lt;code&gt;QtWebKit&lt;/code&gt;
is built first, then the &lt;code&gt;phantomjs&lt;/code&gt; binary is built against it.&lt;/p&gt;
&lt;p&gt;The problem is that these custom forks include additional patches to make
WebKit suitable for Phantom's needs. And these patches are not available
in the stock WebKit library that Ubuntu uses.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Yes, that's correct. We need additional functionality that
vanilla QtWebKit doesn't have. That's why we use custom version.&lt;/p&gt;
&lt;p&gt;Vitaly Slobodin, PhantomJS&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;At the moment of this writing Vitaly's qtwebkit fork is 28 commits ahead and
39 commits behind qt:dev. I'm surprised Ubuntu's PhantomJS even works.&lt;/p&gt;
&lt;p&gt;The solution IMO is to bundle the additional sources into the src.deb package
and use the same building procedure as upstream.&lt;/p&gt;</summary><category term="QA"></category><category term="fedora.planet"></category></entry><entry><title>On Python Infinite Loops</title><link href="http://atodorov.org/blog/2016/07/15/on-python-infinite-loops/" rel="alternate"></link><updated>2016-07-15T09:30:00+03:00</updated><author><name>Alexander Todorov</name></author><id>tag:atodorov.org,2016-07-15:blog/2016/07/15/on-python-infinite-loops/</id><summary type="html">&lt;p&gt;How do you write an endless loop without using &lt;code&gt;True&lt;/code&gt;, &lt;code&gt;False&lt;/code&gt;, number constants
and comparison operators in Python ?&lt;/p&gt;
&lt;p&gt;I've been working on the mutation test tool
&lt;a href="https://github.com/sixty-north/cosmic-ray"&gt;Cosmic Ray&lt;/a&gt; and discovered that it
was missing a boolean replacement operator, that is an operator which will switch
&lt;code&gt;True&lt;/code&gt; to &lt;code&gt;False&lt;/code&gt; and vice versa, so I wrote one. I've also added some tests to
Cosmic Ray's test suite and then I hit the infinite loop problem.
CR's test suite contains the following code inside a module called &lt;code&gt;adam.py&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;break&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The test suite executes mutations on &lt;code&gt;adam.py&lt;/code&gt; and then runs some tests which
it expects to fail. During execution one of the mutations is
&lt;code&gt;replace break with continue&lt;/code&gt; which makes the above loop infinite. The test suite
times out after a while and kills the mutation. Everything fails as expected and
we're good.&lt;/p&gt;
&lt;p&gt;Adding my boolean replacement operator broke this function. All of the other mutations
work as expected but then the loop becomes&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;break&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;When we test this particular mutation there is no infinite loop so Cosmic Ray's
test suite doesn't time out like it should and an error is reported.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;job ID 25:Outcome.SURVIVED:adam
command: cosmic-ray worker adam boolean_replacer 2 unittest -- tests
&lt;span class="gd"&gt;--- mutation diff ---&lt;/span&gt;
&lt;span class="gd"&gt;--- a/home/travis/build/MrSenko/cosmic-ray/test_project/adam.py&lt;/span&gt;
&lt;span class="gi"&gt;+++ b/home/travis/build/MrSenko/cosmic-ray/test_project/adam.py&lt;/span&gt;
&lt;span class="gu"&gt;@@ -32,6 +32,6 @@&lt;/span&gt;
     return x
 
 def trigger_infinite_loop():
&lt;span class="gd"&gt;-    while True:&lt;/span&gt;
&lt;span class="gi"&gt;+    while False:&lt;/span&gt;
         break
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So the question becomes how to write the loop condition in such a way that nothing
will mutate it but it will still remain true so that when &lt;code&gt;break&lt;/code&gt; becomes &lt;code&gt;continue&lt;/code&gt;
this piece of code will become an infinite loop ? Using &lt;code&gt;True&lt;/code&gt; or &lt;code&gt;False&lt;/code&gt; constants
obviously is a no go. Same goes for numeric constants, e.g. &lt;code&gt;1&lt;/code&gt; or comparison
operators like &lt;code&gt;&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;&lt;/code&gt;, &lt;code&gt;is&lt;/code&gt;, &lt;code&gt;not&lt;/code&gt;, etc. - all of them will be mutated and will
break the loop condition.&lt;/p&gt;
&lt;p&gt;So I took a look at the docs for
&lt;a href="https://docs.python.org/2.4/lib/truth.html"&gt;truth value testing&lt;/a&gt; and discovered
&lt;a href="https://github.com/sixty-north/cosmic-ray/pull/155"&gt;my solution&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;break&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I'm creating an object instance here which will not be mutated by any of the
existing mutation operators.&lt;/p&gt;
&lt;p&gt;Thanks for reading and happy testing!&lt;/p&gt;</summary><category term="QA"></category><category term="Python"></category></entry><entry><title>Bug in TuxCon Website</title><link href="http://atodorov.org/blog/2016/07/13/bug-in-tuxcon-website/" rel="alternate"></link><updated>2016-07-13T14:10:00+03:00</updated><author><name>Alexander Todorov</name></author><id>tag:atodorov.org,2016-07-13:blog/2016/07/13/bug-in-tuxcon-website/</id><summary type="html">&lt;p&gt;&lt;img alt="TuxCon bug" src="/images/bugs/tuxcon_bug.png" title="TuxCon bug" /&gt;&lt;/p&gt;
&lt;p&gt;Here comes July 9th 2016 and the start of &lt;a href="http://tuxcon.mobi"&gt;TuxCon&lt;/a&gt; ...
with a bug on their website! The image above is taken during the first
talk of the conference. Obviously the count down timer is completely off.&lt;/p&gt;
&lt;p&gt;In
&lt;a href="https://github.com/TuxCon/tuxcon-website/blob/master/js/init.js#L100"&gt;init.js:100&lt;/a&gt;
there is this piece of code&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;finalDate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;2016/07/09&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="nx"&gt;$&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;div#counter&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;countdown&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;finalDate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;on&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;update.countdown&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;event&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nx"&gt;$&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;html&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;event&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;strftime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;lt;span&amp;gt;%D &amp;lt;em&amp;gt;days&amp;lt;/em&amp;gt;&amp;lt;/span&amp;gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
                                &lt;span class="s1"&gt;&amp;#39;&amp;lt;span&amp;gt;%H &amp;lt;em&amp;gt;hours&amp;lt;/em&amp;gt;&amp;lt;/span&amp;gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
                                &lt;span class="s1"&gt;&amp;#39;&amp;lt;span&amp;gt;%M &amp;lt;em&amp;gt;minutes&amp;lt;/em&amp;gt;&amp;lt;/span&amp;gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
                                &lt;span class="s1"&gt;&amp;#39;&amp;lt;span&amp;gt;%S &amp;lt;em&amp;gt;seconds&amp;lt;/em&amp;gt;&amp;lt;/span&amp;gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;span class="p"&gt;});&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It counts backwards and updates the HTML until &lt;code&gt;finalDate&lt;/code&gt; is reached. Then
the HTML is no longer updated and the default values are shown, which in
this case are non zero. A simple
&lt;a href="https://github.com/TuxCon/tuxcon-website/pull/1"&gt;patch&lt;/a&gt; fixes the problem.&lt;/p&gt;
&lt;p&gt;Initialize your variables properly and happy testing!&lt;/p&gt;</summary><category term="QA"></category></entry><entry><title>Testing the 8-bit computer Puldin</title><link href="http://atodorov.org/blog/2016/07/12/testing-the-8-bit-computer-puldin/" rel="alternate"></link><updated>2016-07-12T14:10:00+03:00</updated><author><name>Alexander Todorov</name></author><id>tag:atodorov.org,2016-07-12:blog/2016/07/12/testing-the-8-bit-computer-puldin/</id><summary type="html">&lt;p&gt;&lt;img alt="Puldin creators" src="/images/puldin_creators.jpg" title="Puldin creators" /&gt;&lt;/p&gt;
&lt;p&gt;Last weekend I visited &lt;a href="http://tuxcon.mobi"&gt;TuxCon&lt;/a&gt; in Plovdiv and
was very happy to meet and talk to some of the creators of the Puldin
computer! On the picture above are (left to right) Dimitar Georgiev
- wrote the text editor, Ivo Nenov - BIOS, DOS and core OS developer,
Nedyalko Todorov - director of the vendor company and Orlin Shopov -
BIOS, DOS, compiler and core OS developer.&lt;/p&gt;
&lt;p&gt;Puldin is 100% pure Bulgarian development, while the “Pravetz” brand
was copy of Apple ][ (Pravetz 8A, 8C, 8M), Oric (Pravets 8D) and IBM-PC
(Pravetz 16). The Puldin computers were build from scratch both hardware
and software and were produced in Plovdiv in the late 80s and early 90s.
50000 pieces were made, at least 35000 of them have been exported to Russia
and paid for. A typical configuration in a Russian class room consisted of
several Puldin computers and a single Pravetz 16. According to Russian sources
the last usage of these computers was in 2003 serving as Linux terminals
and being maintained without any support from the vendor
(b/c it ceased to exist).&lt;/p&gt;
&lt;p&gt;&lt;img alt="Puldin 601" src="/images/puldin601.jpg" title="Puldin 601" /&gt;&lt;/p&gt;
&lt;p&gt;One of the main objectives of Puldin was full compatibility with IBM-PC.
At the time IBM had been releasing extensive documentation about how their
software and hardware works which has been used by Puldin's creators as
their software specs. Despite IBM-PC using faster CPU the Puldin 601 had
a comparable performance due to aggressive software and compiler optimizations.&lt;/p&gt;
&lt;p&gt;Testing wise the guys used to compare Puldin's functionality with that of IBM-PC.
It was a hard requirement to have full compatibility on the file storage layer,
that means floppy disks written on Puldin had to be readable on IBM-PC and vice
versa. Same goes for programs compiled on Puldin - they had to execute on IBM-PC.&lt;/p&gt;
&lt;p&gt;Everything of course had been tested manually and on top of that all the software
had to be burned to ROM before you can do anything with it. As you can imagine the
testing process had been quite slow and painful compared to today's standards.
I've asked the guys if they'd happened to find a bug in IBM-PC which wasn't present
in their code but they couldn't remember.&lt;/p&gt;
&lt;p&gt;What was interesting for me on the hardware side was the fact that you can plug
the computer directly to a cheap TV set and that it's been one of the first computers
which could operate on 12V DC, powered directly from a car battery.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Pravetz 8" src="/images/pravetz16.jpg" title="Pravetz 8" /&gt;&lt;/p&gt;
&lt;p&gt;There was also a fully functional Pravetz 8 with an additional VGA port to
connect it to the LCD monitor as well as a SD card reader wired to function as a
floppy disk reader (the small black dot behind the joystick).&lt;/p&gt;
&lt;p&gt;For those who missed it (and understand Bulgarian) I have a
&lt;a href="https://www.youtube.com/watch?v=uFGnrqa2RSY&amp;amp;list=PLFjlI7p-h1hxBP3cIjEqePSeoBDHud5Db&amp;amp;index=1"&gt;video recording on YouTube&lt;/a&gt;.
For more info about the history and the hardware please check-out
&lt;a href="https://olimex.wordpress.com/2015/01/12/retro-computer-puldin-the-only-bulgarian-8-bit-computer-developed-from-scratch/"&gt;Olimex post on Puldin&lt;/a&gt;
(in English). For more info on Puldin and Pravetz please visit
&lt;a href="http://pyldin.info"&gt;pyldin.info&lt;/a&gt; (in Russian) and
&lt;a href="http://pravetz8.com"&gt;pravetz8.com&lt;/a&gt; (in Bulgarian)
using Google translate if need be.&lt;/p&gt;</summary><category term="QA"></category><category term="fedora.planet"></category><category term="events"></category></entry><entry><title>Testing Data Structures in Pykickstart</title><link href="http://atodorov.org/blog/2016/07/06/testing-data-structures-in-pykickstart/" rel="alternate"></link><updated>2016-07-06T15:10:00+03:00</updated><author><name>Alexander Todorov</name></author><id>tag:atodorov.org,2016-07-06:blog/2016/07/06/testing-data-structures-in-pykickstart/</id><summary type="html">&lt;p&gt;When designing automated test cases we often think either about increasing
coverage or in terms of testing more use-cases aka behavior scenarios. Both
are valid approaches to improve testing and both of them seem to focus
around execution control flow (or business logic). However program behavior
is sometimes controlled via the contents of its data structures and this is
something which is rarely tested. &lt;/p&gt;
&lt;p&gt;In
&lt;a href="https://github.com/rhinstaller/pykickstart/pull/26#discussion_r32790705"&gt;this comment&lt;/a&gt;
Brian C. Lane and Vratislav Podzimek from Red Hat are talking about a data structure
which maps Fedora versions to particular implementations of kickstart commands.
For example&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;RHEL7Handler&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BaseHandler&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;version&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RHEL7&lt;/span&gt;

    &lt;span class="n"&gt;commandMap&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s"&gt;&amp;quot;auth&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;commands&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;authconfig&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FC3_Authconfig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s"&gt;&amp;quot;authconfig&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;commands&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;authconfig&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FC3_Authconfig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s"&gt;&amp;quot;autopart&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;commands&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;autopart&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;F20_AutoPart&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s"&gt;&amp;quot;autostep&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;commands&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;autostep&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FC3_AutoStep&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s"&gt;&amp;quot;bootloader&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;commands&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bootloader&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;RHEL7_Bootloader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In their particular case the Fedora 21 &lt;code&gt;logvol&lt;/code&gt; implementation introduced the
&lt;code&gt;--profile&lt;/code&gt; parameter but in
Fedora 22 and Fedora 23 the &lt;code&gt;logvol&lt;/code&gt; command mapped to the Fedora 20 implementation and the
&lt;code&gt;--profile&lt;/code&gt; parameter wasn't available. This is unexpected change in program behavior
although the &lt;code&gt;logvol.py&lt;/code&gt; and &lt;code&gt;handlers/f22.py&lt;/code&gt; files have
&lt;a href="https://github.com/atodorov/pykickstart-coverage/blob/master/coverage-report.log"&gt;99% and 100% code coverage&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This morning I did some coding and created an automated test for this problem. The test
iterates over all command maps. For each command in the map (that is data structure member)
we load the module which provides all possible implementations for that command. In the
loaded module
we search for implementations which have newer versions than what is in the map,
but not newer than the current Fedora version under test. With a little bit of pruning
the current list of offenses is&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;ERROR&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="n"&gt;handlers&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;devel&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;py&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;fcoe&amp;quot;&lt;/span&gt; &lt;span class="n"&gt;command&lt;/span&gt; &lt;span class="n"&gt;maps&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;F13_Fcoe&amp;quot;&lt;/span&gt; &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt;
&lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="n"&gt;pykickstart&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;commands&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;fcoe&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt; &lt;span class="n"&gt;there&lt;/span&gt; &lt;span class="k"&gt;is&lt;/span&gt; &lt;span class="n"&gt;newer&lt;/span&gt; &lt;span class="n"&gt;implementation&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;RHEL7_Fcoe&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;

&lt;span class="n"&gt;ERROR&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="n"&gt;handlers&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;devel&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;py&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;FcoeData&amp;quot;&lt;/span&gt; &lt;span class="n"&gt;maps&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;F13_FcoeData&amp;quot;&lt;/span&gt; &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt;
&lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="n"&gt;pykickstart&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;commands&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;fcoe&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt; &lt;span class="n"&gt;there&lt;/span&gt; &lt;span class="k"&gt;is&lt;/span&gt; &lt;span class="n"&gt;newer&lt;/span&gt; &lt;span class="n"&gt;implementation&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;RHEL7_FcoeData&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;

&lt;span class="n"&gt;ERROR&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="n"&gt;handlers&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;devel&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;py&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;user&amp;quot;&lt;/span&gt; &lt;span class="n"&gt;command&lt;/span&gt; &lt;span class="n"&gt;maps&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;F19_User&amp;quot;&lt;/span&gt; &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt;
&lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="n"&gt;pykickstart&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;commands&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;user&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt; &lt;span class="n"&gt;there&lt;/span&gt; &lt;span class="k"&gt;is&lt;/span&gt; &lt;span class="n"&gt;newer&lt;/span&gt; &lt;span class="n"&gt;implementation&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;F24_User&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;

&lt;span class="n"&gt;ERROR&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="n"&gt;handlers&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;f24&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;py&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;user&amp;quot;&lt;/span&gt; &lt;span class="n"&gt;command&lt;/span&gt; &lt;span class="n"&gt;maps&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;F19_User&amp;quot;&lt;/span&gt; &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt;
&lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="n"&gt;pykickstart&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;commands&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;user&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt; &lt;span class="n"&gt;there&lt;/span&gt; &lt;span class="k"&gt;is&lt;/span&gt; &lt;span class="n"&gt;newer&lt;/span&gt; &lt;span class="n"&gt;implementation&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;F24_User&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;

&lt;span class="n"&gt;ERROR&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="n"&gt;handlers&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;f22&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;py&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;logvol&amp;quot;&lt;/span&gt; &lt;span class="n"&gt;command&lt;/span&gt; &lt;span class="n"&gt;maps&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;F20_LogVol&amp;quot;&lt;/span&gt; &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt;
&lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="n"&gt;pykickstart&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;commands&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;logvol&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt; &lt;span class="n"&gt;there&lt;/span&gt; &lt;span class="k"&gt;is&lt;/span&gt; &lt;span class="n"&gt;newer&lt;/span&gt; &lt;span class="n"&gt;implementation&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;F21_LogVol&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;

&lt;span class="n"&gt;ERROR&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="n"&gt;handlers&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;f22&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;py&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;LogVolData&amp;quot;&lt;/span&gt; &lt;span class="n"&gt;maps&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;F20_LogVolData&amp;quot;&lt;/span&gt; &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt;
&lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="n"&gt;pykickstart&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;commands&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;logvol&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt; &lt;span class="n"&gt;there&lt;/span&gt; &lt;span class="k"&gt;is&lt;/span&gt; &lt;span class="n"&gt;newer&lt;/span&gt; &lt;span class="n"&gt;implementation&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;F21_LogVolData&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;

&lt;span class="n"&gt;ERROR&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="n"&gt;handlers&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;f18&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;py&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;network&amp;quot;&lt;/span&gt; &lt;span class="n"&gt;command&lt;/span&gt; &lt;span class="n"&gt;maps&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;F16_Network&amp;quot;&lt;/span&gt; &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt;
&lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="n"&gt;pykickstart&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;commands&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;network&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt; &lt;span class="n"&gt;there&lt;/span&gt; &lt;span class="k"&gt;is&lt;/span&gt; &lt;span class="n"&gt;newer&lt;/span&gt; &lt;span class="n"&gt;implementation&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;F18_Network&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The first two are possibly false negatives or related to the naming conventions used
in this module. However the rest appear to be legitimate problems. The &lt;code&gt;user&lt;/code&gt; command
has introduced the &lt;code&gt;--groups&lt;/code&gt; parameter in Fedora 24 (devel is Fedora 25 currently) but the
parser will fail to recognize this parameter. The &lt;code&gt;logvol&lt;/code&gt; problem is recognized as well
since it was never patched. And the Fedora 18 &lt;code&gt;network&lt;/code&gt; command implements a new property called
&lt;code&gt;hostname&lt;/code&gt; which has probably never been available to be used.&lt;/p&gt;
&lt;p&gt;You can follow my current work in
&lt;a href="https://github.com/rhinstaller/pykickstart/pull/91"&gt;PR #91&lt;/a&gt; and happy testing your
data structures.&lt;/p&gt;</summary><category term="QA"></category><category term="fedora.planet"></category></entry><entry><title>Don't Upgrade Galaxy S5 to Android 6.0</title><link href="http://atodorov.org/blog/2016/06/09/dont-upgrade-galaxy-s5-to-android-60/" rel="alternate"></link><updated>2016-06-09T00:10:00+03:00</updated><author><name>Alexander Todorov</name></author><id>tag:atodorov.org,2016-06-09:blog/2016/06/09/dont-upgrade-galaxy-s5-to-android-60/</id><summary type="html">&lt;p&gt;Samsung is shipping out buggy software like a boss, no doubt about it.
I've written a bit about &lt;a href="http://atodorov.org/blog/categories/samsung/"&gt;their bugs&lt;/a&gt; previously.
However I didn't expect them to release Android 6.0.1 and render my
&lt;a href="http://amzn.to/1tiiqze"&gt;Galaxy S5&lt;/a&gt; completely useless with respect
to the feature I use the most.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Lockscreen" src="/images/samsung/lockscreen_bug.jpg" title="Lockscreen" /&gt;&lt;/p&gt;
&lt;h2&gt;Tell me the weather for Brussels&lt;/h2&gt;
&lt;p&gt;So on Monday I've let Android upgrade to 6.0.1 to be completely surprised
that the lockscreen shows the weather report for Brussels, while
I'm based in Sofia. I've checked &lt;em&gt;AccuWeather&lt;/em&gt; (I did go to
&lt;a href="http://atodorov.org/blog/2016/02/02/fosdem-2016-report/"&gt;Brussels earlier this year&lt;/a&gt;)
but it displayed only Sofia and Thessaloniki. To get rid of this widget
go to &lt;code&gt;Settings -&amp;gt; Lockscreen -&amp;gt; Additional information&lt;/code&gt; and turn it off!&lt;/p&gt;
&lt;p&gt;I think this weather report comes from GPS/Location based data, which I
have turned off by default but did use a while back ago. After turning
the widget off and back on it didn't appear on the lockscreen. I suspect
they fall back to showing the last good known value when data is missing
instead of handling the error properly.&lt;/p&gt;
&lt;h2&gt;Apps are gone&lt;/h2&gt;
&lt;p&gt;Some of my installed apps are missing now. So far I've noticed that the
&lt;em&gt;Gallery&lt;/em&gt; and &lt;em&gt;S Health&lt;/em&gt; icons have disappeared from my homescreen. I think
&lt;em&gt;S Health&lt;/em&gt; came from Samsung's app store but still they shouldn't have removed
it silently. Now I wonder what happened to my data.&lt;/p&gt;
&lt;p&gt;I don't see why &lt;em&gt;Gallery&lt;/em&gt; was removed though. The only way to view pictures
is to use the camera app preview functionality which is kind of grose.&lt;/p&gt;
&lt;h2&gt;Grayscale in powersafe mode is gone&lt;/h2&gt;
&lt;p&gt;The killer feature on these higher end Galaxy devices is the &lt;em&gt;Powersafe mode&lt;/em&gt;
and &lt;em&gt;Ultra Powersafe mode&lt;/em&gt;. I use them a lot and by default have my phone in
&lt;em&gt;Powersafe mode&lt;/em&gt; with grayscale colors enabled. It is easier on the eyes and
also safes your battery.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; grayscale colors don't affect some displays but these devices use
AMOLED screens which need different amounts of power to display different
colors. More black means less power required!&lt;/p&gt;
&lt;p&gt;After the upgrade grayscale is no more. There's not even an on/off switch.
I've managed to find a workaround though. First you need to enable developer mode
by tapping 7 times on &lt;code&gt;About device -&amp;gt; Build number&lt;/code&gt;. Then go to
&lt;code&gt;Settings -&amp;gt; Developer options&lt;/code&gt;, look for the &lt;code&gt;Hardware Accelerated Rendering&lt;/code&gt;
section and select &lt;code&gt;Simulate Color Space -&amp;gt; Monochromacy&lt;/code&gt;! This is a bit ugly
hack and doesn't have the convenience of turning colors on/off by tapping
the quick Powersafe mode button at the top of the screen!&lt;/p&gt;
&lt;p&gt;It looks like Samsung didn't think this upgrade well enough or didn't test it
well enough ? In my line of work (installation and upgrade testing) I've rarely
seen such a big blunder. Thanks for reading and happy testing!&lt;/p&gt;</summary><category term="QA"></category><category term="fedora.planet"></category><category term="Samsung"></category></entry><entry><title>How To Hire Software Testers, Pt. 3</title><link href="http://atodorov.org/blog/2016/06/03/how-to-hire-software-testers-pt-3/" rel="alternate"></link><updated>2016-06-03T11:15:00+03:00</updated><author><name>Alexander Todorov</name></author><id>tag:atodorov.org,2016-06-03:blog/2016/06/03/how-to-hire-software-testers-pt-3/</id><summary type="html">&lt;p&gt;In previous posts (links below) I have described my process of interviewing
QA candidates. Today I'm quoting an excerpt from the book
&lt;a href="http://it-interviews.com/"&gt;Mission: My IT career&lt;/a&gt;(Bulgarian only) by Ivaylo Hristov,
one of &lt;a href="http://komfo.com"&gt;Komfo&lt;/a&gt;'s co-founders.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Fedora pen" src="/images/fedora/pen.png" title="Fedora pen" /&gt;&lt;/p&gt;
&lt;p&gt;He writes&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;Probably the most important personal trait of a QA engineer is to
be able to think outside given boundaries and prejudices
(about software that is). When necessary to be non-conventional and
apply different approaches to the problems being solved. This will help
them find defect which nobody else will notice.

Most often errors/mistakes in software development are made due to
wrong expectations or wrong assumptions. Very often this happens because
developers hope their software will be used in one particular way
(as it was designed to) or that a particular set of data will be returned.
Thus the skill to think outside the box is the most important skill
we (as employers) are looking to find in a QA candidate. At job interviews
you can expect to be given tasks and questions which examine those skills.
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;How would you test a pen?&lt;/h2&gt;
&lt;p&gt;This is Ivaylo's favorite question for QA candidates. He's looking for
attention to details and knowing when to stop testing. Some of the possible
answers related to core functionality are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Does the pen write in the correct color&lt;/li&gt;
&lt;li&gt;Does the color fades over time&lt;/li&gt;
&lt;li&gt;Does the pen operate normally at various temperatures? What temperature
  intervals would you choose for testing&lt;/li&gt;
&lt;li&gt;Does the pen operate normally at various atmospheric pressure&lt;/li&gt;
&lt;li&gt;When writing, does the pen leave excessive ink&lt;/li&gt;
&lt;li&gt;When writing, do you get a continuous line or not&lt;/li&gt;
&lt;li&gt;What pressure does the user need to apply in order to write a
  continuous line&lt;/li&gt;
&lt;li&gt;What surfaces can the pen write on? What surfaces would you test&lt;/li&gt;
&lt;li&gt;Are you able to write on a piece of paper if there is something soft underneath&lt;/li&gt;
&lt;li&gt;What is the maximum inclination angle at which the pen is able to write
  without problems&lt;/li&gt;
&lt;li&gt;Does the ink dry fast&lt;/li&gt;
&lt;li&gt;If we spill different liquids onto a sheet of paper, on which we had written
  something, does the ink stay intact or smear&lt;/li&gt;
&lt;li&gt;Can you use pencil rubber to erase the ink? What else would you test&lt;/li&gt;
&lt;li&gt;How long can you write before we run out of ink&lt;/li&gt;
&lt;li&gt;How fat is the ink line&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then Ivaylo gives a few more non-obvious answers&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Verify that all labels on the pen/ink cartridge are correctly spelled
  and how durable they are (try to erase them)&lt;/li&gt;
&lt;li&gt;Strength test - what is the maximum height you can drop the pen from
  without breaking it&lt;/li&gt;
&lt;li&gt;Verify that dimensions are correct&lt;/li&gt;
&lt;li&gt;Test if the pen keeps writing after not being used for some time
  (how long)&lt;/li&gt;
&lt;li&gt;Testing individual pen components under different temperature and
  atmospheric conditions&lt;/li&gt;
&lt;li&gt;Verify that materials used to make the pen are safe, e.g. when you put
  the pen in your mouth&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When should you stop ? According to the book there can be between 50 and 100
test cases for a single pen, maybe more. It's not a good sign if you stop at
the first 3!&lt;/p&gt;
&lt;p&gt;If you want to know what skills are revealed via these questions please
read my other posts on the topic:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://atodorov.org/blog/2016/04/12/how-to-hire-software-testers-pt-1/"&gt;The login form question&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://atodorov.org/blog/2016/04/16/how-to-hire-software-testers-pt-2/"&gt;How do you test a Sudoku solver&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thanks for reading and happy testing!&lt;/p&gt;</summary><category term="QA"></category><category term="fedora.planet"></category></entry><entry><title>Capybara's find().click doesn't always fire onClick</title><link href="http://atodorov.org/blog/2016/05/31/capybaras-findclick-doesnt-always-fire-onclick/" rel="alternate"></link><updated>2016-05-31T14:01:00+03:00</updated><author><name>Alexander Todorov</name></author><id>tag:atodorov.org,2016-05-31:blog/2016/05/31/capybaras-findclick-doesnt-always-fire-onclick/</id><summary type="html">&lt;p&gt;Recently I've observed a strange behavior in one of the test suites I'm
working with - a test which submits a web form appeared to fail at a rate
between 10% and 30%. This immediately made me think there is some kind of
race-condition but it turned out that Capybara's &lt;code&gt;find().click&lt;/code&gt; method
doesn't always fire the onClick event in the browser!&lt;/p&gt;
&lt;p&gt;The test suite uses Capybara, Poltergeist and PhantomJS. The element we
click on is an image, coupled to a hidden check-box underneath. When the image
is clicked onClick is fired and the check-box is updated accordingly. In the
failed cases the underlying check-box wasn't updated!
Searching the web reveals a similar problem described by
&lt;a href="http://aokolish.me/blog/2012/01/22/testing-hover-events-with-capybara/"&gt;Alex Okolish&lt;/a&gt;
so we've tried his solution:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;div&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;.replacement&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;visible&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kp"&gt;true&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;trigger&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;:click&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;How to Test&lt;/h2&gt;
&lt;p&gt;The failure behavior being somewhat flaky I've opted for running the
test multiple times and see what happens when it fails. Initially I've
executed the test in batches of 10 and 20 repetitions to get a feeling
of how often does it fail before proceeding with debugging. Debugging
was done by logging variables and state on the console and repeating
multiple times. Once a possible solution was
proposed we've run the tests in batches of 100 repetitions and counted
how often they failed.&lt;/p&gt;
&lt;p&gt;At the end, when Alex's solution was discovered we've repeated the
testing around 1000 times just to make sure it works reliably.
So far this has been working without issues!&lt;/p&gt;
&lt;p&gt;I've spent around a week working on this together with a co-worker
and we didn't really want to spend more time trying to figure out
what was going wrong with our tools. Once we saw that &lt;code&gt;trigger&lt;/code&gt; does
the job we didn't continue debugging Capybara or PhantomJS.&lt;/p&gt;</summary><category term="QA"></category><category term="Ruby"></category></entry><entry><title>Changing Rails consider_all_requests_local in RSpec fails</title><link href="http://atodorov.org/blog/2016/04/27/changing-rails-consider_all_requests_local-in-rspec-fails/" rel="alternate"></link><updated>2016-04-27T15:30:00+03:00</updated><author><name>Alexander Todorov</name></author><id>tag:atodorov.org,2016-04-27:blog/2016/04/27/changing-rails-consider_all_requests_local-in-rspec-fails/</id><summary type="html">&lt;p&gt;As many others I've been trying to change
&lt;code&gt;Rails.application.config.consider_all_requests_local&lt;/code&gt; and
&lt;code&gt;Rails.application.config.action_dispatch.show_exceptions&lt;/code&gt; inside my
RSpec tests in order to test custom error pages in a Rails app. However this
doesn't work. My code looked like this&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;feature&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Exceptions&amp;#39;&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt;
  &lt;span class="n"&gt;before&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt;
    &lt;span class="no"&gt;Rails&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;application&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;action_dispatch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show_exceptions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;true&lt;/span&gt;
    &lt;span class="no"&gt;Rails&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;application&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;consider_all_requests_local&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kp"&gt;false&lt;/span&gt;
  &lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This works only if I execute &lt;code&gt;exceptions_spec.rb&lt;/code&gt; alone. However when something
else executes before that it fails. The config values are
correctly updated but that doesn't seem to have effect.&lt;/p&gt;
&lt;p&gt;The answer and solution comes from
&lt;a href="http://thepugautomatic.com/2014/08/404-with-rails-4/#comment-1714338343"&gt;Henrik N&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;action_dispatch.show_exceptions gets copied and cached in
Rails.application.env_config, so even if you change
Rails.application.config.action_dispatch.show_exceptions in this
before block the value isn't what you expect when it's used in
ActionDispatch::DebugExceptions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In fact &lt;code&gt;DebugExceptions&lt;/code&gt; uses &lt;code&gt;env['action_dispatch.show_exceptions']&lt;/code&gt;. The
correct code should look like this&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;before&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt;
  &lt;span class="nb"&gt;method&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="no"&gt;Rails&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;application&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;method&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;:env_config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;expect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="no"&gt;Rails&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;application&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;receive&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;:env_config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;with&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;no_args&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt;
    &lt;span class="nb"&gt;method&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;call&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;merge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
      &lt;span class="s1"&gt;&amp;#39;action_dispatch.show_exceptions&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="kp"&gt;true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="s1"&gt;&amp;#39;action_dispatch.show_detailed_exceptions&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="kp"&gt;false&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="s1"&gt;&amp;#39;consider_all_requests_local&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="kp"&gt;false&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;end&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This allows the test to work regardless of the order of execution
of spec files. I don't know why but I also had to leave
&lt;code&gt;show_detailed_exceptions&lt;/code&gt; otherwise I wasn't getting the desired results.&lt;/p&gt;</summary><category term="QA"></category><category term="Ruby"></category></entry><entry><title>Mismatch in Pyparted Interfaces</title><link href="http://atodorov.org/blog/2016/04/26/mismatch-in-pyparted-interfaces/" rel="alternate"></link><updated>2016-04-26T10:50:00+03:00</updated><author><name>Alexander Todorov</name></author><id>tag:atodorov.org,2016-04-26:blog/2016/04/26/mismatch-in-pyparted-interfaces/</id><summary type="html">&lt;p&gt;Last week my co-worker Marek Hruscak, from Red Hat, found an interesting case of
mismatch between the two interfaces provided by pyparted. In this article
I'm going to give an example, using simplified code and explain what is
happening. From pyparted's documentation we learn the following&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;pyparted is a set of native Python bindings for libparted.  libparted is the
library portion of the GNU parted project.  With pyparted, you can write
applications that interact with disk partition tables and filesystems.&lt;/p&gt;
&lt;p&gt;The Python bindings are implemented in two layers.  Since libparted itself
is written in C without any real implementation of objects, a simple 1:1
mapping of externally accessible libparted functions was written.  This
mapping is provided in the _ped Python module.  You can use that module if
you want to, but it's really just meant for the larger parted module.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;_ped&lt;/span&gt;       &lt;span class="nx"&gt;libparted&lt;/span&gt; &lt;span class="nx"&gt;Python&lt;/span&gt; &lt;span class="nx"&gt;bindings&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;direct&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;mapping&lt;/span&gt;
&lt;span class="nx"&gt;parted&lt;/span&gt;     &lt;span class="nx"&gt;Native&lt;/span&gt; &lt;span class="nx"&gt;Python&lt;/span&gt; &lt;span class="nx"&gt;code&lt;/span&gt; &lt;span class="nx"&gt;building&lt;/span&gt; &lt;span class="nx"&gt;on&lt;/span&gt; &lt;span class="nx"&gt;_ped&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;complete&lt;/span&gt; &lt;span class="kd"&gt;with&lt;/span&gt; &lt;span class="nx"&gt;classes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
           &lt;span class="nx"&gt;exceptions&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;and&lt;/span&gt; &lt;span class="nx"&gt;advanced&lt;/span&gt; &lt;span class="nx"&gt;functionality&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/blockquote&gt;
&lt;p&gt;The two interfaces are the &lt;code&gt;_ped&lt;/code&gt; and &lt;code&gt;parted&lt;/code&gt; modules. As a user I expect them
to behave exactly the same but they don't. For example some partition properties
are read-only in libparted and &lt;code&gt;_ped&lt;/code&gt; but not in &lt;code&gt;parted&lt;/code&gt;. This is the mismatch
I'm talking about.&lt;/p&gt;
&lt;p&gt;Consider the following tests (also available on
&lt;a href="https://github.com/atodorov/pyparted/tree/not_read_only_demo"&gt;GitHub&lt;/a&gt;)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="gh"&gt;diff --git a/tests/baseclass.py b/tests/baseclass.py&lt;/span&gt;
&lt;span class="gh"&gt;index 4f48b87..30ffc11 100644&lt;/span&gt;
&lt;span class="gd"&gt;--- a/tests/baseclass.py&lt;/span&gt;
&lt;span class="gi"&gt;+++ b/tests/baseclass.py&lt;/span&gt;
&lt;span class="gu"&gt;@@ -168,6 +168,12 @@ class RequiresPartition(RequiresDisk):&lt;/span&gt;
         self._part = _ped.Partition(disk=self._disk, type=_ped.PARTITION_NORMAL,
         self._part = _ped.Partition(disk=self._disk, type=_ped.PARTITION_NORMAL,
                                     start=0, end=100, fs_type=_ped.file_system_type_get(&amp;quot;ext2&amp;quot;))
 
&lt;span class="gi"&gt;+        geom = parted.Geometry(self.device, start=100, length=100)&lt;/span&gt;
&lt;span class="gi"&gt;+        fs = parted.FileSystem(type=&amp;#39;ext2&amp;#39;, geometry=geom)&lt;/span&gt;
&lt;span class="gi"&gt;+        self.part = parted.Partition(disk=self.disk, type=parted.PARTITION_NORMAL,&lt;/span&gt;
&lt;span class="gi"&gt;+                                    geometry=geom, fs=fs)&lt;/span&gt;
&lt;span class="gi"&gt;+&lt;/span&gt;
&lt;span class="gi"&gt;+&lt;/span&gt;
 # Base class for any test case that requires a hash table of all
 # _ped.DiskType objects available
 class RequiresDiskTypes(unittest.TestCase):
&lt;span class="gh"&gt;diff --git a/tests/test__ped_partition.py b/tests/test__ped_partition.py&lt;/span&gt;
&lt;span class="gh"&gt;index 7ef049a..26449b4 100755&lt;/span&gt;
&lt;span class="gd"&gt;--- a/tests/test__ped_partition.py&lt;/span&gt;
&lt;span class="gi"&gt;+++ b/tests/test__ped_partition.py&lt;/span&gt;
&lt;span class="gu"&gt;@@ -62,8 +62,10 @@ class PartitionGetSetTestCase(RequiresPartition):&lt;/span&gt;
         self.assertRaises(exn, setattr, self._part, &amp;quot;num&amp;quot;, 1)
         self.assertRaises(exn, setattr, self._part, &amp;quot;fs_type&amp;quot;,
             _ped.file_system_type_get(&amp;quot;fat32&amp;quot;))
&lt;span class="gd"&gt;-        self.assertRaises(exn, setattr, self._part, &amp;quot;geom&amp;quot;,&lt;/span&gt;
&lt;span class="gd"&gt;-                                     _ped.Geometry(self._device, 10, 20))&lt;/span&gt;
&lt;span class="gi"&gt;+        with self.assertRaises((AttributeError, TypeError)):&lt;/span&gt;
&lt;span class="gi"&gt;+#            setattr(self._part, &amp;quot;geom&amp;quot;, _ped.Geometry(self._device, 10, 20))&lt;/span&gt;
&lt;span class="gi"&gt;+            self._part.geom = _ped.Geometry(self._device, 10, 20)&lt;/span&gt;
&lt;span class="gi"&gt;+&lt;/span&gt;
         self.assertRaises(exn, setattr, self._part, &amp;quot;disk&amp;quot;, self._disk)
 
         # Check that values have the right type.
&lt;span class="gh"&gt;diff --git a/tests/test_parted_partition.py b/tests/test_parted_partition.py&lt;/span&gt;
&lt;span class="gh"&gt;index 0a406a0..8d8d0fd 100755&lt;/span&gt;
&lt;span class="gd"&gt;--- a/tests/test_parted_partition.py&lt;/span&gt;
&lt;span class="gi"&gt;+++ b/tests/test_parted_partition.py&lt;/span&gt;
&lt;span class="gu"&gt;@@ -23,7 +23,7 @@&lt;/span&gt;
 import parted
 import unittest
 
&lt;span class="gd"&gt;-from tests.baseclass import RequiresDisk&lt;/span&gt;
&lt;span class="gi"&gt;+from tests.baseclass import RequiresDisk, RequiresPartition&lt;/span&gt;
 
 # One class per method, multiple tests per class.  For these simple methods,
 # that seems like good organization.  More complicated methods may require
&lt;span class="gu"&gt;@@ -34,11 +34,11 @@ class PartitionNewTestCase(unittest.TestCase):&lt;/span&gt;
         # TODO
         self.fail(&amp;quot;Unimplemented test case.&amp;quot;)
 
&lt;span class="gd"&gt;-@unittest.skip(&amp;quot;Unimplemented test case.&amp;quot;)&lt;/span&gt;
&lt;span class="gd"&gt;-class PartitionGetSetTestCase(unittest.TestCase):&lt;/span&gt;
&lt;span class="gi"&gt;+class PartitionGetSetTestCase(RequiresPartition):&lt;/span&gt;
     def runTest(self):
&lt;span class="gd"&gt;-        # TODO&lt;/span&gt;
&lt;span class="gd"&gt;-        self.fail(&amp;quot;Unimplemented test case.&amp;quot;)&lt;/span&gt;
&lt;span class="gi"&gt;+        with self.assertRaises((AttributeError, TypeError)):&lt;/span&gt;
&lt;span class="gi"&gt;+            #setattr(self.part, &amp;quot;geometry&amp;quot;, parted.Geometry(self.device, start=10, length=20))&lt;/span&gt;
&lt;span class="gi"&gt;+            self.part.geometry = parted.Geometry(self.device, start=10, length=20)&lt;/span&gt;
 
 @unittest.skip(&amp;quot;Unimplemented test case.&amp;quot;)
 class PartitionGetFlagTestCase(unittest.TestCase):
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The test in &lt;code&gt;test__ped_partition.py&lt;/code&gt; works without problems, I've modified it for
visual reference only. This was also the inspiration behind the test in
&lt;code&gt;test_parted_partition.py&lt;/code&gt;. However the second test fails with&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;======================================================================
FAIL: runTest (tests.test_parted_partition.PartitionGetSetTestCase)
----------------------------------------------------------------------
Traceback (most recent call last):
  File &amp;quot;/tmp/pyparted/tests/test_parted_partition.py&amp;quot;, line 41, in runTest
    self.part.geometry = parted.Geometry(self.device, start=10, length=20)
AssertionError: (&amp;lt;type &amp;#39;exceptions.AttributeError&amp;#39;&amp;gt;, &amp;lt;type &amp;#39;exceptions.TypeError&amp;#39;&amp;gt;) not raised

----------------------------------------------------------------------
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now it's clear that something isn't quite the same between the two interfaces.
If we look at &lt;code&gt;src/parted/partition.py&lt;/code&gt; we see the following snippet&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;137     fileSystem = property(lambda s: s._fileSystem, lambda s, v: setattr(s, &amp;quot;_fileSystem&amp;quot;, v))
138     geometry = property(lambda s: s._geometry, lambda s, v: setattr(s, &amp;quot;_geometry&amp;quot;, v))
139     system = property(lambda s: s.__writeOnly(&amp;quot;system&amp;quot;), lambda s, v: s.__partition.set_system(v))
140     type = property(lambda s: s.__partition.type, lambda s, v: setattr(s.__partition, &amp;quot;type&amp;quot;, v))
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The geometry property is indeed read-write but the system property is write-only.
&lt;code&gt;git blame&lt;/code&gt; leads us to the interesting
&lt;a href="https://github.com/rhinstaller/pyparted/commit/2fc0ee2b"&gt;commit 2fc0ee2b&lt;/a&gt;, which changes
definitions for quite a few properties and removes the &lt;code&gt;_readOnly&lt;/code&gt; method which raises
an exception. Even more interesting is the fact that the &lt;code&gt;Partition.geometry&lt;/code&gt; property
hasn't been changed. If you look closer you will notice that the deleted definition and
the new one are exactly the same. Looks like the problem existed even before this change.&lt;/p&gt;
&lt;p&gt;Digging down even further we find
&lt;a href="https://github.com/rhinstaller/pyparted/commit/7599aa1ae505f3784ca4936b58b38b8dffb752ff"&gt;commit 7599aa1&lt;/a&gt;
which is the very first implementation of the &lt;code&gt;parted&lt;/code&gt; module. There you can see the
&lt;code&gt;_readOnly&lt;/code&gt; method and some properties like &lt;code&gt;path&lt;/code&gt; and &lt;code&gt;disk&lt;/code&gt; correctly marked as such
but &lt;code&gt;geometry&lt;/code&gt; isn't.&lt;/p&gt;
&lt;p&gt;Shortly after this commit the first test was added (4b9de0e) and a bit
later the second, empty test class, was added (c85a5e6). This only goes
to show that every piece of software needs appropriate QA coverage, which
pyparted was kind of lacking (and I'm trying to change that).&lt;/p&gt;
&lt;p&gt;The reason this bug went unnoticed for so long
is the limited exposure of pyparted. To my knowledge anaconda, the Fedora installer
is its biggest (if not single) consumer and maybe it uses only the &lt;code&gt;_ped&lt;/code&gt;
interface (I didn't check) or it doesn't try to do silly things like setting
a value to a read-only property.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;
The lesson from this story is to test all of your interfaces and also
make sure they are behaving in exactly the same manner!
&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Thanks for reading and happy testing!&lt;/p&gt;</summary><category term="QA"></category><category term="Python"></category><category term="fedora.planet"></category></entry><entry><title>Capybara's within() Altering expect(page) Scope</title><link href="http://atodorov.org/blog/2016/04/24/capybaras-within-altering-expectpage-scope/" rel="alternate"></link><updated>2016-04-24T23:50:00+03:00</updated><author><name>Alexander Todorov</name></author><id>tag:atodorov.org,2016-04-24:blog/2016/04/24/capybaras-within-altering-expectpage-scope/</id><summary type="html">&lt;p&gt;&lt;strong&gt;
When making assertions inside a &lt;code&gt;within&lt;/code&gt; block the assertion scope
is limited to the element selected by the &lt;em&gt;within()&lt;/em&gt; function, although
it looks like you are asserting on the entire page!
&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;scenario&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Pressing Escape closes autocomplete popup&amp;#39;&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt;
  &lt;span class="n"&gt;within&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;#new-broadcast&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt;
    &lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;#broadcast_field&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Hello &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="n"&gt;start_typing_name&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;#broadcast_field&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;@Bret&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="c1"&gt;# will fail below&lt;/span&gt;
      &lt;span class="n"&gt;expect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;page&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;have_selector&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;.ui-autocomplete&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
      &lt;span class="n"&gt;send_keys&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;#broadcast_field&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;:escape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;end&lt;/span&gt;
  &lt;span class="n"&gt;expect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;page&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;have_no_selector&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;.ui-autocomplete&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The above code failed the first &lt;code&gt;expect()&lt;/code&gt; and it took me some time before
I figured it out. Capybara's test suite itself gives you the answer&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;it&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;should assert content in the given scope&amp;quot;&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt;
  &lt;span class="vi"&gt;@session&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;within&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;:css&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;#for_foo&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt;
    &lt;span class="n"&gt;expect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="vi"&gt;@session&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;not_to&lt;/span&gt; &lt;span class="n"&gt;have_content&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;First Name&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="k"&gt;end&lt;/span&gt;
  &lt;span class="n"&gt;expect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="vi"&gt;@session&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;have_content&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;First Name&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So know your frameworks and happy testing.&lt;/p&gt;</summary><category term="QA"></category><category term="Ruby"></category></entry><entry><title>3 Bugs in Grajdanite</title><link href="http://atodorov.org/blog/2016/04/17/3-bugs-in-grajdanite/" rel="alternate"></link><updated>2016-04-17T10:34:00+03:00</updated><author><name>Alexander Todorov</name></author><id>tag:atodorov.org,2016-04-17:blog/2016/04/17/3-bugs-in-grajdanite/</id><summary type="html">&lt;p&gt;&lt;a href="https://play.google.com/store/apps/details?id=com.xevica.grajdanite"&gt;Grajdanite&lt;/a&gt;
is a social app that allows everyone (in Bulgaria) to photograph vehicles in breach of
traffic rules or misbehaving drivers, upload them online and ask them to
appologize. They also offer some functionality to report offenses to the
authorities are are partnering with local municipalities and law enforcement
agencies to make the process easier. And of course this is one of my
favorite apps as of latest.&lt;/p&gt;
&lt;h2&gt;Missing Icon in My Profile&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Missing icon" src="/images/grajdanite/bug1.png" title="Missing icon bug" /&gt;&lt;/p&gt;
&lt;p&gt;The more offenses you report the more points you get.
Points lead to ranks (e.g. junior officer, senior officer, etc).
The page showing your points and rank is missing an icon. If I had to guess
this is the badge which comes with different ranks.&lt;/p&gt;
&lt;h2&gt;Preloading the Very First Form Value&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Preloading gone wrong" src="/images/grajdanite/bug2.png" title="Preloading gone wrong" /&gt;&lt;/p&gt;
&lt;p&gt;Once you opt for reporting an offence to the authorities you need to specify
the address where the action took place, your name, phone and e-mail address.
The app correctly saves your details and pre-loads them later to speed-up
data entry. However I typed my e-mail wrong the very first time. Now every time
I want to report something the app pre-loads the wrong address. Even after I
change it to the correct one, the next time I still see the very first, wrong value.&lt;/p&gt;
&lt;p&gt;In code this is probably something like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;# pre-load
form.email = store.get(&amp;quot;email&amp;quot;, &amp;quot;&amp;quot;)
form.show()

# save
if form.firstTime():
    store.save(&amp;quot;email&amp;quot;, form.email)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The fix is to save the form value every time (not expensive operation here)
or check if the current value is different from the last time and only then save it.&lt;/p&gt;
&lt;h2&gt;DST and Time Sync&lt;/h2&gt;
&lt;p&gt;The last bug is in the app confirmation email. Once an offence is reported the
user receives an email with the uploaded photo and the information they have
provided. The email includes a timestamp. However the email timestamp is 
1 hour off from the actual time. In particular it is 1 hour behind the current time
and I think the email server doesn't follow summer time.&lt;/p&gt;
&lt;p&gt;The result from this is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Report an offense&lt;/li&gt;
&lt;li&gt;Wait 1 minute for the email to be received;&lt;/li&gt;
&lt;li&gt;The email says the offense happened 1 hour ago!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All of these bugs are in version 3.86.3, which is the latest one.&lt;/p&gt;</summary><category term="QA"></category></entry><entry><title>How To Hire Software Testers, Pt. 2</title><link href="http://atodorov.org/blog/2016/04/16/how-to-hire-software-testers-pt-2/" rel="alternate"></link><updated>2016-04-16T10:34:00+03:00</updated><author><name>Alexander Todorov</name></author><id>tag:atodorov.org,2016-04-16:blog/2016/04/16/how-to-hire-software-testers-pt-2/</id><summary type="html">&lt;p&gt;In my
&lt;a href="http://atodorov.org/blog/2016/04/12/how-to-hire-software-testers-pt-1/"&gt;previous post&lt;/a&gt; I have
described the process I follow when interviewing candidates for a QA position.
The first question is designed to expose the applicant's way of thinking.
My second question is designed to examine their technical understanding
and to a lesser extent their way of thinking.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Sudoku" src="/images/sudoku.png" title="Sudoku" /&gt;&lt;/p&gt;
&lt;h2&gt;How do You Test a Sudoku Solving Function&lt;/h2&gt;
&lt;p&gt;You have implementation of a sudoku solver function with the following pseudocode:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;func Sudoku(Array[2]) {
    ...
    return Array[2]
}
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;The function solves a sudoku puzzle;&lt;/li&gt;
&lt;li&gt;Input parameter is a two-dimensional array with the known numbers (from 1 to 9) in the Sudoku grid;&lt;/li&gt;
&lt;li&gt;The output is a two-dimensional array with the numbers from the solved puzzle.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You have 10 minutes to write down a list of all test cases you can think of!&lt;/p&gt;
&lt;h2&gt;Behind The Scenes&lt;/h2&gt;
&lt;p&gt;One set of possible tests is to examine the input and figure out if the
function has been passed valid data.
In the real-world programs interact with each other, they are not alone.
Sometimes it happens that a valid output from one program isn't a
valid input for the next one. Also we have malicious users who will try to
break the program.&lt;/p&gt;
&lt;p&gt;If a person manages to test for this case then
I know they have a bit more clue about how software is used in the real-world.
This also touches a bit on white-box testing, where the tester has full info
about the software under test. In this example the implementation is
intentionally left blank.&lt;/p&gt;
&lt;p&gt;OTOH I've seen answers where the applicant blindly assumes that the input
is 1-9, because the spec says so, and excludes the entire input testing from
their scope. I classify this answer as immediate failure, because a tester should
never assume anything and test to verify their initial conditions are indeed
as stated in the documentation.&lt;/p&gt;
&lt;p&gt;Another set of possible tests is to verify the correct work of the function.
That is to verify the proposed Sudoku solution is indeed following the rules
of the game. This is what we usually refer to black-box testing. The tester
doesn't know how the SUT works internally, they only know the input data and
the expected output. &lt;/p&gt;
&lt;p&gt;If a person fails to describe at least one such test case
they have essentially failed the question. What is the point of a SUT which
doesn't crash (suppose that all previous tests passed) but doesn't
produce the desired correct result ?&lt;/p&gt;
&lt;p&gt;Then there are test cases related to the environment in which this Sudoku
solver function operates. This is where I examine the creativity of the person,
their familiarity with other platforms and to some extent their thinking out of
the box. Is the Sudoku solver iterative or recursive ? What if we're on an
embedded system and recursion is too heavy for it ? How much power does the
function require, how fast it works, etc.&lt;/p&gt;
&lt;p&gt;A person that provides at least one answer in this category has bonus points
over the others who didn't. IMO it is very important for a tester to have
experience with various platforms and environments because this helps them
see edge cases which others will not be able to see. I also consider a strong
plus if the person shows they can operate outside their comfort zone.&lt;/p&gt;
&lt;p&gt;If we have time I may ask the applicant to write the tests using a programming
language they know. This is to verify their coding and automation skills.&lt;/p&gt;
&lt;p&gt;OTOH having the tests as code will show me how much the person knows about testing
vs. coding. I've seen solutions where people write a for loop, looping over all
numbers from 1 to 100 and testing if they are a valid input to &lt;code&gt;Sudoku()&lt;/code&gt;.
Obviously this is pointless and they failed the test.&lt;/p&gt;
&lt;p&gt;Last but not least, the question asks for testing a particular Sudoku solver
implementation. I expect the answers to be designed around the given function.
However I've seen answers designed around a
&lt;a href="http://sudoku-solutions.com/"&gt;Sudoku solver website&lt;/a&gt; or described as
intermediate states in an interactive Sudoku game (e.g. wrong answers shown in red).
I consider these invalid because the question is to test a particular
given function, not anything Sudoku related. If you do this in real-life that
means you are not testing the SUT directly but maybe touching it indirectly
(at best). This is not what a QA job is about.&lt;/p&gt;
&lt;h2&gt;What Are The Correct Answers&lt;/h2&gt;
&lt;p&gt;Here are some of the possible tests.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Test with single dimensional input array - we expect an error;&lt;/li&gt;
&lt;li&gt;Test with 3 dimensional input array - we expect an error;&lt;/li&gt;
&lt;li&gt;Then proceed testing with 2 dimensional array;&lt;/li&gt;
&lt;li&gt;Test with number less than 1 (usually 0) - expect error;&lt;/li&gt;
&lt;li&gt;Test with number greater than 9 (usually 10) - expect error;&lt;/li&gt;
&lt;li&gt;Test how the function handles non-numerical data - chars &amp;amp; symbols
(essentially the same thing for our function);&lt;/li&gt;
&lt;li&gt;Test with strings which actually represent a number, e.g. "1";&lt;/li&gt;
&lt;li&gt;Test with floating point numbers, e.g. 1.0, 2.0, 3.0 - may or may not
work depending on how the code is written;&lt;/li&gt;
&lt;li&gt;If floating point numbers are accepted, then test with a different locale.
Is "1.0" the same as "1,0";&lt;/li&gt;
&lt;li&gt;Test with &lt;code&gt;null&lt;/code&gt;, &lt;code&gt;nil&lt;/code&gt;, &lt;code&gt;None&lt;/code&gt; (whatever the language supports) -
this should be a valid value for unknown numbers and not cause a crash;&lt;/li&gt;
&lt;li&gt;Test if the function validates that the provided input follows the
Sudoku rules by passing it duplicate numbers in one row, column or
square. It should produce an error;&lt;/li&gt;
&lt;li&gt;Test if the input data contains the minimum number of givens, 17
for a general Sudoku, so that a solution can be found. Otherwise the function
may go into an
&lt;a href="http://atodorov.org/blog/2015/01/05/endless-loop-bug-candy-crush-saga-level-80/"&gt;endless loop&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;Verify the proposed solution conforms to Sudoku rules;&lt;/li&gt;
&lt;li&gt;Test with a fully solved puzzle as input - output should be exactly the same;&lt;/li&gt;
&lt;li&gt;If on mobile, measure battery consumption for 1 minute of operation. I've
seen a game which uses 1% battery power for 1 minute of game play;&lt;/li&gt;
&lt;li&gt;Test for buffer overflows;&lt;/li&gt;
&lt;li&gt;Test for speed of execution (performance);&lt;/li&gt;
&lt;li&gt;Test performance on single and multiple (core) CPUs - depending on the
language and how the function is written this may produce a difference or not;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I'm sure I'm missing something so please use the comments below to tell me your suggestions.&lt;/p&gt;
&lt;p&gt;Thanks for reading and happy testing!&lt;/p&gt;</summary><category term="QA"></category><category term="fedora.planet"></category></entry><entry><title>How To Hire Software Testers, Pt. 1</title><link href="http://atodorov.org/blog/2016/04/12/how-to-hire-software-testers-pt-1/" rel="alternate"></link><updated>2016-04-12T12:34:00+03:00</updated><author><name>Alexander Todorov</name></author><id>tag:atodorov.org,2016-04-12:blog/2016/04/12/how-to-hire-software-testers-pt-1/</id><summary type="html">&lt;p&gt;Many people have asked me how do I make sure a person who applies for a
QA/software tester position is a good fit ? On the opposite side people
have asked online how do they give correct answers on test related questions
at job interviews. I have two general questions to help me decide if a
person knows about testing and if they are a good fit for the team or not.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Login form" src="/images/login_form_wireframe.jpg" title="Login form" /&gt;&lt;/p&gt;
&lt;h2&gt;How do You Test a Login Form&lt;/h2&gt;
&lt;p&gt;You are given the login form above and the following constraints:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Log in is possible with username and password or through the social networks;&lt;/li&gt;
&lt;li&gt;After successful registration an email with the following content is sent to the user:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;Helo and welcome to atodorov.org! Click _here_ to confirm your emeil address.&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;You have 10 minutes to write down a list of all test cases you can think of!&lt;/p&gt;
&lt;h2&gt;Behind The Scenes&lt;/h2&gt;
&lt;p&gt;The question looks trivial but isn't as easy to answer as you may think.
If you haven't spent the last 20 years of your life in a cave, chances are
that you will give technically correct answers but this is not the only
thing I'm looking for.&lt;/p&gt;
&lt;p&gt;The question is designed to simulate a real-world scenario, where the QA
person is given a piece of software, or requirements document and tasked with
creating a test plan for it. The question is intentionally vague because that's
how real-world works, most often testers don't have all the requirements and
specifications available beforehand.&lt;/p&gt;
&lt;p&gt;The time constrain, especially when the interview is performed in person,
simulates work under pressure - get the job done as soon as possible.&lt;/p&gt;
&lt;p&gt;While I review the answers I'm trying to figure out how does the person think,
not how much about technology they know. I'm trying to figure out what are their
strong areas and where they need to improve. IMO being able to think as a tester
and having attention to details, being able to easily spot corner cases and
look at the problem from different angles is much more important than
technical knowledge in a particular domain.&lt;/p&gt;
&lt;p&gt;As long as a person is suited to think like a tester they can learn to
apply their critical thinking to any software under test and use various
testing techniques to discover or safeguard against problems.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A person that answers quickly and intuitively is better than a person who
takes a long time to figure out what to test. I can see they are active
thinkers and can work without micro-management and hand-holding.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A person that goes on and on
describing different test cases is better than one who limits themselves to
the most obvious cases. I can see they have an exploratory passion, which
is the key to finding many bugs and making the software better;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A person that goes to explore the system in breadth is better than one who
keeps banging on the same test case with more and more variations. I can see
they are noticing the various aspects of the software (e.g. social login,
email confirmation, etc) but also to some extent, not investing all of their
resources (the remaining time to answer) into a single direction. Also in
real-world testing, testing the crap out of something is useful up to a point.
Afterwards we don't really see any significant value from additional testing
efforts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A person that is quick to figure out one or two corner cases is better
than a person who can't. This tells me they are thinking about what goes
on under the hood and trying to predict unpredictable behavior - for example
what happens if you try to register with already registered username or email?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A person that asks questions in order to minimize uncertainty and vagueness
is better than the one who doesn't. In real-world if the tester doesn't know
something they have to ask. Quite often even developers and product managers
don't know the answer. Then how are we developing software if we don't know
what it is supposed to do ?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If given more time (writing interview), a person that organizes their answers
into steps (1, 2, 3) is a bit better than one who simply throws at you random
answers without context. Similar thought applies to people who write down their
test pre-conditions before writing down scenarios. From this I can see that
the person is well organized and will have no trouble writing detailed test cases,
with pre-conditions, steps to execute and expected results. This is what QAs do.
Also we have the, sometimes tedious, task of organizing all test results into
a test case management system (aka test book) for further reference.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The question intentionally includes some mistakes. In this example 2 spelling
errors in the email text. Whoever manages to spot them and tell me about it is
better than others who don't spot the errors or assume that's how it is. QAs job
is to always question everything and never blindly trust that the state of the
system is the way it is. Also simple errors like typos can be
&lt;a href="http://atodorov.org/blog/2016/01/15/tesla-needs-more-qa/"&gt;embarrassing&lt;/a&gt; or generate
unnecessary support calls.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bravo if you tested not only the outgoing email but also social login. This
shows attention to details, not to mention social is 1/3rd of our example system.
It also shows that QA's job doesn't end with testing
the core, perceived functionality of the system. QA tests everything,
even interactions with external systems if that is necessary.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;What Are The Correct Answers&lt;/h2&gt;
&lt;p&gt;I will document some of the possible answers as I recall them from memory.
I will update the list with other interesting answers given by students who
applied to my
&lt;a href="https://github.com/HackBulgaria/QA-and-Automation-101"&gt;QA and Automation 101&lt;/a&gt;
course, answering this very same question.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Test if users can register using valid username, email and password;&lt;/li&gt;
&lt;li&gt;Test if SUT gives an error message when email or password (or username)
format doesn't follow a particular format (e.g. no special symbols);&lt;/li&gt;
&lt;li&gt;After registration, test that the user can login successfully;&lt;/li&gt;
&lt;li&gt;Depending on requirements test if the user can login before they have
confirmed their email address;&lt;/li&gt;
&lt;li&gt;Test that upon registration a confirmation email is actually sent;&lt;/li&gt;
&lt;li&gt;Spell-check the email text;&lt;/li&gt;
&lt;li&gt;Test if the &lt;em&gt;click here&lt;/em&gt; piece of text is a hyperlink;&lt;/li&gt;
&lt;li&gt;Verify that when clicked, the hyperlink successfully confirmed email/activates the account
(depending on what confirmed/activated means per requirements);&lt;/li&gt;
&lt;li&gt;Test what happens if the link is clicked a second time;&lt;/li&gt;
&lt;li&gt;Test what happens if the link is clicked after 24 or 48 hrs;&lt;/li&gt;
&lt;li&gt;Test that the social network icons, actually link to the desired SN and
not someplace else;&lt;/li&gt;
&lt;li&gt;Test if new user accounts can be created via all specified social networks;&lt;/li&gt;
&lt;li&gt;Test what happens if there is an existing user, who registered with a password
and they (or somebody else) tries to register via social with an account that has
the same email address, aka account hijacking;&lt;/li&gt;
&lt;li&gt;Same as previous test but try to register a new user, using email address that
was previously used with social login;&lt;/li&gt;
&lt;li&gt;Test what happens if users forget their password - intentionally we don't have
the '[] Forgot my password' checkbox. This is both usability feature and missing
requirements;&lt;/li&gt;
&lt;li&gt;Test for simple SQL injections like
&lt;a href="http://php.net/manual/en/images/fa7c5b5f326e3c4a6cc9db19e7edbaf0-xkcd-bobby-tables.png"&gt;Bobby Tables&lt;/a&gt;.
&lt;em&gt;btw I was given this image as an answer which scored high on the geek-o-meter&lt;/em&gt;;&lt;/li&gt;
&lt;li&gt;Test for XSS - &lt;a href="https://twitter.com/dergeruhn/status/476764918763749376"&gt;Tweetdeck didn't&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;Test if non-activated/non-confirmed usernames expire after some time and can be used again;&lt;/li&gt;
&lt;li&gt;Test of fields tab order - something I haven't done in 15 or more years
but still valid and I've seen sites getting it wrong quite often;&lt;/li&gt;
&lt;li&gt;When trying to login test what happens when username/password is wrong or empty;&lt;/li&gt;
&lt;li&gt;Test if email is required for login - this isn't clear from the requirements
so it is a valid answer. Better answer is to clarify that;&lt;/li&gt;
&lt;li&gt;Test if username/email or password is case sensitive. Valid test and indeed I
recently saw a problem where upon registration users entered their emails using
some capital letters but they were lower-cased before saving to the DB. Later this
broke a piece of code which forgot to apply the lowercase on the input data. The code
was handling account reactivation;&lt;/li&gt;
&lt;li&gt;Test if the password field shows the actual password or not. I haven't seen this
in person but I'm certain there is some site which maybe used CSS and nice images
instead of the default ugly password field and that didn't work on all browsers;&lt;/li&gt;
&lt;li&gt;Test if you can copy&amp;amp;paste the masked password, probably trying to steal somebody
else's password. Last time I saw this was on early Windows 95 with the modem connection
dialog. Very briefly it allowed you to copy the text from the field and paste it into
Notepad to reveal the actual password;&lt;/li&gt;
&lt;li&gt;If we're on mobile (intentionally not specified) test for buffer overflows;
Actually test that everywhere and see what happens;&lt;/li&gt;
&lt;li&gt;Test if the social network buttons use the same action verb. In the example
we have &lt;em&gt;Log in&lt;/em&gt;, &lt;em&gt;Connect&lt;/em&gt; and &lt;em&gt;Sign in&lt;/em&gt;. This is sort of usability testing
and helping have a unified look and feel of the product;&lt;/li&gt;
&lt;li&gt;Test which of the &lt;em&gt;Log in&lt;/em&gt; and &lt;em&gt;Sign up&lt;/em&gt; tabs is active at the moment.
The example is intentionally left to look like a wireframe but it is important
for the user to easily tell where they are. Otherwise they'll call support or
even worse, simply give up on us;&lt;/li&gt;
&lt;li&gt;Test if all static files (images) will load if they are deployed onto CDN.
Not surprisingly I've seen &lt;a href="https://github.com/Nitrate/Nitrate/pull/80"&gt;this bug&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;In case we have a "[] Remember me" checkbox, test if it actually remembers
the user credentials. Yesterday I saw this same functionality not working on
a specialized desktop app in the corner case where you supply a different
connection endpoint (server) instead of the ones already provided. The user
defined value is accepted but not saved automatically;&lt;/li&gt;
&lt;li&gt;Test if the "Remember me" functionality actually saves your last credentials
or only the first ones you provided. There is a similar bug in 
&lt;a href="https://play.google.com/store/apps/details?id=com.xevica.grajdanite"&gt;Grajdanite&lt;/a&gt;,
where once you enter a wrong email, it is remembered and every time the form is
pre-filled with the previous value (which is wrong). I'm yet to report it though;&lt;/li&gt;
&lt;li&gt;Cross-browser testing - hmm, login and registration should work on all browsers
you say. It's not browser dependent, is it? Well yeah, login isn't browser dependent
unless we did something stupid like pre-handling the form submit via non-cross-platform
JavaScript or even
&lt;a href="https://github.com/gilsondev/pelican-clean-blog/pull/6"&gt;accidentally doing so&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;Test with Unicode characters, especially non Latin ones. It's been many years
since we had Unicode but quite a few apps haven't learned how to deal with Unicode
text properly.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I'm certain there are more answers and I will update the list as I figure them out.
You can always post in the comments and tell me something I've missed.&lt;/p&gt;
&lt;h2&gt;How to Pass The Job Interview&lt;/h2&gt;
&lt;p&gt;This is a question I often see on &lt;a href="https://www.quora.com/profile/Alexander-Todorov"&gt;Quora&lt;/a&gt;.
&lt;em&gt;I have a job interview tomorrow. How do I test a login form (or whatever) ?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;If this section is what you're after I suspect you are a junior or wanna-be
software tester. As you've seen the interviewer isn't really interested in what you know already,
at least not as much. We're interested in getting to know how you
think in the course of 30-60 minutes.&lt;/p&gt;
&lt;p&gt;If you ever find yourself being asked a similar question just start thinking and
answering and don't stop. Vocalize your thoughts, even if you don't know what will
happen when testing a certain condition. Then keep going on and on. Look at the problem
from all angles, explain how you'd test various aspects and features of the SUT.
Then move on to the next bit. Always think about what you may have forgotten and
revisit your answers - this is what real QAs do - learn from mistakes.
Ask questions, don't ever assume anything. If something is unclear ask to be
clarified. For example I've seen a person who doesn't use social networks and
didn't know how social login/registration worked. They did good by asking me to
describe how that works.&lt;/p&gt;
&lt;p&gt;Your goal is to make the interviewer ask you to stop answering. Then tell them
a few more answers.&lt;/p&gt;
&lt;p&gt;However beware of cheating. You may cheat a little bit by
saying you will test this and that or design scenarios you have no clue about.
Maybe you read them in my blog or elsewhere. If the interviewer knows their job
(which they should) they will instantly ask you another question to verify what
you say. Don't forget the interviewer is probably an experienced tester and validating
assumptions is what they do every day.&lt;/p&gt;
&lt;p&gt;For example, if you told me something about security testing or SQL injection
or XSS I will ask you to explain that in more details. If you forgot to mention,
one of them, say XSS but only heard about SQL injection I will ask you about the
other one. This will immediately tell me if you have a clue what you are talking
about.&lt;/p&gt;
&lt;p&gt;Feel free to send me suggestions and answers in the comments below. You can find
the second part of this post at
&lt;a href="http://atodorov.org/blog/2016/04/16/how-to-hire-software-testers-pt-2/"&gt;How do you test a Sudoku solving function&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thanks for reading and happy testing!&lt;/p&gt;</summary><category term="QA"></category><category term="fedora.planet"></category></entry><entry><title>Time Calculation bug in BlackBerry Z10</title><link href="http://atodorov.org/blog/2016/03/31/time-calculation-bug-in-blackberry-z10/" rel="alternate"></link><updated>2016-03-31T16:34:00+03:00</updated><author><name>Alexander Todorov</name></author><id>tag:atodorov.org,2016-03-31:blog/2016/03/31/time-calculation-bug-in-blackberry-z10/</id><summary type="html">&lt;p&gt;&lt;img alt="BlackBerry Z10 bug" src="/images/bbz10_calls_bug.png" title="BlackBerry Z10 bug" /&gt;&lt;/p&gt;
&lt;p&gt;I thought BlackBerry 10 is dead but apparently they still provide updates
and introduce new bugs :). This one happened a few days ago with
Software Release 10.3.2.2474. As you can see the time calculations are totally
wrong!&lt;/p&gt;
&lt;p&gt;The current
time is 12:45, March 28th. The last call is reported as &lt;em&gt;5 minutes ago&lt;/em&gt;
but its time stamp is 3 and a half hours ago!&lt;/p&gt;
&lt;p&gt;The second call is reported as &lt;em&gt;11 minutes ago&lt;/em&gt; but in reality it is
3 days ago. I'm not sure about the time stamp but that is probably wrong
as well.&lt;/p&gt;
&lt;p&gt;The reason for this IMO is their design to have a fixed start of the epoch
for every OS release - probably the build time of the release. When the OS
is fully booted and connected to network it synchronizes with time servers
and updates the local time. This of course fails in case of no WiFi, no
cellular data or if automatic time synchronization is turned off!&lt;/p&gt;
&lt;p&gt;The result is that every object (calls, images, etc) which has a time stamp
attached to it gets an incorrect value. Maybe some of these are recalculated
back to the current time, once it is synchronized, and others probably not.
Otherwise I'd expect all calls to be reported way back in time!&lt;/p&gt;
&lt;p&gt;For more information about time related bugs checkout
&lt;a href="https://www.youtube.com/watch?v=MVI87HzfskQ"&gt;Tom Scott's Why 1/1/1970 Bricks Your iPhone&lt;/a&gt;
video and read my article
&lt;a href="http://atodorov.org/blog/2016/03/08/floating-point-precision-error-with-ruby/"&gt;Floating-point precision error with Ruby&lt;/a&gt;.&lt;/p&gt;</summary><category term="QA"></category></entry></feed>