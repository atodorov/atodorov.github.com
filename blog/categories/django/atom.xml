<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Django | atodorov.org - you can logoff, but you can never leave]]></title>
  <link href="http://atodorov.org/blog/categories/django/atom.xml" rel="self"/>
  <link href="http://atodorov.org/"/>
  <updated>2015-05-20T11:40:48+03:00</updated>
  <id>http://atodorov.org/</id>
  <author>
    <name><![CDATA[Alexander Todorov]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Free Software Testing Books]]></title>
    <link href="http://atodorov.org/blog/2015/05/20/free-software-testing-books/"/>
    <updated>2015-05-20T11:35:00+03:00</updated>
    <id>http://atodorov.org/blog/2015/05/20/free-software-testing-books</id>
    <content type="html"><![CDATA[<p>There's a huge list of
<a href="https://github.com/ligurio/free-software-testing-books/blob/master/free-software-testing-books.md">free books</a>
on the topic of software testing. This will definitely be my summer reading list.
I hope you find it helpful.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Videos from Bulgaria Web Summit 2015]]></title>
    <link href="http://atodorov.org/blog/2015/04/20/videos-from-bulgaria-web-summit-2015/"/>
    <updated>2015-04-20T10:45:00+03:00</updated>
    <id>http://atodorov.org/blog/2015/04/20/videos-from-bulgaria-web-summit-2015</id>
    <content type="html"><![CDATA[<p><img src="/images/bgws2015.jpg" title="We're full" alt="We're full" /></p>

<p><a href="http://bulgariawebsummit.com">Bulgaria Web Summit</a> 2015 is over. The event was
incredible and I had a lot of fun moderating the main room. We had many people
coming from other countries and I've made lots of new friends.
Thank you to everyone who attended!</p>

<p>You can find video recordings of all talks in the main room (in order of appearance) below:</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/3THnzZCI4sw?rel=0" frameborder="0" allowfullscreen></iframe>




<iframe width="560" height="315" src="https://www.youtube.com/embed/LtgPnYkEj3E?rel=0" frameborder="0" allowfullscreen></iframe>




<iframe width="560" height="315" src="https://www.youtube.com/embed/4IFyMSvoy-c?rel=0" frameborder="0" allowfullscreen></iframe>




<iframe width="560" height="315" src="https://www.youtube.com/embed/ZGLKZSnCIUU?rel=0" frameborder="0" allowfullscreen></iframe>




<iframe width="560" height="315" src="https://www.youtube.com/embed/GBv4QWFDETY?rel=0" frameborder="0" allowfullscreen></iframe>




<iframe width="560" height="315" src="https://www.youtube.com/embed/DhnsmsvSG7w?rel=0" frameborder="0" allowfullscreen></iframe>


<p>Hope to see you next time in Sofia!</p>

<p>Mean while I learned about <a href="http://devitconf.org/">DEVit</a> in Thessaloniki in May and another one in Zagreb in October.
See you there :)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Speeding Up Celery Backends, Part 3]]></title>
    <link href="http://atodorov.org/blog/2014/11/11/speeding-up-celery-backends-part-3/"/>
    <updated>2014-11-11T15:59:00+02:00</updated>
    <id>http://atodorov.org/blog/2014/11/11/speeding-up-celery-backends-part-3</id>
    <content type="html"><![CDATA[<p>In the second part of this article we've seen
<a href="/blog/2014/11/07/speeding-up-celery-backends-part-2/">how slow Celery actually is</a>.
Now let's explore what happens inside and see if we can't speed things up.</p>

<p>I've used <a href="http://pycallgraph.slowchop.com/en/latest/">pycallgraph</a> to create
call graph visualizations of my application. It has the nice feature to also show
execution time and use different colors for fast and slow operations.</p>

<p>Full command line is:</p>

<pre><code>pycallgraph -v --stdlib --include ... graphviz -o calls.png -- ./manage.py celery_load_test
</code></pre>

<p>where the <code>--include</code> is used to limit the graph to a particular Python module(s).</p>

<h2>General findings</h2>

<p><img src="/images/celery/general.png" title="call graph" alt="call graph" /></p>

<ul>
<li>The first four calls is where most of the time is spent as seen on the picture.</li>
<li>As it seems most of the slow down comes from Celery itself, not the underlying messaging
transport Kombu (not shown on picture)</li>
<li><code>celery.app.amqp.TaskProducer.publish_task</code> takes half of the execution time of
<code>celery.app.base.Celery.send_task</code></li>
<li><code>celery.app.task.Task.delay</code> directly executes <code>.apply_async</code> and can be skipped if one
rewrites the code.</li>
</ul>


<h2>More findings</h2>

<p>In <code>celery.app.base.Celery.send_task</code> there is this block of code:</p>

<pre><code>349         with self.producer_or_acquire(producer) as P:
350             self.backend.on_task_call(P, task_id)
351             task_id = P.publish_task(
352                 name, args, kwargs, countdown=countdown, eta=eta,
353                 task_id=task_id, expires=expires,
354                 callbacks=maybe_list(link), errbacks=maybe_list(link_error),
355                 reply_to=reply_to or self.oid, **options
356             )
</code></pre>

<p><code>producer</code> is always None because delay() doesn't pass it as argument.
I've tried passing it explicitly to apply_async() as so:</p>

<pre><code>from djapp.celery import *

# app = debug_task._get_app() # if not defined in djapp.celery
producer = app.amqp.producer_pool.acquire(block=True)
debug_task.apply_async(producer=producer)
</code></pre>

<p>However this doesn't speedup anything. If we replace the above code block like this:</p>

<pre><code>349         with producer as P:
</code></pre>

<p>it blows up on the second iteration because producer and its channel is already None !?!</p>

<p>If you are unfamiliar with the with statement in Python please read
<a href="http://effbot.org/zone/python-with-statement.htm">this article</a>. In short the with statement is
a compact way of writing try/finally. The underlying <code>kombu.messaging.Producer</code> class does a
<code>self.release()</code> on exit of the with statement.</p>

<p>I also tried killing the with statement and using producer directly but with limited success. While
it was not released(was non None) on subsequent iterations the memory usage grew much more and there
wasn't any performance boost.</p>

<h2>Conclusion</h2>

<p>The with statement is used throughout both Celery and Kombu and I'm not at all sure if
there's a mechanism for keep-alive connections. My time constraints are limited and I'll probably
not spend anymore time on this problem soon.</p>

<p>Since my use case involves task producer and consumers on localhost I'll try to workaround the
current limitations by using Kombu directly
(see <a href="https://gist.github.com/atodorov/2bc1fcd34531ad260ed7">this gist</a>) with a transport that
uses either a UNIX domain socket or a name pipe (FIFO) file.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Speeding up Celery Backends, Part 2]]></title>
    <link href="http://atodorov.org/blog/2014/11/07/speeding-up-celery-backends-part-2/"/>
    <updated>2014-11-07T15:48:00+02:00</updated>
    <id>http://atodorov.org/blog/2014/11/07/speeding-up-celery-backends-part-2</id>
    <content type="html"><![CDATA[<p>In the <a href="/blog/2014/11/05/speeding-up-celery-backends/">first part</a> of this
post I looked at a few celery backends and discovered they didn't meet my needs.
Why is the Celery stack slow? How slow is it actually?</p>

<h2>How slow is Celery in practice</h2>

<ul>
<li>Queue: 500`000 msg/sec</li>
<li>Kombu:  14`000 msg/sec</li>
<li>Celery:  2`000 msg/sec</li>
</ul>


<h2>Detailed test description</h2>

<p>There are three main components of the Celery stack:</p>

<ul>
<li>Celery itself</li>
<li>Kombu which handles the transport layer</li>
<li>Python Queue()'s underlying everything</li>
</ul>


<p>Using the <a href="https://gist.github.com/atodorov/2bc1fcd34531ad260ed7">Queue and Kombu tests</a>
run for 1 000 000 messages I got the following results:</p>

<ul>
<li>Raw Python Queue: Msgs per sec: 500`000</li>
<li>Raw Kombu without Celery where <code>kombu/utils/__init__.py:uuid()</code> is set to return 0

<ul>
<li>with json serializer: Msgs per sec: 5`988</li>
<li>with pickle serializer: Msgs per sec: 12`820</li>
<li>with the custom mem_serializer from <a href="/blog/2014/11/05/speeding-up-celery-backends/">part 1</a>:
Msgs per sec: 14`492</li>
</ul>
</li>
</ul>


<p><strong>Note:</strong> when the test is executed with 100K messages mem_serializer yielded
25`000 msg/sec then the performance is saturated. I've observed similar behavior
with raw Python Queue()'s. I saw some cache buffers being managed internally to avoid OOM
exceptions. This is probably the main reason performance becomes saturated over a longer
execution.</p>

<ul>
<li>Using <a href="https://gist.github.com/atodorov/0156cc41491a5e1ff953">celery_load_test.py</a> modified to
loop 1 000 000 times I got 1908.0 tasks created per sec.</li>
</ul>


<p>Another interesting this worth outlining - in the kombu test there are these lines:
<div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>with producers[connection].acquire(block=True) as producer:&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>for j in range(1000000):
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p></span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>If we swap them the performance drops down to 3875 msg/sec which is comparable with the
Celery results. Indeed inside Celery there's the same <code>with producer.acquire(block=True)</code>
construct which is executed every time a new task is published. Next I will be looking
into this to figure out exactly where the slowliness comes from.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Speeding up Celery Backends, Part 1]]></title>
    <link href="http://atodorov.org/blog/2014/11/05/speeding-up-celery-backends/"/>
    <updated>2014-11-05T15:20:00+02:00</updated>
    <id>http://atodorov.org/blog/2014/11/05/speeding-up-celery-backends</id>
    <content type="html"><![CDATA[<p>I'm working on an application which fires a lot of Celery tasks - the more
the better! Unfortunately Celery backends seem to be rather slow :(.
Using the <a href="https://gist.github.com/atodorov/0156cc41491a5e1ff953">celery_load_test.py</a>
command for Django I was able to capture some metrics:</p>

<ul>
<li>Amazon SQS backend: 2 or 3 tasks/sec</li>
<li>Filesystem backend: 2000 - 2500 tasks/sec</li>
<li>Memory backend: around 3000 tasks/sec</li>
</ul>


<p>Not bad but I need in the order of 10000 tasks created per sec!
The other noticeable thing is that memory backend isn't much faster compared to
the filesystem one! NB: all of these backends actually come from the kombu package.</p>

<h2>Why is Celery slow ?</h2>

<p>Using <code>celery_load_test.py</code> together with
<a href="/blog/2014/11/05/performance-profiling-in-python-with-cprofile/">cProfile</a> I
was able to pin-point some problematic areas:</p>

<ul>
<li><code>kombu/transports/virtual/__init__.py</code>: class Channel.basic_publish() - does
self.encode_body() into base64 encoded string. Fixed with custom transport backend
I called fastmemory which redefines the body_encoding property:</li>
</ul>


<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="nd">@cached_property</span>
</span><span class='line'><span class="k">def</span> <span class="nf">body_encoding</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class='line'>    <span class="k">return</span> <span class="bp">None</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<ul>
<li>Celery uses json or pickle (or other) serializers to serialize the data.
While json yields between 2000-3000 tasks/sec, pickle does around 3500 tasks/sec.
Replacing with a custom serializer which just returns
the objects (since we read/write from/to memory) yields about 4000 tasks/sec tops:
<div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="kn">from</span> <span class="nn">kombu.serialization</span> <span class="kn">import</span> <span class="n">register</span><span class="o">&lt;/</span><span class="n">li</span><span class="o">&gt;</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">ul</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="k">def</span> <span class="nf">loads</span><span class="p">(</span><span class="n">s</span><span class="p">):</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="k">return</span> <span class="n">s</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="k">def</span> <span class="nf">dumps</span><span class="p">(</span><span class="n">s</span><span class="p">):</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="k">return</span> <span class="n">s</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">register</span><span class="p">(</span><span class="s">&#39;mem_serializer&#39;</span><span class="p">,</span> <span class="n">dumps</span><span class="p">,</span> <span class="n">loads</span><span class="p">,</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span>    <span class="n">content_type</span><span class="o">=</span><span class="s">&#39;application/x-memory&#39;</span><span class="p">,</span>
</span><span class='line'>    <span class="n">content_encoding</span><span class="o">=</span><span class="s">&#39;binary&#39;</span><span class="p">)</span>
</span><span class='line'><span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<ul>
<li><code>kombu/utils/__init__.py</code>: def uuid() - generates random unique identifiers
which is a slow operation. Replacing it with <code>return "00000000"</code> boosts performance
to 7000 tasks/sec.</li>
</ul>


<p>It's clear that a constant UUID is not of any practical use but serves well to illustrate
how much does this function affect performance.</p>

<p><strong>Note:</strong>
Subsequent executions of <code>celery_load_test</code> seem to report degraded performance even with
the most optimized transport backend. I'm not sure why is this. One possibility is the random
UUID usage in other parts of the Celery/Kombu stack which drains entropy on the system and
generating more random numbers becomes slower. If you know better please tell me!</p>

<p>I will be looking for a better understanding
of these IDs in Celery and hope to be able to produce a faster uuid() function. Then I'll be
exploring the transport stack even more in order to reach the goal of 10000 tasks/sec.
If you have any suggestions or pointers please share them in the comments.</p>
]]></content>
  </entry>
  
</feed>
