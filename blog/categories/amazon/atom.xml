<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Amazon | atodorov.org - you can logoff, but you can never leave]]></title>
  <link href="http://atodorov.org/blog/categories/amazon/atom.xml" rel="self"/>
  <link href="http://atodorov.org/"/>
  <updated>2014-02-07T01:01:00+02:00</updated>
  <id>http://atodorov.org/</id>
  <author>
    <name><![CDATA[Alexander Todorov]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[AWS Tip: Shrinking EBS Root Volume Size]]></title>
    <link href="http://atodorov.org/blog/2014/02/07/aws-tip-shrinking-ebs-root-volume-size/"/>
    <updated>2014-02-07T00:23:00+02:00</updated>
    <id>http://atodorov.org/blog/2014/02/07/aws-tip-shrinking-ebs-root-volume-size</id>
    <content type="html"><![CDATA[<p>Amazon's Elastic Block Store volumes are easy to use and expand but notoriously
hard to shrink once their size has grown. Here is my tip for shrinking EBS size
and saving some money from over-provisioned storage. I'm assuming that you want to
shrink the root volume which is on EBS.</p>

<ul>
<li>Write down the block device name for the root volume (/dev/sda1) - <em>from AWS console:
Instances; Select instance; Look at Details tab; See Root device or Block devices</em>;</li>
<li>Write down the availability zone of your instance - <em>from AWS console: Instances;
column Availability Zone</em>;</li>
<li>Stop instance;</li>
<li>Create snapshot of the root volume;</li>
<li>From the snapshot, create a second volume, in the <strong>same availability zone</strong> as
your instance (you will have to attach it later). This will be your pristine source;</li>
<li>Create new empty EBS volume (not based on a snapshot), with smaller size,
in the same availability zone - <em>from AWS console: Volumes; Create Volume;
Snapshot == No Snapshot</em>; <strong>IMPORTANT</strong> - size should be large enough to hold
all the files from the source file system (try <code>df -h</code> on the source first);</li>
<li>Attach both volumes to instance while taking note of the block devices names
you assign for them in the AWS console;</li>
</ul>


<p>For example: In my case <code>/dev/sdc1</code> is the source snapshot and <code>/dev/sdd1</code> is the
empty target.</p>

<ul>
<li>Start instance;</li>
<li>Optionally check the source file system with <code>e2fsck -f /dev/sdc1</code>;</li>
<li>Create a file system for the empty volume - <code>mkfs.ext4 /dev/sdd1</code>;</li>
<li>Mount volumes at <code>/source</code> and <code>/target</code> respectively;</li>
<li>Now sync the files: <code>rsync -aHAXxSP /source/ /target</code>. <strong>Note the missing slash (/)
after <code>/target</code></strong>. If you add it you will end up with files inside <code>/target/source/</code>
which you don't want;</li>
<li>Quickly verify the new directory structure with <code>ls -l /target</code>;</li>
<li>Unmount <code>/target</code>;</li>
<li>Optionally check the new file system for consistency <code>e2fsck -f /dev/sdd1</code>;</li>
<li><strong>IMPORTANT</strong> - check how <code>/boot/grub/grub.conf</code> specifies the root volume -
by UUID, by LABEL, by device name, etc. You will have to duplicate the same for the
new smaller volume or update <code>/target/boot/grub/grub.conf</code> to match the new volume.
Check <code>/target/etc/fstab</code> as well!</li>
</ul>


<p>In my case I had to <code>e2label /dev/sdd1 /</code> because both <code>grub.conf</code> and <code>fstab</code> were
using the device label.</p>

<ul>
<li>Shutdown the instance;</li>
<li>Detach all volumes;</li>
<li><strong>IMPORTANT</strong> - attach the new smaller volume to the instance using the same block device
name from the first step (e.g. <code>/dev/sda1</code>);</li>
<li>Start the instance and verify it is working correctly;</li>
<li>DELETE auxiliary volumes and snapshots so they don't take space and accumulate costs!</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Idempotent Django Email Sender with Amazon SQS and Memcache]]></title>
    <link href="http://atodorov.org/blog/2013/12/11/idempotent-django-email-sender-with-amazon-sqs-and-memcache/"/>
    <updated>2013-12-11T23:29:00+02:00</updated>
    <id>http://atodorov.org/blog/2013/12/11/idempotent-django-email-sender-with-amazon-sqs-and-memcache</id>
    <content type="html"><![CDATA[<p>Recently I wrote about my problem with
<a href="/blog/2013/12/06/duplicate-amazon-sqs-messages-cause-multiple-emails/">duplicate Amazon SQS messages causing multiple emails</a>
for <a href="http://www.dif.io">Difio</a>. After considering several options and
feedback from
<a href="https://twitter.com/atodorov_/status/409429840820199424">@Answers4AWS</a>
I wrote a small decorator to fix this.</p>

<p>It uses the cache backend to prevent the task from executing twice
during the specified time frame. The code is available at
<a href="https://djangosnippets.org/snippets/3010/">https://djangosnippets.org/snippets/3010/</a>.</p>

<p>As stated on Twitter you should use Memcache (or ElastiCache) for this.
If using Amazon S3 with my
<a href="https://github.com/atodorov/django-s3-cache">django-s3-cache</a> don't use the
<code>us-east-1</code> region because it is eventually consistent.</p>

<p>The solution is fast and simple on the development side and uses my existing
cache infrastructure so it doesn't cost anything more!</p>

<p>There is still a race condition between marking the message as processed
and the second check but nevertheless this should minimize the possibility of
receiving duplicate emails to an accepted level. Only time will tell though!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Duplicate Amazon SQS Messages Cause Multiple Emails]]></title>
    <link href="http://atodorov.org/blog/2013/12/06/duplicate-amazon-sqs-messages-cause-multiple-emails/"/>
    <updated>2013-12-06T22:47:00+02:00</updated>
    <id>http://atodorov.org/blog/2013/12/06/duplicate-amazon-sqs-messages-cause-multiple-emails</id>
    <content type="html"><![CDATA[<p>Beware if using Amazon Simple Queue Service to send email messages!
Sometime SQS messages are duplicated which results in multiple copies of
the messages being sent. This happened today at <a href="http://www.dif.io">Difio</a>
and is really annoying to users. In this post I will explain why there is no easy
way of fixing it.</p>

<p><blockquote><p>Q: Can a deleted message be received again?</p></p><p><p>Yes, under rare circumstances you might receive a previously deleted message again.<br/>This can occur in the rare situation in which a DeleteMessage operation doesn't<br/>delete all copies of a message because one of the servers in the distributed<br/>Amazon SQS system isn't available at the time of the deletion. That message copy<br/>can then be delivered again. You should design your application so that no errors<br/>or inconsistencies occur if you receive a deleted message again.</p><footer><strong>Amazon FAQ</strong></footer></blockquote></p>

<p>In my case the cron scheduler logs say:</p>

<pre><code>&gt;&gt;&gt; &lt;AsyncResult: a9e5a73a-4d4a-4995-a91c-90295e27100a&gt;
</code></pre>

<p>While on the worker nodes the logs say:</p>

<pre><code>[2013-12-06 10:13:06,229: INFO/MainProcess] Got task from broker: tasks.cron_monthly_email_reminder[a9e5a73a-4d4a-4995-a91c-90295e27100a]
[2013-12-06 10:18:09,456: INFO/MainProcess] Got task from broker: tasks.cron_monthly_email_reminder[a9e5a73a-4d4a-4995-a91c-90295e27100a]
</code></pre>

<p>This clearly shows the same message (see the UUID) has been processed twice!
This resulted in hundreds of duplicate emails :(.</p>

<h2>Why This Is Hard To Fix</h2>

<p>There are two basic approaches to solve this issue:</p>

<ul>
<li>Check some log files or database for previous record of the message having
been processed;</li>
<li>Use idempotent operations that if you process the message again, you
get the same results, and that those results don't create duplicate files/records.</li>
</ul>


<p>The problem with checking for duplicate messages is:</p>

<ul>
<li>There is a race condition between marking the message as processed and the
second check;</li>
<li>You need to use some sort of locking mechanism to safe-guard against the race condition;</li>
<li>In the event of an eventual consistency of the log/DB you can't guarantee that
the previous attempt will show up and so can't guarantee that you won't process
the message twice.</li>
</ul>


<p>All of the above don't seem to work well for distributed applications not to mention
Difio processes millions of messages per month, per node and the logs are quite big.</p>

<p>The second option is to have control of the Message-Id or some other email header
so that the second message will be discarded either at the server (Amazon SES in my case)
or at the receiving MUA. I like this better but I don't think it is technically possible
with the current environment. Need to check though.</p>

<p>I've asked AWS support to look into
<a href="https://forums.aws.amazon.com/thread.jspa?threadID=140782">this thread</a> and hopefully
they will have some more hints. If you have any other ideas please post in the comments!
Thanks!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How I Created a Website In Two Days Without Coding]]></title>
    <link href="http://atodorov.org/blog/2013/07/31/how-i-created-a-website-in-two-days-without-coding/"/>
    <updated>2013-07-31T21:55:00+03:00</updated>
    <id>http://atodorov.org/blog/2013/07/31/how-i-created-a-website-in-two-days-without-coding</id>
    <content type="html"><![CDATA[<p><img src="/images/logos/obuvki41plus_header.png" title="header image" alt="header image" /></p>

<p>This is a simple story about a website I helped create without using any
programming at all. It took me two days because of the images and the logo
design which I've commissioned to a friend.</p>

<p>The website is <a href="http://obuvki41plus.com/">obuvki41plus.com</a> which is a
re-seller business my spouse runs. It specializes in large size, elegant
ladies shoes - Europe size 41 plus (hard to find in Bulgaria),
hence the name.</p>

<h2>Required Functionality</h2>

<ul>
<li>Display a catalog of items for sale with detailed information about
each item;</li>
<li>Make it possible for people to comment and share the items;</li>
<li>Very basic shopping cart which stores the selected items and then
redirects to a page with order instructions. Actual order is made via
phone for several reasons which I will explain in
<a href="/blog/2013/08/01/why-taking-orders-by-phone-works-for-my-start-up/">another post</a>;</li>
<li>Add a feedback/contact form;</li>
<li>Look nice on mobile devices.</li>
</ul>


<h2>Technology</h2>

<ul>
<li>The website is static, all pages are simple HTML and is hosted in
Amazon S3;</li>
<li>Comments are provided by Facebook's
<a href="https://developers.facebook.com/docs/reference/plugins/comments/">Comments Box</a>
plug-in;</li>
<li>Social media buttons and tracking are provided by
<a href="https://www.addthis.com/">AddThis</a>;</li>
<li>Visitors analytics is standard and is from
<a href="http://www.google.com/analytics/">Google Analytics</a>;</li>
<li>Template is from <a href="http://pages.github.com/">GitHub Pages</a> with slight
modifications; Works on mobile too;</li>
<li>Logo is custom designed by my friend
<a href="https://www.facebook.com/aluinpoli">Polina Valerieva</a>;</li>
<li>Feedback/contact form is by <a href="https://www.uservoice.com/">UserVoice</a>;</li>
<li>Shopping cart is by <a href="http://simplecartjs.org/">simpleCart(js)</a>.
I've created a simple animation effect when pressing the "ADD TO CART"
link to visually alert the user. This is done with jQuery.</li>
</ul>


<p>I could have used some JavaScript templating engine like
<a href="http://handlebarsjs.com/">Handlebars</a> but at the time I didn't know about
it and I prefer not to write JavaScript if possible :).</p>

<h2>Colophon</h2>

<p>I did some coding after the initial release eventually.
I've transformed the website to a Django
based site which is exported as static HTML.</p>

<p>This helps me with faster deployment/management as everything is stored
in git, allows templates inheritance and also makes the site ready to
add more functionality if required.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What Runs Your Start-up - Imagga]]></title>
    <link href="http://atodorov.org/blog/2013/07/29/what-runs-your-startup-imagga/"/>
    <updated>2013-07-29T12:32:00+03:00</updated>
    <id>http://atodorov.org/blog/2013/07/29/what-runs-your-startup-imagga</id>
    <content type="html"><![CDATA[<p><img src="http://atodorov.org/images/startup/imagga.png" alt="Imagga" style="float:left; margin-right:10px;" /></p>

<p><a href="http://imagga.com/">Imagga</a> is a cloud platform that helps businesses and
individuals organize their images in a fast and cost-effective way. They
develop a range of advanced proprietary image recognition and image processing
technologies, which are built into several services such as smart image
cropping, color extraction and multi-color search, visual similarity search and
auto-tagging.</p>

<p>During
<a href="/blog/2013/05/23/balkan-venture-forum-sofia-post-mortem/">Balkan Venture Forum</a>
in Sofia I sat down with Georgi Kadrev to talk about technology.
Surprisingly this hi-tech service is built on top of standard low-tech components
and lots of hard work.</p>

<h2>Main Technologies</h2>

<p>Core functionality is developed in C and C++ with the OpenCV library.
Imagga relies heavily on own image processing algorithms for their core
features. These were built as a combination of their own research activities
and publications from other researchers.</p>

<p>Image processing is executed by worker nodes configured with their own
software stack. Nodes are distributed among Amazon EC2 and other data centers.</p>

<p>Client libraries to access Imagga API are available in PHP, Ruby and Java.</p>

<p>Imagga has built several websites to showcase their technology.
<a href="http://cropp.me/">Cropp.me</a>, <a href="http://colorslike.me/">ColorsLike.me</a>,
<a href="http://www.stockpodium.com">StockPodium</a> and <a href="http://autotag.me/">AutoTag.me</a>
were built with PHP, JavaScript and jQuery above a standard LAMP stack.</p>

<p>Recently Imagga also started using GPU computing with nVidia Tesla cards.
They use C++ and Python bindings for
<a href="https://developer.nvidia.com/what-cuda">CUDA</a>.</p>

<h2>Why Not Something Else?</h2>

<p><blockquote><p>As an initially bootstrapping start-up we chose something that is basically free,<br/>reliable and popular - that's why started with the LAMP stack. It proved to be<br/>stable and convenient for our web needs and we preserved it.<br/>The use of C++ is a natural choice for computational intensive tasks that we<br/>need to perform for the purpose of our core expertise - image processing.<br/>Though we initially wrote the whole core technology code from scratch, we later<br/>switched to OpenCV for some of the building blocks as it is very well optimized<br/>and continuously extended image processing library.</p></p><p><p>With the raise of affordable high-performance GPU processors and their availability<br/>in server instances, we decided it's time to take advantage of this highly parallel<br/>architecture, perfectly suitable for image processing tasks.</p><footer><strong>Georgi Kadrev</strong></footer></blockquote></p>

<h2>Want More Info?</h2>

<p>If youâ€™d like to hear more from Imagga please comment below.
I will ask them to follow this thread and reply to your questions.</p>
]]></content>
  </entry>
  
</feed>
