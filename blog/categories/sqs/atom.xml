<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: SQS | atodorov.org - you can logoff, but you can never leave]]></title>
  <link href="http://atodorov.org/blog/categories/sqs/atom.xml" rel="self"/>
  <link href="http://atodorov.org/"/>
  <updated>2014-01-24T22:12:52+02:00</updated>
  <id>http://atodorov.org/</id>
  <author>
    <name><![CDATA[Alexander Todorov]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Idempotent Django Email Sender with Amazon SQS and Memcache]]></title>
    <link href="http://atodorov.org/blog/2013/12/11/idempotent-django-email-sender-with-amazon-sqs-and-memcache/"/>
    <updated>2013-12-11T23:29:00+02:00</updated>
    <id>http://atodorov.org/blog/2013/12/11/idempotent-django-email-sender-with-amazon-sqs-and-memcache</id>
    <content type="html"><![CDATA[<p>Recently I wrote about my problem with
<a href="/blog/2013/12/06/duplicate-amazon-sqs-messages-cause-multiple-emails/">duplicate Amazon SQS messages causing multiple emails</a>
for <a href="http://www.dif.io">Difio</a>. After considering several options and
feedback from
<a href="https://twitter.com/atodorov_/status/409429840820199424">@Answers4AWS</a>
I wrote a small decorator to fix this.</p>

<p>It uses the cache backend to prevent the task from executing twice
during the specified time frame. The code is available at
<a href="https://djangosnippets.org/snippets/3010/">https://djangosnippets.org/snippets/3010/</a>.</p>

<p>As stated on Twitter you should use Memcache (or ElastiCache) for this.
If using Amazon S3 with my
<a href="https://github.com/atodorov/django-s3-cache">django-s3-cache</a> don't use the
<code>us-east-1</code> region because it is eventually consistent.</p>

<p>The solution is fast and simple on the development side and uses my existing
cache infrastructure so it doesn't cost anything more!</p>

<p>There is still a race condition between marking the message as processed
and the second check but nevertheless this should minimize the possibility of
receiving duplicate emails to an accepted level. Only time will tell though!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Duplicate Amazon SQS Messages Cause Multiple Emails]]></title>
    <link href="http://atodorov.org/blog/2013/12/06/duplicate-amazon-sqs-messages-cause-multiple-emails/"/>
    <updated>2013-12-06T22:47:00+02:00</updated>
    <id>http://atodorov.org/blog/2013/12/06/duplicate-amazon-sqs-messages-cause-multiple-emails</id>
    <content type="html"><![CDATA[<p>Beware if using Amazon Simple Queue Service to send email messages!
Sometime SQS messages are duplicated which results in multiple copies of
the messages being sent. This happened today at <a href="http://www.dif.io">Difio</a>
and is really annoying to users. In this post I will explain why there is no easy
way of fixing it.</p>

<p>{% blockquote Amazon FAQ %}
Q: Can a deleted message be received again?</p>

<p>Yes, under rare circumstances you might receive a previously deleted message again.
This can occur in the rare situation in which a DeleteMessage operation doesn't
delete all copies of a message because one of the servers in the distributed
Amazon SQS system isn't available at the time of the deletion. That message copy
can then be delivered again. You should design your application so that no errors
or inconsistencies occur if you receive a deleted message again.
{% endblockquote %}</p>

<p>In my case the cron scheduler logs say:</p>

<pre><code>&gt;&gt;&gt; &lt;AsyncResult: a9e5a73a-4d4a-4995-a91c-90295e27100a&gt;
</code></pre>

<p>While on the worker nodes the logs say:</p>

<pre><code>[2013-12-06 10:13:06,229: INFO/MainProcess] Got task from broker: tasks.cron_monthly_email_reminder[a9e5a73a-4d4a-4995-a91c-90295e27100a]
[2013-12-06 10:18:09,456: INFO/MainProcess] Got task from broker: tasks.cron_monthly_email_reminder[a9e5a73a-4d4a-4995-a91c-90295e27100a]
</code></pre>

<p>This clearly shows the same message (see the UUID) has been processed twice!
This resulted in hundreds of duplicate emails :(.</p>

<h2>Why This Is Hard To Fix</h2>

<p>There are two basic approaches to solve this issue:</p>

<ul>
<li>Check some log files or database for previous record of the message having
been processed;</li>
<li>Use idempotent operations that if you process the message again, you
get the same results, and that those results don't create duplicate files/records.</li>
</ul>


<p>The problem with checking for duplicate messages is:</p>

<ul>
<li>There is a race condition between marking the message as processed and the
second check;</li>
<li>You need to use some sort of locking mechanism to safe-guard against the race condition;</li>
<li>In the event of an eventual consistency of the log/DB you can't guarantee that
the previous attempt will show up and so can't guarantee that you won't process
the message twice.</li>
</ul>


<p>All of the above don't seem to work well for distributed applications not to mention
Difio processes millions of messages per month, per node and the logs are quite big.</p>

<p>The second option is to have control of the Message-Id or some other email header
so that the second message will be discarded either at the server (Amazon SES in my case)
or at the receiving MUA. I like this better but I don't think it is technically possible
with the current environment. Need to check though.</p>

<p>I've asked AWS support to look into
<a href="https://forums.aws.amazon.com/thread.jspa?threadID=140782">this thread</a> and hopefully
they will have some more hints. If you have any other ideas please post in the comments!
Thanks!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tip: Caching Large Objects for Celery and Amazon SQS ]]></title>
    <link href="http://atodorov.org/blog/2013/06/19/tip-caching-large-objects-for-celery-and-amazon-sqs/"/>
    <updated>2013-06-19T14:29:00+03:00</updated>
    <id>http://atodorov.org/blog/2013/06/19/tip-caching-large-objects-for-celery-and-amazon-sqs</id>
    <content type="html"><![CDATA[<p>Some time ago a guy called Matt
<a href="https://groups.google.com/forum/?fromgroups=#!topic/celery-users/RFAuGjZwtmg">asked</a>
about passing large objects through their messaging queue. They were switching from
RabbitMQ to Amazon SQS which has a limit of 64K total message size.</p>

<p>Recently I've made some changes in <a href="http://www.dif.io">Difio</a> which require passing
larger objects as parameters to a Celery task. Since Difio is also using SQS I faced the
same problem. Here is the solution using a cache back-end:</p>

<p>{% codeblock lang:python %}</p>

<p>from celery.task import task
from django.core import cache as cache_module</p>

<p>def some_method():</p>

<pre><code>... skip ...

task_cache = cache_module.get_cache('taskq')
task_cache.set(uuid, data, 3600)

handle_data.delay(uuid)

... skip ...
</code></pre>

<p>@task
def handle_data(uuid):</p>

<pre><code>task_cache = cache_module.get_cache('taskq')
data = task_cache.get(uuid)

if data is None:
    return

... do stuff ...
</code></pre>

<p>{% endcodeblock %}</p>

<p>Objects are persisted in a secondary cache back-end, not the default one, to avoid
accidental destruction. <code>uuid</code> parameter is a string.</p>

<p>Although the objects passed are smaller than 64K I haven't seen any issues
with this solution so far. Let me know if you are using something similar in your code
and how it works for you.</p>
]]></content>
  </entry>
  
</feed>
